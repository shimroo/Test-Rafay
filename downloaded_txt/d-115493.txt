




the changelog | comments on family, technology, and society








































the changelog
comments on family, technology, and society



menu

						skip to content					

about
friends



search for:



 








censorship is complicated: what internet history says about meta/facebook


january 8, 2025freedom, online lifebbs, censorship, debian, facebook, free speech, internet, usenetjohn goerzen 


in light of this week’s announcement by meta (facebook, instagram, threads, etc), i have been pondering this question: why am i, a person that has long been a staunch advocate of free speech and encryption, leery of sites that talk about being free speech-oriented?  and, more to the point, why an i — a person that has been censored by facebook for mentioning the open source social network mastodon — not cheering a “lighter touch”?
the answers are complicated, and take me back to the early days of social networking.  yes, i mean the 1980s and 1990s.
before digital communications, there were barriers to reaching a lot of people.  especially money.  this led to a sort of self-censorship: it may be legal to write certain things, but would a newspaper publish a letter to the editor containing expletives?  probably not.
as digital communications started to happen, suddenly people could have their own communities.  not just free from the same kinds of monetary pressures, but free from outside oversight (parents, teachers, peers, community, etc.)  when you have a community that the majority of people lack the equipment to access — and wouldn’t understand how to access even if they had the equipment — you have a place where self-expression can be unleashed.
and, as j. c. herz covers in what is now an unintentional history (her book surfing on the internet was published in 1995), self-expression was unleashed.  she enjoyed the wit and expression of everything from odd corners of usenet to the text-based open world of moos and muds.  she even talks about groups dedicated to insults (flaming) in positive terms.
but as i’ve seen time and again, if there are absolutely no rules, then whenever a group gets big enough — more than a few dozen people, say — there are troublemakers that ruin it for everyone.  maybe it’s trolling, maybe it’s vicious attacks, you name it — it will arrive and it will be poisonous.
i remember the debates within the debian community about this.  debian is one of the pillars of the internet today, a nonprofit project with free speech in its dna.  and yet there were inevitably the poisonous people.  debian took too long to learn that allowing those people to run rampant was causing more harm than good, because having a well-worn delete key and a tolerance for insults became a requirement for being a debian developer, and that drove away people that had no desire to deal with such things.  (i should note that debian strikes a much better balance today.)
but in reality, there were never absolutely no rules.  if you joined a bbs, you used it at the whim of the owner (the “sysop” or system operator).  the sysop may be a 16-yr-old running it from their bedroom, or a retired programmer, but in any case they were letting you use their resources for free and they could kick you off for any or no reason at all.  so if you caused trouble, or perhaps insulted their cat, you’re banned.  but, in all but the smallest towns, there were other options you could try.
on the other hand, sysops enjoyed having people call their bbss and didn’t want to drive everyone off, so there was a natural balance at play.  as networks like fidonet developed, a sort of uneasy approach kicked in: don’t be excessively annoying, and don’t be easily annoyed.  like it or not, it seemed to generally work.  a bbs that repeatedly failed to deal with troublemakers could risk removal from fidonet.
on the more institutional usenet, you generally got access through your university (or, in a few cases, employer).  most universities didn’t really even know they were running a usenet server, and you were generally left alone.  until you did something that annoyed somebody enough that they tracked down the phone number for your dean, in which case real-world consequences would kick in.  a site may face the usenet death penalty — delinking from the network — if they repeatedly failed to prevent malicious content from flowing through their site.
some bbss let people from minority communities such as lgbtq+ thrive in a place of peace from tormentors.  a lot of them let people be themselves in a way they couldn’t be “in real life”.  and yes, some harbored trolls and flamers.
the point i am trying to make here is that each bbs, or usenet site, set their own policies about what their own users could do.  these had to be harmonized to a certain extent with the global community, but in a certain sense, with bbss especially, you could just use a different one if you didn’t like what the vibe was at a certain place.
that this free speech ethos survived was never inevitable.  there were many attempts to regulate the internet, and it was thanks to the advocacy of groups like the eff that we have things like strong encryption and a degree of freedom online.
with the rise of the very large platforms — and here i mean compuserve and aol at first, and then facebook, twitter, and the like later — the low-friction option of just choosing a different place started to decline.  you could participate on a fidonet forum from any of thousands of bbss, but you could only participate in an aol forum from aol.  the same goes for facebook, twitter, and so forth.  not only that, but as social media became conceived of as very large sites, it became impossible for a person with enough skill, funds, and time to just start a site themselves.  instead of neading a few thousand dollars of equipment, you’d need tens or hundreds of millions of dollars of equipment and employees.
all that means you can’t really run facebook as a nonprofit.  it is a business.  it should be absolutely clear to everyone that facebook’s mission is not the one they say it is — “[to] give people the power to build community and bring the world closer together.”  if that was their goal, they wouldn’t be creating ai users and ai spam and all the rest.  zuck isn’t showing courage; he’s sucking up to trump and those that will pay the price are those that always do: women and minorities.
really, the point of any large social network isn’t to build community.  it’s to make the owners their next billion.  they do that by convincing people to look at ads on their site.  zuck is as much a windsock as anyone else; he will adjust policies in whichever direction he thinks the wind is blowing so as to let him keep putting ads in front of eyeballs, and stomp all over principles — even free speech — doing it.  don’t expect anything different from any large commercial social network either.  bluesky is going to follow the same trajectory as all the others.
the problem with a one-size-fits-all content policy is that the world isn’t that kind of place.  for instance, i am a pacifist.  there is a place for a group where pacifists can hang out with each other, free from the noise of the debate about pacifism.  and there is a place for the debate.  forcing everyone that signs up for the conversation to sign up for the debate is harmful.  preventing the debate is often also harmful.  one company can’t square this circle.
beyond that, the fact that we care so much about one company is a problem on two levels.  first, it indicates how succeptible people are to misinformation and such.  i don’t have much to offer on that point.  secondly, it indicates that we are too centralized.
we have a solution there: mastodon.  mastodon is a modern, open source, decentralized social network.  you can join any instance, easily migrate your account from one server to another, and so forth.  you pick an instance that suits you.  there are thousands of others you can choose from.  some aggressively defederate with instances known to harbor poisonous people; some don’t.
and, to harken back to the bbs era, if you have some time, some skill, and a few bucks, you can run your own mastodon instance.
personally, i still visit facebook on occasion because some people i care about are mainly there.  but it is such a terrible experience that i rarely do.  meta is becoming irrelevant to me.  they are on a path to becoming irrelevant to many more as well.  maybe this is the moment to go “shrug, this sucks” and try something better.
(and when you do, feel free to say hi to me at @jgoerzen@floss.social on mastodon.)



view all 16 comments 





review of reputable, functional, and secure email service


may 16, 2024freedom, online life, technologyemail, internetjohn goerzen 




i last reviewed email services in 2019.  that review focused a lot of attention on privacy.  at the time, i selected mailbox.org as my provider, and have been using them for these 5 years since.  however, both their service and their support have gone significantly downhill since, so it is time for me to look at other options.


here i am focusing strongly on email.  some of the providers mentioned here provide other services (im, video calls, groupware, etc.), and to the extent they do, i am ignoring them.



what matters in 2024


i want to start off by acknowledging that what you need in email probably depends on your circumstances and the country in which you live.  for me, i begin by naming that the largest threat most of us face isn’t from state actors but from criminals: hackers, ransomware gangs, etc.  it is important to take as many steps as possible to secure one’s account against that.  privacy and security are both part of the mix.  i still value privacy but i am acknowledging, as migadu does, that “email as we know it and encryption are incompatible.”  although some of these services strongly protect parts of the conversation, the reality is that most people will be emailing people using plain old email services which don’t.  for stronger security, something like signal would be needed.  (i wrote about signal in 2021 also.)


interestingly, openpgp support seems to be something of a standard feature in the providers i reviewed by this point.  all or almost all of them provide integration with browser-based encryption as well as server-side encryption if you prefer that.


although mailbox.org can automatically pgp-encrypt every message that arrives in plaintext, for general use, this is unwieldy; there isn’t good tooling for searching mailboxes where every message is encrypted, etc.   so i never enabled that feature at mailbox.  i still value security and privacy, but a pragmatic approach addresses the most pressing threats first.




my criteria


the basic requirements for an email service include:


ability to use my own domains
strong privacy policy
ability for me to use my own imap and smtp clients on both desktop and mobile
it must be extremely reliable
it must not be free
it must have excellent support for those rare occasions when it is needed
support for basic aliases


why do i say it must not be free?  because if someone is providing a service with the quality i’m talking about here, and not charging for it, it implies something is fishy: either they are unscrupulous, are financially unstable, or the product is something else like ads.  i am not aware of any provider that matches the other criteria with a free account anyhow.  these providers range from about $30 to $90 per year, so cheaper than a netflix subscription.


immediately, this rules out several options:


proton doesn’t let me use my own clients on mobile (their bridge is desktop-only)
tuta also doesn’t let me use my own clients
posteo doesn’t let me use my own domain
mxroute.com lacks a strong privacy policy, and its policy has numerous causes for concern (for instance, “if you repeatedly send email to invalid/unroutable recipients, they may be published on our github”)


i will have a bit more to say about a couple of these providers below.


there are some additional criteria that are strongly desired but not absolutely required:


ability to set individual access passwords for every device/app
support for two-factor authentication (2fa/tfa/totp) for web-based access
support for basics in filtering: ability to filter on envelope recipient (so if i get bcc’d, i can still filter), and ability to execute more than one action on filter match (eg, deliver to two folders, or deliver to a folder and forward to someone else)


imap and smtp don’t really support 2fa, so by setting individual passwords for every device, you can at least limit the blast radius and cut off a specific device if something is (or might be) compromised.




the candidates


i considered these providers: startmail, mailfence, runbox, fastmail, kolab, mailbox.org, and migadu.  i’ll review each, and highlight the pricing of the plan i would most likely use.  each provider offers multiple plans; some may be more expensive and some may be cheaper than the one i reviewed.  i included a link to each provider’s full pricing information so you can compare for your needs.


i set up trials with each of these (except mailbox.org, with which i already had a paid account).  it so happend that i had actual questions for support for each one, which gave me an opportunity to see how support responded.  i did not fabricate questions, and would not have contacted support if i didn’t have real ones.  (this means that i asked different questions of each provider, because they were the real questions i had.)  i’ll jump to the spoiler right now: i eventually chose migadu, with fastmail and mailfence as close seconds.


i looked for providers myself, and also solicited recommendations in a mastodon thread.




mailbox.org


i begin with mailbox, as it was my top choice in 2019 and the incumbent.


until this year, i had been quite happy with it.  i had cause to reach their support less than once a year on average, and each time they replied the same day or next day.  now, however, they are failing on reliability and on support.


their spam filter has become overly aggressive.  it has blocked quite a bit of legitimate mail.  when contacting their support about a prior issue earlier this year, they initially took 4 days to reply, and then 6 days to reply after that.  ouch.  they had me disable some spam settings.


it didn’t really help.  i continue to lose mail.  i don’t know how much, because they block a lot of it before it even hits the spam folder.  one of my friends texted to say mail was dropping.  i raised a new ticket with mailbox, which took them 5 days to reply to.  their reply was unhelpful.  “as the internet is not a static system, unforeseen events can always occur.”  well yes, that’s true, and i get it, false positives exist with email.  but this was from an isp’s mail system with an address that had been established for years, and it was part of a larger pattern of rejecting quite a bit of legit mail.  and every interaction with them recently hasn’t resulted in them actually doing anything to resolve anything.  it’s just a paragraph or two of reply that does nothing and helps nothing.


when i complained that it took 5 days to reply, they said “we have not been able to reply sooner as we are currently experiencing a high volume of customer enquiries.”  even though their sla for my account is a not-great “48 business hour” turnaround, they still missed it and their reason is “we’re busy.”  i finally asked what rbl had caught the blocked email, since when i checked, the sender wasn’t on any rbl.  mailbox’s reply: they only keep their logs for 7 days, so next time i should contact them within 7 days.  which, of course, i did; it was them that kept delaying.  ugh!  it’s like they’ve become a cable company.


even worse is how they have been blocking mail from grapheneos’s discussion form.  see their thread about it.  in short, graphene’s mail server has a clean reputation and mailbox has no problem with it.  but because one of graphene’s ipv6 webservers has an ipv6 allocation of a size mailbox doesn’t like, they drop mail.  it’s ridiculous, and mailbox was dismissive of this well-known and well-regarded open source project.  so if the likes of grapheneos can’t get good faith effort to deliver their mail, what chance does an individual like me have?


i’m sorry, but i’m literally paying you to deliver email for me and provide good support.  if you can’t do either of those, you don’t get to push that problem down onto me.  hire appropriate staff.


on the technical side, they support aliases, my own clients, and have a reasonable privacy policy.  their 2fa support exists for the web interface (though weirdly not the support site), though it is somewhat weird.  they do not support app passwords.


a somewhat unique feature is the @secure.mailbox.org domain.  if you try to receive mail at that address, mailbox.org will block it unless it uses tls.  same for sending.  this isn’t e2ee, but it does at least require things not be in plaintext for the last hop to mailbox.


verdict: not recommended due to poor reliability and support.


mailbox.org summary:


website: https://mailbox.org/en/
reliability: iffy due to over-aggressive spam filtering
support: poor; takes 4-6 days for a reply and replies are unhelpful
individual access passwords: no
2fa: yes, but with a pin instead of a password as the other factor
filtering: full sieve feature set and gui editor
spam settings: greylisting on/off, reject some/all spam, etc.  but they’re insufficient to address mailbox’s overzealousness, which support says i cannot workaround within the interface.
server storage location: germany
plan as reviewed: standard [pricing link]

cost per year: eur 30 (about $33)
mail storage included: 10gb
limits on send/receive volume: none
aliases: 50 on your domain name, 25 on mailbox.org
additional mailboxes: available; each one at the same fee as the primary mailbox





startmail


i really wanted to like startmail.  its “vault” is an interesting idea and should contribute to the security and privacy of an account.  they clearly care about privacy.


it falls down in filtering.  they have no way to filter on envelope recipient (bcc or similar).  their support confirmed this to me and that’s a showstopper.


startmail support was also as slow as mailbox, taking 5 days to respond to me.


two showstoppers right there.


verdict: not recommended due to slow support responsiveness and weak filtering.


startmail summary:


website: https://www.startmail.com/
reliability: seems to be fine
support: mediocre; took 5 days for a reply, but the reply was helpful
individual app access passwords: yes
2fa: yes
filtering: poor; cannot filter on envelope recipient, and can’t build filters with multiple actions
spam settings: none
server storage location: the netherlands
plan as reviewed: custom domain (trial was personal), [pricing link]

cost per year: $70
mail storage included: 20gb
limits on send/receive volume: none
aliases: unlimited, with lots of features: can set expiration, etc.
additional mailboxes: not available





kolab


kolab now is mainly positioned as a full groupware service, but they do have a email-only option which i investigated.  there isn’t much documentation about it compared to other providers, and also not much in the way of settings.  you can turn greylisting on or off.  and…. that’s it.


it has a full suite of filtering options.  they set an x-envelope-to header which you can use with the arbitrary header match to do the right thing even for bcc situations.  filters can have multiple conditions and multiple actions.  it is sieve-based and you can download your sieve definitions.


if you enable 2fa, you disable imap and smtp; not great.


verdict: not an impressive enough email featureset to justify going with it.


kolab now summary:


website: https://kolabnow.com/
reliability: seems to be fine
support: fine responsiveness (next day)
invidiaul app passwords: no
2fa: yes, but if you enable it, they disable imap and smtp
filtering: excellent
spam settings: only greylisting on/off
server storage location: switzerland; they have lots of details on their setup
plan as reviewed: “just email” [pricing link]

cost per year: chf 60, about $66
mail storage included: 5gb
limitations on send/receive volume: none
aliases: yes.  not sure if there are limits.
additional mailboxes: yes if you set up a group account.  “flexible pricing based on user count” is not documented anywhere i could find.





mailfence


mailfence is another option, somewhat similar to startmail but without the unique vault.  i had some questions about filters, and support was quite responsive, responding in a couple of hours.


some of their copy on their website is a bit misleading, but support clarified when i asked them.  they do not offer encryption at rest (like most of the entries here).


mailfence’s filtering system is the kind i’d like to see.  it allows multiple conditions and multiple actions for each rule, and has some unique actions as well (notify by sms or xmpp).  support says that “recipients” matches envelope recipients.  however, one ommission is that i can’t match on arbitrary headers; only the canned list of headers they provide.


they have only two spam settings:


spam filter on/off
whitelist


given some recent complaints about their spam filter being overly aggressive, i find this lack of control somewhat concerning.  (however, i discount complaints about people begging for more features in free accounts; free won’t provide the kind of service i’m looking for with any provider.)  there are generally just very few settings for email as well.


verdict: response and helpful support, filtering has the right structure but lacks arbitrary header match.  could be a good option.


mailfence summary:


website: https://mailfence.com/
reliability: seems to be fine
support: excellent responsiveness and helpful replies (after some initial confusion about my question of greylisting)
individual app access passwords: no.  you can set a per-service password (eg, an imap password), but those will be shared with all devices speaking that protocol.
2fa: yes
filtering: good; only misses the ability to filter on arbitrary headers
spam settings: very few
server storage location: belgium
plan as reviewed: entry [pricing link]

cost per year: $42
mail storage included: 10gb, with a maximum of 50,000 messages
limits on send/receive volume: none
aliases: 50.  aliases can’t be deleted once created (there may be an exeption to this for aliases on your own domain rather than mailfence.com)
additional mailboxes: their page on this is a bit confusing, and the pricing page lacks the information promised.  it looks like you can pay the same $42/year for additional mailboxes, with a limit of up to 2 additional paid mailboxes and 2 additional free mailboxes tied to the account.





runbox


this one came recommended in a mastodon thread.  i had some questions about it, and support response was fantastic – i heard from two people that were co-founders of the company!  even within hours, on a weekend.  incredible!  this kind of response was only surpassed by migadu.


i initially wrote to runbox with questions about the incoming and outgoing message limits, which i hadn’t seen elsewhere, as well as the bandwidth limit.  they said the bandwidth limit is no longer enforced on paid accounts.  the incoming and outgoing limits are enforced, and all email (even spam) counts towards the limit.  notably the outgoing limit is per recipient, so if you send 10 messages to your 50-recipient family group, that’s the limit.  however, they also indicated a willingness to reset the limit if something happens.  unfortunately, hitting the limit results in a hard bounce (smtp 5xx) rather than a temporary failure (smtp 4xx) so it can result in lost mail.  this means i’d be worried about some attack or other weirdness causing me to lose mail.


their filter is a pain point.  here are the challenges:


you can’t directly match on a bcc recipient.  support advised to use a “headers” match, which will search for something anywhere in the headers.  this works and is probably “good enough” since this data is in the received: headers, but it is a little more imprecise.
they only have a “contains”, not an “equals” operator.  so, for instance, a pattern searching for “test@example.com” would also match “newtest@example.com”.  support advised to put the email address in angle brackets to avoid this.  that will work… mostly.  angle brackets aren’t always required in headers.
there is no way to have multiple actions on the filter (there is just no way to file an incoming message into two folders).  this was the ultimate showstopper for me.


support advised they are planning to upgrade the filter system in the future, but these are the limitations today.


verdict: a good option if you don’t need much from the filtering system.  lots of privacy emphasis.


runbox summary:


website: https://runbox.com/
reliability: seems to be fine, except returning 5xx codes if per-day limits are exceeded
support: excellent responsiveness and replies from founders
individual app passwords: yes
2fa: yes
filtering: poor
spam settings: very few
server storage location: norway
plan as reviewed: mini [pricing link]

cost per year: $35
mail storage included: 10gb
limited on send/receive volume: receive 5000 messages/day, send 500 recipients/day
aliases: 100 on runbox.com; unlimited on your own domain
additional mailboxes: $15/yr each, also with 10gb non-shared storage per mailbox





fastmail


fastmail came recommended to me by a friend i’ve known for decades.


here’s the thing about fastmail, compared to all the services listed above: it all just works.  everything.  filtering, spam prevention, it is all there, all feature-complete, and all just does the right thing as you’d hope.  their filtering system has a canned dropdown for “to/cc/bcc”, it supports multiple conditions and multiple actions, and just does the right thing.  (delivering to multiple folders is a little cumbersome but possible.)  it has a particularly strong feature set around administering multiple accounts, including things like whether users can prevent admins from reading their mail.


the not-so-great part of the picture is around privacy.  fastmail is based in australia, where the government has extensive power around spying on data, even to the point of forcing companies to add wiretap capabilities.  fastmail’s privacy policy states user data may be held in australia, usa, india, and netherlands.  by default, they share data with unidentified “spam companies”, though you can disable this in settings.  on the other hand, they do make a good effort towards privacy.


i contacted support with some questions and got back a helpful response in three hours.  however, one of the questions was about in which countries my particular data would be stored, and the support response said they would have to get back to me on that.  it’s been several days and no word back.


verdict: a featureful option that “just works”, with a lot of features for managing family accounts and the like, but lacking in the privacy area.


fastmail summary:


website: https://www.fastmail.com/
reliability: seems to be fine
support: good response time on most questions; dropped the ball on one tha trequired research
individual app access passwords: yes
2fa: yes
filtering: excellent
spam settings: can set filter aggressiveness, decide whether to share spam data with “spam-fighting companies”, configure how to handle backscatter spam, and evaluate the personal learning filter.
server storage locations: australia, usa, india, and the netherlands.  legal jurisdiction is australia.
plan as reviewed: individual [pricing link]

cost per year: $60
mail storage included: 50gb
limits on send/receive volume: 300/hour
aliases: unlimited from what i can see
additional mailboxes: no; requires a different plan for that





migadu

migadu was a service i’d never heard of, but came recommended to me on mastodon.


i listed migadu last because it is a class of its own compared to all the other options.  every other service is basically a webmail interface with a few extra settings tacked on.


migadu has a full-featured email admin console in addition.  by that i mean you can:


view usage graphs (incoming, outgoing, storage) over time
manage dns (if you want migadu to run your nameservers)
manage multiple domains, and cross-domain relationships with mailboxes
view a limited set of logs
configure accounts, reset their passwords if needed/authorized, etc.
configure email address rewrite rules with wildcards and so forth


basically, if you were the sort of person that ran your own mail servers back in the day, here is migadu giving you most of that functionality.  effectively you have a web interface to do all the useful stuff, and they handle the boring and annoying bits.  this is a really attractive model.


migadu support has been fantastic.  they are quick to respond, and went above and beyond.  i pointed out that their x-envelope-to header, which is needed for filtering by bcc, wasn’t being added on emails i sent myself.  they replied 5 hours later indicating they had added the feature to add x-envelope-to even for internal mails!  wow!  i am impressed.


with migadu, you buy a pool of resources: storage space and incoming/outgoing traffic.  what you do within that pool is up to you.  you can set up users (“mailboxes”), aliases, domains, whatever you like.  it all just shares the pool.  you can restrict users further so that an individual user has access to only a subset of the pool resources.


i was initially concerned about migadu’s daily send/receive message count limits, but in visiting with support and reading the documentation, what really comes out is that migadu is a service with a personal touch.  hitting the incoming traffic limit will cause a smtp temporary fail (4xx) response so you won’t lose legit mail – and support will work with you if it’s a problem for legit uses.  in other words, restrictions are “soft” and they are interpreted reasonably.


one interesting thing about migadu is that they do not offer accounts under their domain.  that is, you must bring your own domain.  that’s pretty easy and cheap, of course.  it also puts you in a position of power, because it is easy to migrate email from one provider to another if you own the domain.


filtering is done via sieve.  there is a gui editor which lets you accomplish most things, though it has an odd blind spot where you can’t file a message into multiple folders.  however, you can edit a sieve ruleset directly and you get the full sieve featureset, which is extensive (and does support filing a message into multiple folders).  i note that the sieve :envelope match doesn’t work, but migadu adds an x-envelope-to header which is just as good.


i particularly love a company that tells you all the reasons you might not want to use them.  migadu’s pro/con list is an honest drawbacks list (of course, their homepage highlights all the features!).


verdict: fantastically powerful, excellent support, and good privacy.  i chose this one.


migadu summary:


website: https://migadu.com/
reliability: excellent
support: fantastic.  good response times and they added a feature (or fixed a bug?) a few hours after i requested it.
individual access passwords: yes.  create “identities” to support them.
2fa: yes, on both the admin interface and the webmail interface
filtering: excellent, based on sieve.  gui editor doesn’t support multiple actions when filing into a folder, but full sieve functionality is exposed.
spam settings:

on the domain level, filter aggressiveness, greylisting on/off, black and white lists
on the mailbox level, filter aggressiveness, black and whitelists, action to take with spam; compatible with filters.

server storage location: france; legal jurisdiction switzerland
plan as reviewed: mini [pricing link]

cost per year: $90
mail storage included: 30gb (“soft” quota)
limits on send/receive volume: 1000 messgaes in/day, 100 messages out/day (“soft” quotas)
aliases: unlimited on an unlimited number of domains
additional mailboxes: unlimited and free; uses pooled quotas, but individual quotas can be set





others


here are a few others that i didn’t think worthy of getting a trial:


mxroute was recommended by several.  lots of concerning things in their policy, such as:

if you repeatedly send mail to unroutable recipients, they may publish the addresses on github
they will terminate your account if they think you are “rude” or want to contest a charge
they reserve the right to cancel your service at any time for any (or no) reason.

proton keeps coming up, and i will not consider it so long as i am locked into their client on mobile.
skiff comes up sometimes, but they were acquired by notion.
disroot comes up; this discussion highlights a number of reasons why i avoid them.  their terms of service (tos) is inconsistent with a general-purpose email account (i guess for targeting nonprofits and activists, that could make sense).  particularly laughable is that they claim to be friends of open source, but then would take down your account if you upload “copyrighted” material.  news flash: in order for an open source license to be meaningful, the underlying work is copyrighted.  it is perfectly legal to upload copyrighted material when you wrote it or have the license to do so!




conclusions


there are a lot of good options for email hosting today, and in particular i appreciate the excellent personal support from companies like migadu and runbox.  support small businesses!





view all 28 comments 





photographic comparison: is the kobo libra colour display worse than the kobo libra 2?


may 7, 2024hardware, reviewsebooks, ereaders, kobojohn goerzen 


i’ve been using e ink-based ereaders for quite a number of years now.  i’ve had my kobo libra 2 for a few years, and was looking forward to the kobo libra colour — the first color e ink display in a mainstream ereader line.
i found the display to be a mixed bag; contrast seemed a lot worse on b&w images, and the device “backlight” (it’s not technically a “back” light) seemed to cause a particular contrast reduction in dark mode.  i went searching for information on this.  i found a lot of videos on “kobo libra 2 vs libra colour” and so forth, but they were all pretty much useless.  these were the mistakes they made:

being videos.  photos would show the differences in better detail.
shooting videos with cameras with automatic light levels.  since the thing we’re trying to evaluate here is how much darker the kobo libra colour screen is than the kobo libra screen, having a camera that automatically adjusts for brighter or darker images defeats the purpose.  cell phone cameras (still and video) all do this by default and i saw evidence of it in all the videos.
placing the two devices side-by-side instead of in identical locations for subsequent shots.  this led to different shadows on each device (because of course the people shooting videos had to have their phone and head between the light source and the device), again preventing a good comparison.

so i dug out my canon dslr, tripod, and set up shots.  every shot here is set at iso 100.  every shot in the same setting has the same exposure settings, which i document.  the one thing i forgot to shut off was automatic white balance; you can notice it is active if you look closely at the backgrounds, but wb isn’t really relevant to this comparison anyhow.
because there has also been a lot of concern about how well fine b&w details will show up on the kobo libra colour screen, i shot all photos using a pdf test image from the open source hplip package (testpage.ps.gz converted to pdf).  this also rules out font differences between the devices.  i ensured a full screen refresh before each shot.
this is all because color e ink is effectively a filter called kaleido over the b&w layer.  this causes dimming and some other visual effects.
you can click on any image here to see a full-resolution view.  the full-size images are the exact jpeg coming from the camera, with only two modifications: 1) metadata has been redacted for privacy reasons, and 2) some images were losslessly rotated after the shoot.
ok, onwards!
outdoors, bright sun, shot from directly overhead
bright sun is ideal lighting for an e ink display.  they need no lighting at all in this scenario, and in fact, if you turn on their internal display light, it will probably not be very noticeable.  of course, this is in contrast to phone lcd screens, for which bright sunlight is the worst.
scene: morning sunlight reaching the ereaders at an angle.  the angle was sufficient so that no shadows were cast by the camera or tripod.
device light: off on both
exposure: 1/160, f16, iso 100
 
you can see how much darker the libra colour is here.  though in these bright conditions, it is still plenty bright.  there may actually be situations in which the libra 2 is too bright in direct sunlight, requiring a person to squint or whatnot.
looking at the radial lines, it is a bit difficult to tell because the difference in brightness, but i don’t see a hugely obvious reduction in quality in the libra 2.  later i have a shot where i try to match brightness, and we’ll check it out again there.
outdoors, shade, shot from directly overhead
for the next shot, i set the ereaders in shade, but still well-lit with the diffuse sunlight from all around.
the first two have both device lights off.  for the third, i set the device light on the kobo colour to 100%, full cool shade, to try to see how close i could get it to the libra 2 brightness.  (sorry it looks like i forgot to close the toolbar on the colour for this set, but it doesn’t modify the important bits of the underlying image.)
device light: initially off on both
exposure: 1/60, f6.4, iso 100
  
here you can see the light on the libra colour was nearly able to match the brightness on the libra 2.
indoors, room lit with overhead and window light, device light off
we continue to move into dimmer light with this next shot.
device light: off on both
exposure: 1/4, f5, iso 100
 
indoors, room lit with overhead and window light, device light on
now we have the first head-to-head with the device light on.  i set the libra 2 to my favorite warmth setting, found a brightness that looked good, and then tried my best to match those settings on the libra colour.  my camera’s light meter aided in matching brightness.
device light: on (libra 2 at 40%, libra colour at 59%)
exposure: 1/8, f5, iso 100
(apparently i am terrible at remembering to dismiss menus, sigh.)
 
indoors, dark room, dark mode, at an angle
the kobo libra colour surprised me with its dark mode.  when viewed at an oblique angle, the screen gets pretty washed out.  i maintained the same brightness settings here as i did above.  it is much more noticeable when the brightness is set down to my preferred nighttime level (4%), or with a more significant angle.
since you can’t see my tags, the order of the photos here will be: libra 2 (standard orientation), colour (standard orientation), colour (turned around.
device light: on (as above)
exposure: 1/4, f5.6, iso 100
  
notice how i said i maintained the same brightness settings as before, and yet the libra colour looks brighter than the libra 2 here, whereas it looked the same in the prior non-dark mode photos.  here’s why.  i set the exposure of each set of shots based on camera metering.  as we have seen from the light-off photos, the brightness of a white pixel is a lot less on a libra colour than on the libra 2.  however, it is likely that the brightness of a black pixel is about that same.  therefore, contrast on the libra colour is lower than on the libra 2.  the traditional shot is majority white pixels, so to make the libra colour brightness match that of the libra 2, i had to crank up the brightness on the libra colour to compensate for the darker “white” background.  with me so far?
now with the inverted image, you can see what that does.  it doesn’t just raise the brightness of the white pixels, but it also raises the brightness of the black pixels.  this is expected because we didn’t raise contrast, only brightness.
also, in the last image, you can see it is brighter to the right.  again, other conditions that are more difficult to photograph make that much more pronounced.  viewing the libra colour from one side (but not the other), in dark mode, with the light on, produces noticeably worse contrast on one side.
conclusions
this isn’t a slam dunk.  let’s walk through this:
i don’t think there is any noticeable loss of detail on the libra colour.  the radial lines appeared as well defined on it as on the libra 2.  oddly, with the backlight, some striations were apparent in the gray gradient test, but i wouldn’t be using an e ink device for clear photographic reproduction anyhow.
if you read mostly black and white: if you had been using a kobo libra colour and were handed a libra 2, you would go, “wow!  what an upgrade!  the screen is so much brighter!”  there’s little reason to get a libra colour.  the libra 2 might be hard to find these days, but the new clara bw (with a 6″ instead of the 7″ screen on the libra series) might be just the thing for you.  the libra 2 is at home in any lighting, from direct sun to pitch black, and has all the usual e ink benefits (eg, battery life measured in weeks) and drawbacks (slower refresh rate) that we’re all used to.
if you are interested in photographic color reproduction mostly indoors: consider a small tablet.  the libra colour’s 4096 colors are going to appear washed out compared to what you’re used to on a lcd screen.
if you are interested in color content indoors and out: the libra colour might be a good fit.  it could work well for things where superb color rendition isn’t essential — for instance, news stories (the pocket integration or calibre’s news feature could be nice there), comics, etc.
in a moderately-lit indoor room, it looks like the libra colour’s light can lead it to results that approach libra 2 quality.  so if most of your reading is in those conditions, perhaps the libra colour is right for you.
as a final aside, i wrote in this article about the kobo devices.  i switched from kindles to kobos a couple of years ago due to the greater openness of the kobo devices (you can add things like nickel menu and koreader to them, and they have built-in support for more useful formats), their featureset, and their cost.  the top-of-the-line kindle devices will have a screen very similar if not identical to the libra 2, so you can very easily consider this to be a comparison between the oasis and the libra colour as well.



view all 10 comments 





facebook is censoring stories about climate change and illegal raid in marion, kansas


april 6, 2024uncategorizedjohn goerzen 


it is, sadly, not entirely surprising that facebook is censoring articles critical of meta.
the kansas reflector published an artical about meta censoring environmental articles about climate change — deeming them “too controversial”.
facebook then censored the article about facebook censorship, and then after an independent site published a copy of the climate change article, facebook censored it too.
the cnn story says facebook apologized and said it was a mistake and was fixing it.
color me skeptical, because today i saw this:

yes, that’s right: today, april 6, i get a notification that they removed a post from august 12.  the notification was dated april 4, but only showed up for me today.
i wonder why my post from august 12 was fine for nearly 8 months, and then all of a sudden, when the same website runs an article critical of facebook, my 8-month-old post is a problem.  hmm.

riiiiiight.  cybersecurity.
this isn’t even the first time they’ve done this to me.
on september 11, 2021, they removed my post about the social network mastodon (click that link for screenshot).  a post that, incidentally, had been made 10 months prior to being removed.
while they ultimately reversed themselves, i subsequently wrote facebook’s blocking decisions are deliberate — including their censorship of mastodon.
that this same pattern has played out a second time — again with something that is a very slight challenege to facebook — seems to validate my conclusion.  facebook lets all sort of hateful garbage infest their site, but anything about climate change — or their own censorship — gets removed, and this pattern persists for years.
there’s a reason i prefer mastodon these days.  you can find me there as @jgoerzen@floss.social.
so.  i’ve written this blog post.  and then i’m going to post it to facebook.  let’s see if they try to censor me for a third time.  bring it, facebook.



view all 5 comments 





the xz issue isn’t about open source


april 4, 2024society, softwareflossjohn goerzen 


you’ve probably heard of the recent backdoor in xz.  there have been a lot of takes on this, most of them boiling down to some version of:
the problem here is with open source software.
i want to say not only is that view so myopic that it pushes towards the incorrect, but also it blinds us to more serious problems.
now, i don’t pretend that there are no problems in the floss community.  there have been various pieces written about what this issue says about the floss community (usually without actionable solutions).  i’m not here to say those pieces are wrong.  just that there’s a bigger picture.
so with this xz issue, it may well be a state actor (aka “spy”) that added this malicious code to xz.  we also know that proprietary software and systems can be vulnerable.  for instance, a twitter whistleblower revealed that twitter employed indian and chinese spies, some knowingly.  a recent report pointed to security lapses at microsoft, including “preventable” lapses in security.  according to the wikipedia article on the solarwinds attack, it was facilitated by various kinds of carelessness, including passwords being posted to github and weak default passwords.  they directly distributed malware-infested updates, encouraged customers to disable anti-malware tools when installing solarwinds products, and so forth.
it would be naive indeed to assume that there aren’t black hat actors among the legions of programmers employed by companies that outsource work to low-cost countries — some of which have challenges with bribery.
so, given all this, we can’t really say the problem is open source.  maybe it’s more broad:
the problem here is with software.
maybe that inches us closer, but is it really accurate?  we have all heard of boeing’s recent issues, which seem to have some element of root causes in corporate carelessness, cost-cutting, and outsourcing.  that sounds rather similar to the solarwinds issue, doesn’t it?
well then, the problem is capitalism.
maybe it has a role to play, but isn’t it a little too easy to just say “capitalism” and throw up our hands helplessly, just as some do with floss as at the start of this article?  after all, capitalism also brought us plenty of products of very high quality over the years.  when we can point to successful, non-careless products — and i own some of them (for instance, my framework laptop).  we clearly haven’t reached the root cause yet.
and besides, what would you replace it with?  all the major alternatives that have been tried have even stronger downsides.  maybe you replace it with “better regulated capitalism”, but that’s still capitalism.
then the problem must be with consumers.
as this argument would go, it’s consumers’ buying patterns that drive problems.  buyers — individual and corporate — seek flashy features and low cost, prizing those over quality and security.
no doubt this is true in a lot of cases.  maybe greed or status-conscious societies foster it: temu promises people to “shop like a billionaire”, and unloads on them cheap junk, which “all but guarantees that shipments from temu containing products made with forced labor are entering the united states on a regular basis“.
but consumers are also people, and some fraction of them are quite capable of writing fantastic software, and in fact, do so.
so what we need is some way to seize control.  some way to do what is right, despite the pressures of consumers or corporations.
ah yes, dear reader, you have been slogging through all these paragraphs and now realize i have been leading you to this:
then the solution is open source.
indeed.  faults and all, floss is the most successful movement i know where people are bringing us back to the commons: working and volunteering for the common good, unleashing a thousand creative variants on a theme, iterating in every direction imaginable.  we have floss being vital parts of everything from $30 raspberry pis to space missions.  it is bringing education and communication to impoverished parts of the world.  it lets everyone write and release software.  and, unlike the solarwinds and twitter issues, it exposes both clever solutions and security flaws to the world.
if an authentication process in windows got slower, we would all shrug and mutter “microsoft” under our breath.  because, really, what else can we do?  we have no agency with windows.
if an authentication process in linux gets slower, anybody that’s interested — anybody at all — can dive in and ask “why” and trace it down to root causes.
some look at this and say “floss is responsible for this mess.”  i look at it and say, “this would be so much worse if it wasn’t floss” — and experience backs me up on this.
floss doesn’t prevent security issues itself.
what it does do is give capabilities to us all.  the ability to investigate.  ability to fix.  yes, even the ability to break — and its cousin, the power to learn.
and, most rewarding, the ability to contribute.



view all 9 comments 





live migrating from raspberry pi os bullseye to debian bookworm


january 3, 2024linuxraspberry pijohn goerzen 


i’ve been getting annoyed with raspberry pi os (raspbian) for years now.  it’s a fork of debian, but manages to omit some of the most useful things.  so i’ve decided to migrate all of my pis to run pure debian.  these are my reasons:

raspberry pi os has, for years now, specified that there is no upgrade path.  that is, to get to a newer major release, it’s a reinstall.  while i have sometimes worked around this, for a device that is frequently installed in hard-to-reach locations, this is even more important than usual.  it’s common for me to upgrade machines for a decade or more across debian releases and there’s no reason that it should be so much more difficult with raspbian.
as i noted in consider security first, the security situation for raspberry pi os isn’t as good as it is with debian.
raspbian lags behind debian – often times by 6 months or more for major releases, and days or weeks for bug fixes and security patches.
raspbian has no direct backports support, though raspberry pi 3 and above can use debian’s backports (per my instructions as installing debian backports on raspberry pi)
raspbian uses a custom kernel without initramfs support

it turns out it is actually possible to do an in-place migration from raspberry pi os bullseye to debian bookworm.  here i will describe how.  even if you don’t have a raspberry pi, this might still be instructive on how raspbian and debian packages work.
warnings
before continuing, back up your system.  this process isn’t for the neophyte and it is entirely possible to mess up your boot device to the point that you have to do a fresh install to get your pi to boot.  this isn’t a supported process at all.
architecture confusion
debian has three arm-based architectures:

armel, for the lowest-end 32-bit arm devices without hardware floating point support
armhf, for the higher-end 32-bit arm devices with hardware float (hence “hf”)
arm64, for 64-bit arm devices (which all have hardware float)

although the raspberry pi 0 and 1 do support hardware float, they lack support for other cpu features that debian’s armhf architecture assumes.  therefore, the raspberry pi 0 and 1 could only run debian’s armel architecture.
raspberry pi 3 and above are capable of running 64-bit, and can run both armhf and arm64.
prior to the release of the raspberry pi 5 / raspbian bookworm, raspbian only shipped the armhf architecture.  well, it was an architecture they called armhf, but it was different from debian’s armhf in that everything was recompiled to work with the more limited set of features on the earlier raspberry pi boards.  it was really somewhere between debian’s armel and armhf archs.  you could run debian armel on those, but it would run more slowly, due to doing floating point calculations without hardware support.  debian’s raspi faq goes into this a bit.
what i am going to describe here is going from raspbian armhf to debian armhf with a 64-bit kernel.  therefore, it will only work with raspberry pi 3 and above.  it may theoretically be possible to take a raspberry pi 2 to debian armhf with a 32-bit kernel, but i haven’t tried this and it may be more difficult.  i have seen conflicting information on whether armhf really works on a pi 2.  (if you do try it on a pi 2, ignore everything about arm64 and 64-bit kernels below, and just go with the linux-image-armmp-lpae kernel per the armmp page)
there is another wrinkle: debian doesn’t support running 32-bit arm kernels on 64-bit arm cpus, though it does support running a 32-bit userland on them.  so we will wind up with a system with kernel packages from arm64 and everything else from armhf.  this is a perfectly valid configuration as the arm64 – like x86_64 – is multiarch (that is, the cpu can natively execute both the 32-bit and 64-bit instructions).
(it is theoretically possible to crossgrade a system from 32-bit to 64-bit userland, but that felt like a rather heavy lift for dubious benefit on a pi; nevertheless, if you want to make this process even more complicated, refer to the crossgrading page.)
prerequisites and limitations
in addition to the need for a raspberry pi 3 or above in order for this to work, there are a few other things to mention.
if you are using the gpio features of the pi, i don’t know if those work with debian.
i think raspberry pi os modified the desktop environment more than other components.  all of my pis are headless, so i don’t know if this process will work if you use a desktop environment.
i am assuming you are booting from a microsd card as is typical in the raspberry pi world.  the pi’s firmware looks for a fat partition (mbr type 0x0c) and looks within it for boot information.  depending on how long ago you first installed an os on your pi, your /boot may be too small for debian.  use df -h /boot to see how big it is.  i recommend 200mb at minimum.  if your /boot is smaller than that, stop now (or use some other system to shrink your root filesystem and rearrange your partitions; i’ve done this, but it’s outside the scope of this article.)
you need to have stable power.  once you begin this process, your pi will mostly be left in a non-bootable state until you finish.  (you… did make a backup, right?)
basic idea
the basic idea here is that since bookworm has almost entirely newer packages then bullseye, we can “just” switch over to it and let the debian packages replace the raspbian ones as they are upgraded.  well, it’s not quite that easy, but that’s the main idea.
preparation
first, make a backup.  even an image of your microsd card might be nice.  ok, i think i’ve said that enough now.
it would be a good idea to have a hdmi cable (with the appropriate size of connector for your particular pi board) and a hdmi display handy so you can troubleshoot any bootup issues with a console.
preparation: access
the raspberry pi os by default sets up a user named pi that can use sudo to gain root without a password.  i think this is an insecure practice, but assuming you haven’t changed it, you will need to ensure it still works once you move to debian.  raspberry pi os had a patch in their sudo package to enable it, and that will be removed when debian’s sudo package is installed.  so, put this in /etc/sudoers.d/010_picompat:

pi all=(all) nopasswd: all


also, there may be no password set for the root account.  it would be a good idea to set one; it makes it easier to log in at the console.  use the passwd command as root to do so.
preparation: bluetooth
debian doesn’t correctly identify the bluetooth hardware address.  you can save it off to a file by running hcitool dev > /root/bluetooth-from-raspbian.txt.  i don’t use bluetooth, but this should let you develop a script to bring it up properly.
preparation: debian archive keyring
you will next need to install debian’s archive keyring so that apt can authenticate packages from debian.  go to the bookworm download page for debian-archive-keyring and copy the url for one of the files, then download it on the pi.  for instance:

wget http://http.us.debian.org/debian/pool/main/d/debian-archive-keyring/debian-archive-keyring_2023.3+deb12u1_all.deb


use sha256sum to verify the checksum of the downloaded file, comparing it to the package page on the debian site.
now, you’ll install it with:

dpkg -i debian-archive-keyring_2023.3+deb12u1_all.deb


package first steps
from here on, we are making modifications to the system that can leave it in a non-bootable state.
examine /etc/apt/sources.list and all the files in /etc/apt/sources.list.d.  most likely you will want to delete or comment out all lines in all files there.  replace them with something like:

deb http://deb.debian.org/debian/ bookworm main non-free-firmware contrib non-free
deb http://security.debian.org/debian-security bookworm-security main non-free-firmware contrib non-free
deb https://deb.debian.org/debian bookworm-backports main non-free-firmware contrib non-free


(you might leave off contrib and non-free depending on your needs)
now, we’re going to tell it that we’ll support arm64 packages:

dpkg --add-architecture arm64


and finally, download the bookworm package lists:

apt-get update


if there are any errors from that command, fix them and don’t proceed until you have a clean run of apt-get update.
moving /boot to /boot/firmware
the boot fat partition i mentioned above is mounted at /boot by raspberry pi os, but debian’s scripts assume it will be at /boot/firmware.  we need to fix this.  first:

umount /boot
mkdir /boot/firmware


now, edit fstab and change the reference to /boot to be to /boot/firmware.  now:

mount -v /boot/firmware
cd /boot/firmware
mv -vi * ..


this mounts the filesystem at the new location, and moves all its contents back to where apt believes it should be.  debian’s packages will populate /boot/firmware later.
installing the first packages
now we start by installing the first of the needed packages.  eventually we will wind up with roughly the same set debian uses.

apt-get install linux-image-arm64
apt-get install firmware-brcm80211=20230210-5
apt-get install raspi-firmware


if you get errors relating to firmware-brcm80211 from any commands, run that install firmware-brcm80211 command and then proceed.  there are a few packages that raspbian marked as newer than the version in bookworm (whether or not they really are), and that’s one of them.
configuring the bootloader
we need to configure a few things in /etc/default/raspi-firmware before proceeding.  edit that file.
first, uncomment (or add) a line like this:

kernel_arch="arm64"


next, in /boot/cmdline.txt you can find your old raspbian boot command line.  it will say something like:

root=partuuid=...


save off the bit starting with partuuid.  back in /etc/default/raspi-firmware, set a line like this:

rootpart=partuuid=abcdef00


(substituting your real value for abcdef00).
this is necessary because the microsd card device name often changes from /dev/mmcblk0 to /dev/mmcblk1 when switching to debian’s kernel.  raspi-firmware will encode the current device name in /boot/firmware/cmdline.txt by default, which will be wrong once you boot into debian’s kernel.  the partuuid approach lets it work regardless of the device name.
purging the raspbian kernel
run:

dpkg --purge raspberrypi-kernel


upgrading the system
at this point, we are going to run the procedure beginning at section 4.4.3 of the debian release notes.  generally, you will do:

apt-get -u upgrade
apt full-upgrade


fix any errors at each step before proceeding to the next.  now, to remove some cruft, run:

apt-get --purge autoremove


inspect the list to make sure nothing important isn’t going to be removed.
removing raspbian cruft
you can list some of the cruft with:

apt list '~o'


and remove it with:

apt purge '~o'


i also don’t run bluetooth, and it seemed to sometimes hang on boot becuase i didn’t bother to fix it, so i did:

apt-get --purge remove bluez


installing some packages
this makes sure some basic debian infrastructure is available:

apt-get install wpasupplicant parted dosfstools wireless-tools iw alsa-tools
apt-get --purge autoremove


installing firmware
now run:

apt-get install firmware-linux


resolving firmware package version issues
if it gives an error about the installed version of a package, you may need to force it to the bookworm version.  for me, this often happened with firmware-atheros, firmware-libertas, and firmware-realtek.
here’s how to resolve it, with firmware-realtek as an example:


go to https://packages.debian.org/packagename – for instance, https://packages.debian.org/firmware-realtek.  note the version number in bookworm – in this case, 20230210-5.


now, you will force the installation of that package at that version:

apt-get install firmware-realtek=20230210-5




repeat with every conflicting package until done.


rerun apt-get install firmware-linux and make sure it runs cleanly.


also, in the end you should be able to:

apt-get install firmware-atheros firmware-libertas firmware-realtek firmware-linux


dealing with other raspbian packages
the debian release notes discuss removing non-debian packages.  there will still be a few of those.  run:

apt list '?narrow(?installed, ?not(?origin(debian)))'


deal with them; mostly you will need to force the installation of a bookworm version using the procedure in the section resolving firmware package version issues above (even if it’s not for a firmware package).  for non-firmware packages, you might possibly want to add --mark-auto to your apt-get install command line to allow the package to be autoremoved later if the things depending on it go away.
if you aren’t going to use bluetooth, i recommend apt-get --purge remove bluez as well.  sometimes it can hang at boot if you don’t fix it up as described above.
set up networking
we’ll be switching to the debian method of networking, so we’ll create some files in /etc/network/interfaces.d.  first, eth0 should look like this:

allow-hotplug eth0
iface eth0 inet dhcp
iface eth0 inet6 auto


and wlan0 should look like this:

allow-hotplug wlan0
iface wlan0 inet dhcp
    wpa-conf /etc/wpa_supplicant/wpa_supplicant.conf


raspbian is inconsistent about using eth0/wlan0 or renamed interface.  run ifconfig or ip addr.  if you see a long-named interface such as enx<something> or wlp<something>, copy the eth0 file to the one named after the enx interface, or the wlan0 file to the one named after the wlp interface, and edit the internal references to eth0/wlan0 in this new file to name the long interface name.
if using wifi, verify that your ssids and passwords are in /etc/wpa_supplicant/wpa_supplicant.conf.  it should have lines like:

network={
   ssid="networkname"
   psk="passwordhere"
}


(this is where raspberry pi os put them).
deal with dhcp
raspberry pi os used dhcpcd, whereas bookworm normally uses isc-dhcp-client.  verify the system is in the correct state:

apt-get install isc-dhcp-client
apt-get --purge remove dhcpcd dhcpcd-base dhcpcd5 dhcpcd-dbus


set up leds
to set up the leds to trigger on microsd activity as they did with raspbian, follow the debian instructions.  run apt-get install sysfsutils.  then put this in a file at /etc/sysfs.d/local-raspi-leds.conf:

class/leds/act/brightness = 1
class/leds/act/trigger = mmc1


prepare for boot
to make sure all the /boot/firmware files are updated, run update-initramfs -u.  verify that root in /boot/firmware/cmdline.txt references the partuuid as appropriate.  verify that /boot/firmware/config.txt contains the lines arm_64bit=1 and upstream_kernel=1.  if not, go back to the section on modifying /etc/default/raspi-firmware and fix it up.
the moment arrives
cross your fingers and try rebooting into your debian system:

reboot


for some reason, i found that the first boot into debian seems to hang for 30-60 seconds during bootstrap.  i’m not sure why; don’t panic if that happens.  it may be necessary to power cycle the pi for this boot.
troubleshooting
if things don’t work out, hook up the pi to a hdmi display and see what’s up.  if i anticipated a particular problem, i would have documented it here (a lot of the things i documented here are because i ran into them!)  so i can’t give specific advice other than to watch boot messages on the console.  if you don’t even get kernel messages going, then there is some problem with your partition table or /boot/firmware fat partition.  otherwise, you’ve at least got the kernel going and can troubleshoot like usual from there.



view all 12 comments 





consider security first


january 2, 2024softwaresecurityjohn goerzen 


i write this in the context of my decision to ditch raspberry pi os and move everything i possibly can, including my raspberry pi devices, to debian.  i will write about that later.
but for now, i wanted to comment on something i think is often overlooked and misunderstood by people considering distributions or operating systems: the huge importance of getting security updates in an automated and easy way.
background
let’s assume that these statements are true, which i think are well-supported by available evidence:


every computer system (os plus applications) that can do useful modern work has security vulnerabilities, some of which are unknown at any given point in time;


during the lifetime of that computer system, some of these vulnerabilities will be discovered.  for a (hopefully large) subset of those vulnerabilities, timely patches will become available.


now then, it follows that applying those timely patches is a critical part of having a system that it as secure as possible.  of course, you have to do other things as well – good passwords, secure practices, etc – but, fundamentally, if your system lacks patches for known vulnerabilities, you’ve already lost at the security ballgame.
how to stay patched
there is something of a continuum of how you might patch your system.  it runs roughly like this, from best to worst:


all components are kept up-to-date automatically, with no intervention from the user/operator


the operator is automatically alerted to necessary patches, and they can be easily installed with minimal intervention


the operator is automatically alerted to necessary patches, but they require significant effort to apply


the operator has no way to detect vulnerabilities or necessary patches


it should be obvious that the first situation is ideal.  every other situation relies on the timeliness of human action to keep up-to-date with security patches.  this is a fallible situation; humans are busy, take trips, dismiss alerts, miss alerts, etc.  that said, it is rare to find any system living truly all the way in that scenario, as you’ll see.
what is “your system”?
a critical point here is: what is “your system”?  it includes:

your kernel
your base operating system
your applications
all the libraries needed to run all of the above

some oss, such as debian, make little or no distinction between the base os and the applications.  others, such as many bsds, have a distinction there.  and in some cases, people will compile or install applications outside of any os mechanism.  (it must be stressed that by doing so, you are taking the responsibility of patching them on your own shoulders.)
how do common systems stack up?


debian, with its support for unattended-upgrades, needrestart, debian-security-support, and such, is largely category 1.  it can automatically apply security patches, in most cases can restart the necessary services for the patch to take effect, and will alert you when some processes or the system must be manually restarted for a patch to take effect (for instance, a kernel update).  those cases requiring manual intervention are category 2.  the debian-security-support package will even warn you of gaps in the system.  you can also use debsecan to scan for known vulnerabilities on a given installation.


freebsd has no way to automatically install security patches for things in the packages collection.  as with many rolling-release systems, you can’t automate the installation of these security patches with freebsd because it is not safe to blindly update packages.  it’s not safe to blindly update packages because they may bring along more than just security patches: they may represent major upgrades that introduce incompatibilities, etc.  unlike debian’s practice of backporting fixes and thus producing narrowly-tailored patches, forcing upgrades to newer versions precludes a “minimal intervention” install.  therefore, rolling release systems are category 3.


things such as snap, flatpak, appimage, docker containers, electron apps, and third-party binaries often contain embedded libraries and such for which you have no easy visibility into their status.  for instance, if there was a bug in libpng, would you know how many of your containers had a vulnerability?  these systems are category 4 – you don’t even know if you’re vulnerable.  it’s for this reason that my debian-based docker containers apply security patches before starting processes, and also run unattended-upgrades and friends.


the pernicious library problem
as mentioned in my last category above, hidden vulnerabilities can be a big problem.  i’ve been writing about this for years.  back in 2017, i wrote an article focused on docker containers, but which applies to the other systems like snap and so forth.  i cited a study back then that “over 80% of the :latest versions of official images contained at least one high severity vulnerability.”  the situation is no better now.  in december 2023, it was reported that, two years after the critical log4shell vulnerability, 25% of apps were still vulnerable to it.  also, only 21% of developers ever update third-party libraries after introducing them into their projects.
clearly, you can’t rely on these images with embedded libraries to be secure.  and since they are black box, they are difficult to audit.
debian’s policy of always splitting libraries out from packages is hugely beneficial; it allows finegrained analysis of not just vulnerabilities, but also the dependency graph.  if there’s a vulnerability in libpng, you have one place to patch it and you also know exactly what components of your system use it.
if you use snaps, or appimages, you can’t know if they contain a deeply embedded vulnerability, nor could you patch it yourself if you even knew.  you are at the mercy of upstream detecting and remedying the problem – a dicey situation at best.
who makes the patches?
fundamentally, humans produce security patches.  often, but not always, patches originate with the authors of a program and then are integrated into distribution packages.  it should be noted that every security team has finite resources; there will always be some cves that aren’t patched in a given system for various reasons; perhaps they are not exploitable, or are too low-impact, or have better mitigations than patches.
debian has an excellent security team; they manage the process of integrating patches into debian, produce debian security advisories, maintain the debian security tracker (which maintains cross-references with the cve database), etc.
some distributions don’t have this infrastructure.  for instance, i was unable to find this kind of tracker for devuan or raspberry pi os.  in contrast, ubuntu and arch linux both seem to have active security teams with trackers and advisories.
implications for raspberry pi os and others
as i mentioned above, i’m transitioning my pi devices off raspberry pi os (raspbian).  security is one reason.  although raspbian is a fork of debian, and you can install packages like unattended-upgrades on it, they don’t work right because they use the debian infrastructure, and raspbian hasn’t modified them to use their own infrastructure.  i don’t see any raspberry pi os security advisories, trackers, etc.  in short, they lack the infrastructure to support those debian tools anyhow.
not only that, but raspbian lags behind debian in both new releases and new security patches, sometimes by days or weeks.
live migrating from raspberry pi os bullseye to debian bookworm contains instructions for migrating raspberry pis to debian.



view all 15 comments 





the grumpy cricket (and other enormous creatures)


december 25, 2023children & computingjohn goerzen 


this christmas, one of my gifts to my kids was a text adventure (interactive fiction) game for them.  now that they’ve enjoyed it, i’m releasing it under the gpl v3.
as interactive fiction, it’s like an e-book, but the reader is also the player, guiding the exploration of the world.
the grumpy cricket is designed to be friendly for a first-time player of interactive fiction. there is no way to lose the game or to die. there is an in-game hint system providing context-sensitive hints anytime the reader types hint. there are splashes of humor throughout that got all three of my kids laughing.
i wrote it in 2023 for my kids, which range in age from 6 to 17.  that’s quite a wide range, but they all were enjoying it.
you can download it, get the source, or play it online in a web browser at https://www.complete.org/the-grumpy-cricket/



view all 7 comments 





it’s more important to recognize what direction people are moving than where they are


november 13, 2023societyjohn goerzen 


i recently read a post on social media that went something like this (paraphrased):
“if you buy an ev, you’re part of the problem.  you’re advancing car culture and are actively hurting the planet.  the only ethical thing to do is ditch your cars and put all your effort into supporting transit.  anything else is worthless.”
there is some truth there; supporting transit in areas it makes sense is better than having more cars, even evs.  but of course the key here is in areas it makes sense.
my road isn’t even paved.  i live miles from the nearest town.  and get into the remote regions of the western usa and you’ll find people that live 40 miles from the nearest neighbor.  there’s no realistic way that mass transit is ever going to be a thing in these areas.  and even if it were somehow usable, sending buses over miles where nobody lives just to reach the few that are there will be worse than private evs.  and because i can hear this argument coming a mile away, no, it doesn’t make sense to tell these people to just not live in the country because the planet won’t support that anymore, because those people are literally the ones that feed the ones that live in the cities.
the funny thing is: the person that wrote that shares my concerns and my goals.  we both care deeply about climate change.  we both want positive change.  and i, ahem, recently bought an ev.
i have seen this play out in so many ways over the last few years.  drive a car?  get yelled at.  support the wrong politician?  get a shunning.  not speak up loudly enough about the right politician?  that’s a yellin’ too.
the problem is, this doesn’t make friends.  in fact, it hurts the cause.  it doesn’t recognize this truth:
it is more important to recognize what direction people are moving than where they are.
i support trains and transit.  i’ve donated money and written letters to politicians.  but, realistically, there will never be transit here.  people in my county are unable to move all the way to transit.  but what can we do?  plenty.  we bought an ev.  i’ve been writing letters to the board of our local electrical co-op advocating for relaxation of rules around residential solar installations, and am planning one myself.  it may well be that our solar-powered transportation winds up having a lower carbon footprint than the poster’s transit use.
pick your favorite cause.  whatever it is, consider your strategy: what do you do with someone that is very far away from you, but has taken the first step to move an inch in your direction?  do you yell at them for not being there instantly?  or do you celebrate that they have changed and are moving?



view all 6 comments 





how gapped is your air?


september 15, 2023technologyasynchronous, nncp, security, serialjohn goerzen 


sometimes we want better-than-firewall security for things.  for instance:

an industrial control system for a municipal water-treatment plant should never have data come in or out
or, a variant of the industrial control system: it should only permit telemetry and monitoring data out, and nothing else in or out
a system dedicated to keeping your gpg private keys secure should only have material to sign (or decrypt) come in, and signatures (or decrypted data) go out
a system keeping your tax records should normally only have new records go in, but may on occasion have data go out (eg, to print a copy of an old record)

in this article, i’ll talk about the “high side” (the high-security or high-sensitivity systems) and the “low side” (the lower-sensitivity or general-purpose systems).  for the sake of simplicity, i’ll assume the high side is a single machine, but it could as well be a whole network.
let’s focus on examples 3 and 4 to make things simpler.  let’s consider the primary concern to be data exfiltration (someone stealing your data), with a secondary concern of data integrity (somebody modifying or destroying your data).
you might think the safest possible approach is airgapped – that is, there is literal no physical network connection to the machine at all.  this help!  but then, the problem becomes: how do we deal with the inevitable need to legitimately get things on or off of the system?  as i wrote in dead usb drives are fine: building a reliable sneakernet, by using tools such as nncp, you can certainly create a “sneakernet”: using usb drives as transport.
while this is a very secure setup, as with most things in security, it’s less than perfect. the wikipedia airgap article discusses some ways airgapped machines can still be exploited.  it mentions that security holes relating to removable media have been exploited in the past.  there are also other ways to get data out; for instance, debian ships with gensio and minimodem, both of which can transfer data acoustically.
but let’s back up and think about why we think of airgapped machines as so much more secure, and what the failure modes of other approaches might be.
what about firewalls?
you could very easily set up high-side machine that is on a network, but is restricted to only one outbound tcp port.  there could be a local firewall, and perhaps also a special port on an external firewall that implements the same restrictions.  a variant on this approach would be two computers connected directly by a crossover cable, though this doesn’t necessarily imply being more secure.
of course, the concern about a local firewall is that it could potentially be compromised.  an external firewall might too; for instance, if your credentials to it were on a machine that got compromised.  this kind of dual compromise may be unlikely, but it is possible.
we can also think about the complexity in a network stack and firewall configuration, and think that there may be various opportunities to have things misconfigured or buggy in a system of that complexity.  another consideration is that data could be sent at any time, potentially making it harder to detect.  on the other hand, network monitoring tools are commonplace.
on the other hand, it is convenient and cheap.
i use a system along those lines to do my backups.  data is sent, gpg-encrypted and then encrypted again at the nncp layer, to the backup server.  the nncp process on the backup server runs as an untrusted user, and dumps the gpg-encrypted files to a secure location that is then processed by a cron job using filespooler.  the backup server is on a dedicated firewall port, with a dedicated subnet.  the only ports allowed out are for nncp and ntp, and offsite backups.  there is no default gateway.  not even dns is permitted out (the firewall does the appropriate redirection).  there is one pinhole allowed out, where a subset of the backup data is sent offsite.
i initially used usb drives as transport, and it had no network connection at all.  but there were disadvantages to doing this for backups – particularly that i’d have no backups for as long as i’d forget to move the drives.  the backup system also would have clock drift, and the offsite backup picture was more challenging.  (the clock drift was a problem because i use 2fa on the system; a password, plus a totp generated by a yubikey)
this is “pretty good” security, i’d think.
what are the weak spots?  well, if there were somehow a bug in the nncp client, and the remote nncp were compromised, that could lead to a compromise of the nncp account.  but this itself would accomplish little; some other vulnerability would have to be exploited on the backup server, because the nncp account can’t see plaintext data at all.  i use borgbackup to send a subset of backup data offsite over ssh.  borgbackup has to run as root to be able to access all the files, but the ssh it calls runs as a separate user.  a ssh vulnerability is therefore unlikely to cause much damage.  if, somehow, the remote offsite system were compromised and it was able to exploit a security issue in the local borgbackup, that would be a problem.  but that sounds like a remote possibility.
borgbackup itself can’t even be used over a sneakernet since it is not asynchronous.  a more secure solution would probably be using something like dar over nncp.  this would eliminate the ssh installation entirely, and allow a complete isolation between the data-access and the communication stacks, and notably not require bidirectional communication.  logic separation matters too.  my roundup of data backup and archiving tools may be helpful here.
other attack vectors could be a vulnerability in the kernel’s networking stack, local root exploits that could be combined with exploiting nncp or borgbackup to gain root, or local misconfiguration that makes the sandboxes around nncp and borgbackup less secure.
because this system is in my basement in a utility closet with no chairs and no good place for a console, i normally manage it via a serial console.  while it’s a dedicated line between the system and another machine, if the other machine is compromised or an adversary gets access to the physical line, credentials (and perhaps even data) could leak, albeit slowly.
but we can do much better with serial lines.  let’s take a look.
serial lines
some of us remember rs-232 serial lines and their once-ubiquitous db-9 connectors.  traditionally, their speed maxxed out at 115.2kbps.
serial lines have the benefit that they can be a direct application-to-application link.  in my backup example above, a serial line could directly link the nncp daemon on one system with the nncp caller on another, with no firewall or anything else necessary.  it is simply up to those programs to open the serial device appropriately.
this isn’t perfect, however.  unlike tcp over ethernet, a serial line has no inherent error checking.  modern programs such as nncp and ssh assume that a lower layer is making the link completely clean and error-free for them, and will interpret any corruption as an attempt to tamper and sever the connection.  however, there is a solution to that: gensio.  in my page using gensio and ser2net, i discuss how to run nncp and ssh over gensio.  gensio is a generic framework that can add framing, error checking, and retransmit to an unreliable link such as a serial port.  it can also add encryption and authentication using tls, which could be particularly useful for applications that aren’t already doing that themselves.
more traditional solutions for serial communications have their own built-in error correction.  for instance, uucp and kermit both were designed in an era of noisy serial lines and might be an excellent fit for some use cases.  the zmodem protocol also might be, though it offers somewhat less flexibility and automation than kermit.
i have found that certain usb-to-serial adapters by gearmo will actually run at up to 2mbps on a serial line!  look for the ones on their spec pages with a ftdi chipset rated at 920kbps.  it turns out they can successfully be driven faster, especially if gensio’s relpkt is used.  i’ve personally verified 2mbps operation (linux port speed 2000000) on gearmo’s usa-ftdi2x and the usa-ftdi4x.  (i haven’t seen any single-port options from gearmo with the 920kbps chipset, but they may exist).
still, even at 2mbps, speed may well be a limiting factor with some applications.  if what you need is a console and some textual or batch data, it’s probably fine.  if you are sending 500gb backup files, you might look for something else.  in theory, this usb to rs-422 adapter should work at 10mbps, but i haven’t tried it.
but if the speed works, running a dedicated application over a serial link could be a nice and fairly secure option.
one of the benefits of the airgapped approach is that data never leaves unless you are physically aware of transporting a usb stick.  of course, you may not be physically aware of what is on that stick in the event of a compromise.  this could easily be solved with a serial approach by, say, only plugging in the cable when you have data to transfer.
data diodes
a traditional diode lets electrical current flow in only one direction.  a data diode is the same concept, but for data: a hardware device that allows data to flow in only one direction.
this could be useful, for instance, in the tax records system that should only receive data, or the industrial system that should only send it.
wikipedia claims that the simplest kind of data diode is a fiber link with transceivers connected in only one direction.  i think you could go one simpler: a serial cable with only ground and tx connected at one end, wired to ground and rx at the other.  (i haven’t tried this.)
this approach does have some challenges:


many existing protocols assume a bidirectional link and won’t be usable


there is a challenge of confirming data was successfully received.  for a situation like telemetry, maybe it doesn’t matter; another observation will come along in a minute.  but for sending important documents, one wants to make sure they were properly received.


in some cases, the solution might be simple.  for instance, with telemetry, just writing out data down the serial port in a simple format may be enough.  for sending files, various mitigations, such as sending them multiple times, etc., might help.  you might also look into fec-supporting infrastructure such as blkar and flute, but these don’t provide an absolute guarantee.  there is no perfect solution to knowing when a file has been successfully received if the data communication is entirely one-way.
audio transport
i hinted above that minimodem and gensio both are software audio modems.  that is, you could literally use speakers and microphones, or alternatively audio cables, as a means of getting data into or out of these systems.  this is pretty limited; it is 1200bps, and often half-duplex, and could literally be disrupted by barking dogs in some setups.  but hey, it’s an option.
airgapped with usb transport
this is the scenario i began with, and named some of the possible pitfalls above as well.  in addition to those, note also that usb drives aren’t necessarily known for their error-free longevity.  be prepared for failure.
concluding thoughts
i wanted to lay out a few things in this post.  first, that simply being airgapped is generally a step forward in security, but is not perfect.  secondly, that both physical and logical separation matter.  and finally, that while tools like nncp can make airgapped-with-usb-drive-transport a doable reality, there are also alternatives worth considering – especially serial ports, firewalled hard-wired ethernet, data diodes, and so forth.  i think serial links, in particular, have been largely forgotten these days.

note: this article also appears on my website, where it may be periodically updated.



view all 5 comments 




			posts navigation		

← older posts







recent comments14 bees in a trenchcoat 🐝 on easily accessing all your stuff with a zero-trust mesh vpnrussell coker: links january 2025 - linuxexpert on censorship is complicated: what internet history says about meta/facebookhugo on review of reputable, functional, and secure email servicechewie on review of reputable, functional, and secure email servicemxfraud on review of reputable, functional, and secure email servicetags
airgap
archiving
asynchronous
backup
backups
bicycling
books
dar
darcs
debian
editors
emacs
emacs2018
encryption
facebook
filesystems
git
grandma
haskell
hosting
internet
jacob
lora
mercurial
mesh
mexico2011
moinmoin
music
nncp
obama
org-mode
oscon
radio
religion
security
serendipity
tech support
trac
tuttle
twitter
vcs
version control
video
wiki
zfs



february 2025


s
m
t
w
t
f
s




 1


2345678


9101112131415


16171819202122


232425262728
 



« jan
 
 
disclosurescookies are in use on this site.
to help defray part of the cost of operating the services on complete.org, this page
   at one time provided commissions for products purchased through links in some posts,
   whether my comments were positive or negative.  the commissions have long since been discontinued, the account receiving them closed.  this site is supported solely by me. 








search for:



blogroll

cliff morrow
lambda the ultimate
shae erisson


meta

log in
entries feed
comments feed
wordpress.org

tags
airgap
archiving
asynchronous
backup
backups
bicycling
books
dar
darcs
debian
editors
emacs
emacs2018
encryption
facebook
filesystems
git
grandma
haskell
hosting
internet
jacob
lora
mercurial
mesh
mexico2011
moinmoin
music
nncp
obama
org-mode
oscon
radio
religion
security
serendipity
tech support
trac
tuttle
twitter
vcs
version control
video
wiki
zfs


 



					proudly powered by wordpress				








