






mark davies, professor of (corpus) linguistics
















mark davies
professor of linguistics

 



















						overview

overview

in 2020 i retired from brigham young university, where i was a professor of
linguistics. my primary areas of research
are corpus linguistics,
language change and genre-based variation, the design and optimization of
linguistic databases, and frequency analyses (all for english, spanish,
and portuguese).
please feel free to take a look at my
						cv, a list of my 
publications (all downloadable), or my
						
						google scholar profile.
						
but perhaps the best thing would be to simply try out some of the corpora that
i've
created (english,
spanish,
portuguese), and which i continue
to add to and enhance. these corpora are the
most widely
used corpora in existence, and the

corpus architecture and interface (which i designed myself) offer many
advantages to other large online corpora in terms of speed,
search types,

collocates and
related
topics, 
analyzing entire texts,
word sketches,
virtual
corpora, 
language learning and teaching, the ability to
analyze variation
(historical, dialectal, and genre-based), and
much more.

















			education

education
i received a b.a. in
1986 with a double major in
linguistics and
spanish, which
was followed by an m.a. in
spanish linguistics
			in 1989. i then received a phd from the
university of texas at austin
in 1992, with a specialization in "ibero-romance linguistics (a fancy term
			for spanish and portuguese linguistics). 




			research
   cv
   publications
   (downloadable articles)

research
as a professor of
spanish at illinois state university
			from 1992-2003, most of my publications dealt with historical and
			genre-based variation in spanish and portuguese syntax. i then
			taught at brigham young university (byu) from 2003-2020, where my research dealt primarily with general issues in corpus
design, creation, and use (especially with regards to english), as
well as word frequency. overall, i have published 
			6 books and about 90 articles, and i have given numerous presentations at international conferences
			(with many of them being keynote / plenary talks).




awards

awards

			at byu i
			received the
			karl g.
			maeser research and creative arts award, which recognizes
			achievements in research. this award is given each year to only two
			or three people from the
			1,500+ full-time
			faculty members at byu, and it had not been given to anyone else in the
			college of humanities in the previous eight years. i was also given the
			creative works award, which is given to one person each
			year, who "demonstrates outstanding achievement in the development
			of creative works that have had
			wide acceptance and distribution
			nationally or internationally." finally, i have received
			several
			awards from the college of
			humanities (approx
			
			200 faculty members), including the barker lectureship, a
			two-year college professorship, and two terms (two years + three
			years) as a fellow for the
			humanities center.




grants

grants

			i have received six large federal
			grants to create and analyze corpora. these include four from the
			national endowment for the humanities:
			2001-02 (to create a large corpus of
			historical spanish),
			2004-2006 (to create a large corpus of
			historical portuguese,
			with michael
			ferreira), 2009-2011 (to create a large corpus of
			historical english), and
			2015-2017 (to enlarge the
			spanish and portuguese
			corpora). the two grants from the 
			national science foundation were in 2002-2004 (to examine
			genre-based
			variation in spanish, with
			douglas biber) and
			2013-2016 (to examine "web-genres",
			with douglas biber and
			jesse
			egbert). in addition to these six us-based grants, i have had a
			large subcontract for a grant from the uk arts and humanities
			research council (2014-2016; to create the architecture and web
			interface for large
			
			semantically-tagged corpora). i am also a co-pi for a grant from
			the korea research foundation (2014-2017, with
			jong-bok kim) to
			examine three related syntactic constructions in english from a
			corpus-based perspective. see below for more information on these
			projects.







comparing ai / llms and corpora (2024-2025)

comparing ai / llms and corpora (2024-2025)
i am currently doing research that compares the 
"intuitions" of ai models / large language models (like chatgpt or google gemini) with actual 
corpus data -- for word frequency, phrase frequency, collocates, word 
comparisons (via collocates), and more. in addition, i am looking at what llm's 
"know" about linguistic variation -- between genres, dialects, and historical 
periods. i will be releasing the findings in "white papers" at english-corpora.org 
in march/april 2025, and will then be presenting this data as part of 
invited/plenary talks in spain and germany in summer 2025. 


detailed training and documentation (all corpora)(2023-24)


detailed training and documentation (2023-2024)
in the last two years or so, i have added several detailed pdf help files:
				overview / guided tour,
				architecture,
				association measures,
				collocates (cf sketch engine),
				topics (and collocates),
				word sketches,
				browsing words,
				analyzing texts,
				kwic -> analyze text,
				saved words and phrases,
				saving kwic entries,
				customized word lists,
				search history,
				external resources,
				monitor corpus,
				virtual corpora,
				virtual
				corpora: quick overview.i have also
			added several detailed instructional videos:
overview,
language learning and teaching,
word sketches,
browsing words,
analyze texts,
search history,
customized word lists,
saved words (favorites),
kwic lines: limiting and sorting,
saved kwic lines,
analyze kwic lines,
external resources,
virtual corpora,
examining recent change. 


the
			now corpus as a
			monitor corpus(2022)


the
			now corpus as a
			monitor corpus (2022)
as of march 2022, it is possible to find
			daily keywords in the now corpus (15.9+ billion words as of
			september 2022, and growing by about 200-220 million words each
			month). this is useful to research current events like the invasion
			of ukraine -- or any other current event. it is now also possible to
			quickly and easily search the now corpus by year, and then month,
			
			and then day - something that no other large corpus offers.



spanish and
			portuguese corpora(2021)


spanish and
			portuguese corpora (2021)
a number of new features were added to the
			corpus del español and
			the corpus do português.
			these include the ability to browse and search through the top
			40,000 words in the language, and to see detailed information on
			each word (frequency and distribution, definition, translation to
			100+ languages, images, videos, pronunciation, synonyms, collocates,
			related topics, concordance lines, etc). users can now import entire
			texts, and analyze the texts to find keywords, see detailed
			information on each word in the text, and quickly and easily search
			for related phrases in the corpora.



coca
(analyze texts)(2020)


coca
(analyze texts) (2020)
in coca, users can now analyze entire texts
			(e.g. student compositions or online newspaper articles) using coca
			data. they can find keywords in their texts, and can click on any
			word in the text to see a wide range of information (definition,
			pronunciation, images, videos, synonyms, related words, collocates
			and related topics, clusters, concordances, etc). they can also
			quickly and easily find phrases in coca that are related to phrases
			in their text, which allows them to find "just the right phrase" to
			express a given concept.







			coronavirus corpus
			(2020)


 

			coronavirus corpus 
			(2020)
1.5 billion words of data in almost 1.9
			million texts from jan 2020 - dec 2022. the corpus is designed
			to be the definitive record of the social, cultural, and economic
			impact of the coronavirus (covid-19) in 2020 and beyond. 





			all corpora
			(2020)



			all corpora 
			(2020)
the
			frequency-based data from all of the corpora is now linked to a wide
			range of external resources, including searches of the web, images,
			and billions of words of books; videos from youglish; and
			translations of the corpus phrases in many different languages. all
			of this leverages the power of the most powerful and widely-used
			corpora in the world with huge amounts of data from other sources.






			coca 2020
			(2020)




			coca 2020
			(2020)
the
			corpus of contemporary american english (coca) is probably the most
			widely-used corpus throughout the world, and the only corpus that is
			1) large 2) recent and 3) has texts from a wide range of genres. in
			early 2020, it was nearly doubled in size (to one billion words), it
			now includes texts through dec 2019, and it now includes three new
			genres (blogs, other web pages, and tv/movies subtitles). in
			addition, the "word-oriented" pages (see iweb below) are now
			available for coca as well. (more
			information)





			tv and movie corpora
			(2019)


 
			tv and movie corpora
			(2019)
these are
			the most informal of all of the corpora from english-corpora.org. the tv corpus has 325
			million words in 75,000 tv scripts (comedies and dramas) from
			1950-2018 and the movie corpus has 200 million words in 25,000
			scripts from 1930-2018. in addition to having extremely informal
			language (even more informal than actual spoken corpora like the
			bnc-spoken), the corpora also allow you to look at change over time,
			as well as between dialects. (more
			infomation)





			iweb corpus
			(2018)


 
			iweb corpus
			(2018)
iweb is
			the largest corpus that we've ever created -- 14 billion words,
			which is nearly 14 times the size of coca. (and yet it's still as
			fast as any other corpus, due to its advanced architecture.) the
			corpus allows users to browse through the top 60,000 words in the
			corpus (including by pronunciation), and for each of these words you
			can see a wealth of information -- much of which is not available
			for any of the other corpora from
			
			english-corpora.org, other than coca (including links to pronunciation,
			images, videos, and translations). iweb is perhaps the most
			innovative and learner-friendly corpus that we've ever created. (more
			information; also available in
			
			chinese)





billion word
			extensions to the spanish and portuguese corpora
			(2015-2018)


billion word
			extensions to the spanish and portuguese corpora
			(2015-2018)
in 2015 i was
awarded
(see p37) a three year
grant from the us national
endowment for the humanities to create much larger, updated
versions of the corpus del
español and the 
corpus do português. the corpus del español is now 100 times as
large as before (two billion words, compared to 20 million words for
the 1900s) and the corpus do português is now 50 times as large as
before (one billion words, compared to 20 million words for the
1900s). in addition, each corpus allows users to see the
frequency by country, as is
already possible
for english with the glowbe
corpus.









			early english books
			online
			(2017)

 
			early english books
			online
			(2017)

			part of the
			
			samuels project and funded by the
			ahrc (uk). this corpus contains
			755 million words in more than 25,000 texts from the 1470s to the
			1690s. the corpus provides many types of searches than are not
			available from other eebo corpora online. 





corpus
			of us supreme court opinions
			(2017)

 
corpus
			of us supreme court opinions
			(2017)
this
			corpus contains approximately 130 million words in 32,000 supreme
			court decisions from the 1790s to the current time. this allows
			users to see how words and phrases have been used in a legal context
			since that time. this corpus is related to other activities and
			projects that use corpora to look at legal
			questions.





now corpus
			("news on the web")
			(2016)


now corpus
			("news on the web")
			(2016)

			the corpus automatically grows by
			about 7-8 million words per day, 180-200 million words per month, or
			more than 2 billion words
			each year. so when people
search the now corpus, the data will
be current as of yesterday, which should be useful for
			research that would benefit from up-to-date corpora (i.e. no excuse
			to be limited to stale corpora from 20-25 years ago).





			core corpus (corpus of
			online registers of english)
			(2016)

 
			core corpus (corpus of
			online registers of english)
			(2016)

douglas biber,
jesse egbert, and
			i received a grant
from the us national science foundation to create "a
linguistic taxonomy of english web registers", and this corpus is the result
			of that research (see also
			
1 and 2). the
corpus contains more than 50 million words of text from the web, and it is the first large web-based corpus that is so carefully categorized into so
many different registers. this is
			quite different from other very large corpora that simply present
			huge amounts of data from web pages as giant "blobs", with no real
			attempt to categorize them into linguistically distinct registers.




new corpus interface
(2016)

new corpus interface
(2016)

			the new corpus interface has the
			following improvements and enhances over the interface that had been
			used since 2008: 1) it now works great with mobile devices as well
			2) cleaner, simpler interface 3) more helpful help files 4) simpler,
			more intuitive search syntax. it also allows users to easily and
			quickly create and use "virtual corpora" [vc] (e.g. texts from a
			particular magazine, or related to a particular concept), and then
			search within the vc, compare frequency across different vc, and
			quickly generate keyword lists from the virtual corpus. 






			hansard corpus
			(british parliament)
			(2015)

 

			hansard corpus
			(british parliament)
			(2015)
part of the

samuels project and funded by the
ahrc (uk). this corpus contains
1.6 billion words in 7.6 million speeches in the british parliament
from 1803-2005. a unique feature of the corpus is that it is
semantically
tagged, which allows for powerful meaning-based searches. in
addition, users can create "virtual
corpora" by speaker, time period, house of parliament, and party
in power, and compare across these corpora. the end result is a
corpus that is of value not only to linguists (as the largest
structured corpus of historical british english from the
1800s-1900s), but it is also very useful for historians,
political scientists, and others.






			wikipedia corpus
			(2015)

 

			wikipedia corpus
			(2015)

this
corpus based on 1.9 billion words in 4.4 million articles from
wikipedia. you can quickly
and easily create "virtual corpora" from the 4.4 million web pages (e.g.
electrical engineering, investments, or basketball), and then search just
that corpus, or create keyword lists based on that virtual corpus.
if you want to create a customized corpus for a particular topic,
but don't want to have the hassle of collecting all of the texts
yourself, this should be a very useful corpus.









			downloadable full-text corpus data
(2014)
 

 


			downloadable full-text corpus data
(2014)


			you can download all of the texts
for several of our largest corpora -- tens of billions of words of data. 
with this data on your own computer, you can do many things that
would be difficult or impossible via the
regular web interface, such as
sentiment analysis, topic modeling, named entity recognition,
advanced regex searches, creating treebanks, and creating your own
word frequency,
collocates, and
n-grams lists.





			www.academicwords.info
(2013)
 

 
			www.academicwords.info
(2013)
our academic vocabulary list of english improves
			substantially on the awl created by
			
			coxhead (2000). most of this data is also integrated into the
			wordandphrase
			(academic) site, so that you can see a wealth of information
			about each word.
			
			see the applied linguistics article.





			glowbe: corpus of
			global web-based english
			(2013)



			glowbe: corpus of
			global web-based english
			(2013)
1.9 billion word
			corpus from 1.8 web pages in 20 different english-speaking countries.
			in addition to being very large (20 times as big as the
			bnc), this corpus also
			allows you to carry out powerful searches to
			compare the english
			dialects and see the frequency
			of words, phrases, grammatical constructions, and meaning in these
			twenty different countries.





			www.wordandphrase.info
(2012)
 



			www.wordandphrase.info
(2012)
even more so than the
			standard coca interface,
			this website is
			designed to provide information on nearly everything that you might
			want to know about words and phrases and their usage on one screen
			and with one search. best of all, you can enter entire texts and see
			detailed information about each word in the text, and see related
			phrases from coca.





			google books corpus
(2011)
 



			google books corpus
(2011)


			this

improves greatly on the 
standard n-grams interface from google books. it allows users to
actually use the frequency data (rather than just see it in a
picture), to search by wildcard, lemma, part of speech, and
synonyms, to find collocates, and to compare data in different
historical periods.
			





			corpus of historical american english
			(coha)
			(2010)
 



			corpus of historical american english
			(coha)
			(2010)
400 million word
			corpus of historical american english, 1810-2009. the corpus is 100
			times as large as any other structured corpus of historical english,
			and it is well-balanced by genre in each decade. as a result, it allows
			researchers to examine a wide range of changes in english with
			much
			more accuracy and detail than with any other available corpus. (funded
			by the us national endowment for the
			humanities) 




			english
			
			word frequency,
			
			collocates, and
			
			n-grams
			(2010)


			english
			
			word frequency,
			
			collocates, and
			
			n-grams
			(2010)
based on
			coca and
			other corpora, the data provides a very accurate
			listing of the top 100,000
			words in english (including frequency by genre), the frequency of
			15,300,000+
			collocate pairs, and the
			frequency of all n-grams (1,
			2, 3, 4-grams) in the corpus.





frequency
			dictionary of american english
			(2009)


frequency
			dictionary of american english
			(2009)
the dictionary
			contains the top 5000 words (lemmas) in american english, based on
			the data from the corpus of
			contemporary american english (coca). the dictionary gives the
			top collocates  for each of the 5000 words, which gives a very
			good idea of the overall meaning of each word. (co-authored with
			dee gardner
			(byu), and published by routledge.)





corpus of
			contemporary american english
			(coca)
			(2008)
 


corpus of
			contemporary american english
			(coca)
			(2008)
this 450+ million
			word corpus (now 1 billion words; 2020) is the only large and balanced corpus of american
			english. it is probably the most widely-used online corpus currently
			available. because of its design, it is also perhaps the only large
			corpus of english that can be used to look at
			
			ongoing changes in the language.





frequency
			dictionary of portuguese
			(2007)

 
frequency
			dictionary of portuguese
			(2007)
the dictionary is
			based on the 20 million words from the 1900s portion of the 45
			million word corpus do
			português. it is the first frequency dictionary of portuguese
			that is based on a large corpus from several different genres.
			(co-authored with prof. ana preto-bay of the department of spanish
			and portuguese at byu, and published by routledge.)




corpus do português
			(2006)


corpus do português
			(2006)

45 million word
			corpus of portuguese (1300s-1900s).  the corpus allows users to
			find the frequency, distribution, and use of words, phrases, and
			grammatical constructions in different historical periods, as well
			as in the genres and dialects of modern portuguese. (created in
			conjunction with
			
			michael ferreira of georgetown university, and
			funded by the us national
			endowment for the humanities)





frequency
			dictionary of spanish

			(2005)

 frequency
			dictionary of spanish
			(2005)


			this is the first
major frequency dictionary of spanish that has been published in
english since 1964.  it is based on the 20 million words from
the 1900s portion of the 100 million word
corpus del español, and it
			includes many features not found in any previous dictionary of
			spanish. second edition (with kathy hayward davies) in 2017; based
			on a much larger corpus, with many improved features.





			register variation in
			spanish
			(2004)



			register variation in
			spanish
			(2004)
used large corpora
			of many different registers  of spanish as the basis for a
			"multi-dimensional analysis of register variation in spanish".
			(carried out in conjunction with
			douglas biber of nau,
			and
			
			funded by the us national science
			foundation)





			corpus del español
			(2002)




			corpus del español
			(2002)
100 million word
			corpus of spanish (1200s-1900s).  the corpus allows users to
			find the frequency, distribution, and use of words, phrases, and
			grammatical constructions in different historical periods, as well
			as in the genres of modern spanish. (funded
			by the us national endowment for the
			humanities)




lds general
			conference corpus
			(2000-)



lds general
			conference corpus
			(2000-)
quickly and easily
			search talks from
			
			general conference of the church
			of jesus christ of latter-day saints (mormons).
			this corpus (or collection of texts) contains 25 million words in
			11,000+ talks from 1851 to the current time. you can see the
			frequency of words and phrases and study how words and phrases are
			used differently over time. you can also compare the frequency by
			speaker, and see what keywords characterize a given speaker.








			technology


technology
in order to create
			large corpora and place them online, i have acquired experience in a
			number of different technologies.  these include database
			organization and optimization (mainly with
			sql server,
			including advanced sql
			queries), web-database integration (activex
			data objects), client-side programming (mainly
			dhtml /
			javascript),
			vb.net
			(for processing billions of words of data) and several different corpus and text-related tools.  i also
			maintain the hardware and software for my
			windows servers, including the administration of
			internet information services
			(iis).




			personal


personal
beyond life at the
			university, my interests include
			comparative religion,
			world
			cultures,
			history,
			languages of the world,
			and the 
			relationship between technology and culture, including the
			
			internet.  and of course i
			enjoy spending time with my family -- kathy, our children, and
			our grandchildren.





			email


email
markmark-davies.org


              



