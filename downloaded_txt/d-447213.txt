




thinking for a change







































thinking for a changetreppenwitz in publiccommentsposts













 nayima
  
about

categories 
agile

agile analysis

agile2008

agile2009

agilenorth

agileopen

books

events

games

lean

music

presentations

real options

simblogging

spa2006

spa2008

systems thinking

theory of constraints

toyota way

uncategorized

xp days benelux

xpday


blogroll

emmanuel gaillot
eugeneÂ wallingford
gino marckx
glenÂ alleman
laurentÂ bossavit
marc evers
portia tung
presentation zen
zenika







dec27 
weâ€™re off on an adventure!			 


the dream team nightmare
it took a while to arrive because the book was out of stock at amazon, but itâ€™s finally here, just in time for the holiday break: portia tungâ€˜s â€œdream team nightmareâ€œ!
i sat down with a cup of coffee and started to read the book. portiaâ€™s writing style is engaging and lively with lots of dialogue and very short chapters. at the bottom of the chapters you have to make a choice and then go to a designated page.
iâ€™ve been on agile projects for years, so i expected to breeze through the book. after all, most teams face the same problems. iâ€™ve solved them many times. why should this â€œdream teamâ€ be any different?
after only a few pages i read
patrick [who hired you as an agile coach] asks his secretary to escort you out of the office. before you know it, youâ€™re back on the street.
it feels like youâ€™ve been punched in the gut. thoughts swirl in your head. should have. would have. could have. you decide to take time out to reflect on what happened. youâ€™re not quite ready to give up doing a job you love. not yet.
the end
fired! on my first day. game over. do you want to play again? of course, but now iâ€™ll make smarter choices.
andâ€¦ i was asked to leave the dream team and see if things would go better with another team.Â the end.
â€œif at first you donâ€™t succeed, stubbornly try again and againâ€, thatâ€™s my motto. so, one more try.
aha! we come to a part where my mission is described as a user story with acceptance tests. that will help me to understand better whatâ€™s expected. i restart the book and start working with the team. i flip backwards and forwards as the book directs me to see the consequences of my choices. itâ€™s getting smoother. and then thereâ€™s a conflict with one of the team members. at the end of the 5 days i present my recommendations. and thenâ€¦ nothing happens. the end.
this agile coaching lark is harder than it looks.
this is frustrating. iâ€™ve only scratched the surface of the book, read a few chapters and each time i reach a dead end. reminds me of some projects iâ€™ve been on.
and then i have a brilliant idea.
what would portia do?
iâ€™ve been lucky to work with portia and see her at work. i will restart the last time and at each choice ask myself â€œwhat would portia have done in this situation?â€ portia would

talk to everybody to understand what motivates them
help the team to make their own decisions
reach agreement on the value weâ€™re creating
get a good understanding of where we are
define goals together and clarify them with testable acceptance criteria
take some time to reflect regularly
help the team to create and explore options
lighten up the mood with silly exercises like â€œprofile cardsâ€œ, cookies and team lunches

i can do that. itâ€™sÂ just a matter of taking a bit more time before making a decision. letâ€™s start again, from the top.
this time, with the help of portia (sometimes in the role of jiminy cricket to keep me on the straight and narrow, sometimes as the blue fairy who grants wishes and saves the day) i progress through the book. there are still a few choices, but fewer and fewer.
the team starts working together to visualize their situation. faced with the enormity of their task, they feel demoralized. we come up with three options to deliver value and involve the product owner.
and there it gets tricky. if iâ€™m not careful, the unspoken simmering conflict between team and product owner/management erupts again: the company needs more than the team can deliver; the team blames the product owner for imposing unrealistic scope and deadline.Â the end.
and thatâ€™s where it usually ends, but thereâ€™s another option. thereâ€™s always another option.
instead of saying â€œitâ€™s impossible to deliver this scope by this deadline!â€ you can ask â€œwhat do we need to achieve the goal before the deadline?â€ two other agile teams â€œpredatorâ€ and â€œgreenâ€ can help the dream team. the teams apply real options to decide on the day when they have to decide. meanwhile they create and explore more options to achieve the companyâ€™s goal: different ways to select scope and divide the work between the teams.
finally, a happy end
from there itâ€™s relatively smooth sailing. by now i see the trap on page 216, where i fail to ask the team to review and improve my recommendations because iâ€™m rushed, coming by a mile. if only it were this easy in real life to take a step back for a moment and take the time to thinkâ€¦
and then we arrive at the final chapters: the team present their delivery options and their improvement actions. everybodyâ€™s energized to work on those actions. i donâ€™t know if theyâ€™ll live happily ever after, but the adventure continues.
the end
observations and recommendations

thereâ€™s a list of recommended tools on page 203. appendix 5 contains the coaching tools used in the book. it would be useful to have some links where i can find out more about each of the tools. maybe something for the agile adventure site?
i loved seeing the current and future reality trees in use. it would have been great to see a â€œconflict resolution diagramâ€ be used to bridge the gap between root causes and improvement actions. what are the underlying conflicts that have led and kept the team in its unhappy situation? maybe something for the next agile adventure?
flipping back and forth between chapters isnâ€™t always practical in a paper book. but you can have your cake and eat it too. i bought both the paper and e-book version: the e-book to play, the paper book to loan
there are a few illustrations in the book: current/future reality trees, profile cards. a few more illustrations would have made the story a bit more vivid: what does the kanban board look like? what does the team area look like? no need to make it into a graphic novel. henrik knibergâ€˜s â€œâ€¦ from the trenchesâ€ books are a good example of how illustrations can make the story more vivid.

done. whatâ€™s left on the holiday reading backlog?
some more light reading, because i donâ€™t want to be that agile coach who hasnâ€™t written production code for a decade.
Â  
can anyone recommend a good e-book about clojure that covers core.async?

december 27th, 2013 |  category: agile, books 


jun04 
xp days benelux 2013 â€“ call for session proposals			 

xp days beneluxÂ is an international conference where we learn to bring software to life and grow mature systems that support business needs.
it provides an excellent environment for exchanging ideas, hands-on exercises and extreme experiences.
the best way to learn is toÂ facilitate a session. we like sessions where you explore ideas as well as questions.
the conference will be held on 28 and 29 november, 2013 inÂ mechelen, belgium.
the best way to learn is to facilitate a session at xp days!
weâ€™re looking for sessions where you explore ideas as well as questions. sessions that dig deeper, going beyond the basic techniques and practices. we really want to find out why/how things work or donâ€™t work. we invite you to propose:

hands on coding/design/architecture sessions;
discovery sessions â€“ open ended workshops that explore new topics, common problems, promising techniques, or burning questions;
experiential learning sessions; get people learning by doing & reflecting; for example games or simulations.

weâ€™re not only interested in agile and software related topics but we also want to explore boundaries and cross borders. what can we learn from other disciplines or sciences?
available timeslots are 75 and 150 minutes.
we also welcome short experience reports (30 minutes) that focus on what didnâ€™t work and why.
do you have an interesting story, idea or question?
send us your session ideaÂ today.
whatâ€™s so special about xp days?
for one thing, we constantly try to apply xp, agile, lean, systems thinking, theory of constraints and all the other stuff we talk about. it wouldnâ€™t be an agile conference if it wasnâ€™t organised by using agile values, principles and techniques.
for example: how is the way we build the program agile?
collaborating to get the best possible sessions
we donâ€™t thinkÂ bdufÂ is appropriate or possible, not even for a session description. therefore, we ask session authors to send in a simple proposal: title, subtitle, presenters, description. itâ€™s the equivalent of a story card: the promise for a conversation about the session.
once the proposal is sent in, the author(s) can incrementally fill in more information, as the session becomes clearer. thereâ€™s a separate deadline for submitting proposals (july 13th) and finalising the proposals (august 28th), to avoid â€œstudent syndromeâ€ and last-minute session hurried submissions. within those two timeboxes, the authors work at their own (sustainable) pace.
session authors help other session authors by asking questions and giving feedback using the â€œperfection gameâ€œ. the proposal authors (and the conference organisers) act as â€œpeer coachesâ€œ. through the magic of questions and feedback, the proposal author iteratively improves their proposal(s). arenâ€™t session authors helping their â€œcompetitorsâ€ in the race for a place in the program? we typically get 3 proposals per slot, so the competition is fierce. well, nobody said collaboration was easy. in the end we all benefit from a better conference program. and the organisers see who applies the agile values and who just talks about them.
isnâ€™t this coaching only useful for beginners? it certainly helps new presenters to marshal their ideas. but, as weâ€™ve experienced while pair programming, experienced presenters get new insights and clarity when â€œjuniorsâ€ ask naive questions.

the most powerful coaching questions focus on goals and benefits for participants. why should someone come to this sessions? what is the one idea you want to get across? what will participants be able to do after attending this session that they couldnâ€™t do before? how will you know this session was a success for participants?
a common problem with session proposals is that the authors try to â€œpile onâ€ too many ideas. the usual session feedback reads â€œnot enough timeâ€. the solution is not to take more time, but to present less material more thoroughly. think deeply: what is your minimum viable product? if thereâ€™s any time left in your session you can spend it on interaction with participants and exploration of your central idea, rather than adding more ideas.
release early, release often
a session proposal is not a session. you may have created the greatest session proposal, that doesnâ€™t mean youâ€™ve got the greatest session. thereâ€™s only one way to know if your session design works: implement it, release it and get feedback from participants. we therefore encourage presenters to do â€œtryoutsâ€ of their session. we offer places to do this at the agile belgium and agile holland meetups. you may be able to schedule a tryout at a local user group or in your company. or just invite a few friends and colleagues to try out your session.
seasoned presenters know that it takes at least three iterations to get all the details and the timing right.
preparing to select sessions
now that weâ€™ve iteratively improved session descriptions, the most difficult task is to select the sessions for the program.
near the end of the session improvement process we ask all session authors to select the sessions they want to see in their â€œideal trackâ€. we use these votes as a start of our selection process.
we also use these votes to calculate the â€œvalueâ€ of a draft program. we do this by calculating the program attendance factor (paf) for each voter. the paf value indicates how many of their preferred sessions a voter can attend, given the layout of the program. to count, a preferred session must be in the program and it must not be at the same time as another preferred session. a program that has a higher total paf has more value for voters, because they can attend more preferred sessions.
in preparation for the program committee meeting we create a card for each session. making a conference program requires a lot of visual management:

the color of the card indicates the type of subject so that we can quickly see if the program has a good balance of sessions
the names of the presenters are clear so that we can ensure no presenter has more than one session in a day (sustainable presenter pace)
the card has coloured dots to indicate the type of session and the experience level so that we can see at a glance if thereâ€™s a good balance for participants with different learning styles and experience
the cards have different sizes representing session length so that we canâ€™t put them in program slots of the wrong size

the program committee reviews all session proposals before the program committee meeting. there are so many proposals that program committee members canâ€™t review or even read them all. therefore, the committee divides the sessions among its members and trusts that the other members have good judgment.
before the program committee meeting, the proposals are put into four categories:

the sessions we really want to see in the program. each program committee member can â€œchampionâ€ a session
the sessions that we wonâ€™t consider because theyâ€™re incomplete, withdrawn or inappropriate
the sessions that were selected in the voting process
other sessions

sessions on the board
we have large sheets of paper that represent each of the days with drawn slots that are just the right size for the session cards.

building a program happens in three rounds:

create a first version by putting the championed and most voted for sessions in appropriate slots. we do this track per track. for example: first fill a track on both days with technical sessions, then fill a track with process sessionsâ€¦ itâ€™s ok if some slots are still empty.this first step should go quite quickly, without too much discussion. the goal is notÂ to create a perfect program; itâ€™s not even a workable program. we just need a version to start iterating.
we spot a constraint violation, for example. a presenter who does two sessions in one day or weâ€™ve selectedtoo many sessions of one type or subject. we resolve the constraint by swapping the problem session with another same-sized session already on the board or not on the board. we may also spot improvements in balance or paf score. these are also performed using swaps. the only constraint is that we canâ€™t introduce constraint violations by this swap. we keep iterating until the program stabilizes. we select a few â€œbackupâ€ sessions that we can add to the program in case one of the selected presenters canâ€™t come.
we internally publish a draft version of the program. all organizers can review the program at their leisure to spot any constraint violations or improvements. we often have to take a bit of distance to spot these. when nobody has any more improvement ideas, we contact the authors of selected and rejected proposals. sometimes presenter availability can introduce new constraints (for example, the presenter can only be present on the first day but his session is scheduled on the second day). in that case, we go back to swapping sessions to resolve the constraint.
done. until something happens like a presenter dropping out.

getting better

we continuously improve this session proposal-improvement-review-selection process. weâ€™ve incrementally built a tool that supports our workflow. new features get added just-in-time as we notice irritations or get new ideas for improvement. most of the organisers participate in other conferences where we â€œâ€˜stealâ€ the best ideas. in return, we invite everybody to â€œstealâ€ any idea they like.
in summary: creating a proposal, growing and refining it, coaching other to improve their proposal, doing tryouts and refining the session is a lot of work. and then you might not even get selected because there are so many great proposals.
is it worth it? whatâ€™s the worst thing that could happen? you spent some time to understand an interesting subject better. thatâ€™s not so bad, is it? if you get selected or if you just attend, you get to spend two days with open, intelligent, helpful and experience people to explore and discover interesting new ideas and problems. and one of those ideas or problems could be yours.

june 4th, 2013 |  category: agile, events, xp days benelux, xpday 


may30 
ignore the cost accountants at your peril, because they wonâ€™t ignore you			 

why should i learn about cost accounting?
it may not be immediately obvious, but the way your company manages its finances, costs and budgets has a profound effect on the way projects or products are started, run and terminated. you may have been in situations where you sat back in amazement at some incomprehensible management decision and thought â€œwhat were they thinking?â€
they were probably thinking about budgets and costs. you will be surprised again next time unless you learn to understand and dialogue with your cfo and cost accountants.
fun with cost accounting
pierre hervouet and i have created a session that explains the basics of cost accounting (based on making and pricing cocktails) and presents three alternative views you may find useful:

throughput accounting
lean accounting
beyond budgeting

the session doesnâ€™t certify you as an accounting master, but it provides food for thought and pointers to plenty of material you can dig into if you want to know more. the materials (currently only in french) are published on the agilecoach site.
join the conversation
why donâ€™t you go see your cfo or cost accountants today? ask them whatâ€™s keeping them up at night. you may discover that you face much the same issues and that the solutions are similar. yes, yes, accounting and budgeting are getting more agile, more lean!
the agile alliance has a new â€œagile accounting standardsâ€ program to â€œengage with fasbâ€™s emerging issues task force to promote and develop an agile accounting standard that will better define and standardize internal it development costs for organizations that use an iterative or agile software development methodologyâ€ because Â â€œthe phased gate language [of the current standard] results in significant confusion and challenges of interpreting how to map the iterative work that happens throughout an agile project lifecycle and is becoming an increasing urgent issue.â€œ

may 30th, 2013 |  category: agile, lean, presentations, systems thinking, theory of constraints 


apr03 
a simple property dialog â€“ an alternative approach			 

performing a root cause analysis for a simple bug takes too much time
in a previous post i described how we performed a root cause analysis for a simple bug: one incorrect value Â in a dropdown. performing such a heavy analysis (which generates a lot of rework) may not be appropriate for every bug.
hereâ€™s how another team handled a very similar bug: one value missing from a dropdown.
the fast way to deal with simple bugs

thereâ€™s a bug: in one screen one of the dropdowns misses one value: â€œxâ€. this bug is unexpected: in all other screens the behaviour of the dropdowns is correct, each of them contains â€œxâ€ as the final and default value. this bug only appears in one screen. why only this screen? whatâ€™s so special about this screen? isnâ€™t this the same code for all dropdowns with the same behaviour?
the bug is reported during internal testing. the bug can be repeated very easily.
developers grumble (i told you the â€œthank youâ€ step in theÂ Â algorithm to write perfect codeÂ was difficult!)
a developer takes the bug and fixes it: an extra value â€œxâ€ is added to the dropdown in the screen
tester validates: go into the screen; open the dropdown; the value â€œxâ€ is there. bug closed.
software gets shipped to customer
done. next bug!

easy.
a few days laterâ€¦

customer files a bug report: each time they enter this specific screen an â€œxâ€ gets added to the dropdown. enter the screen twice, you see â€œxâ€ â€œxâ€. enter again, you see â€œxâ€ â€œxâ€ â€œxâ€. and so onâ€¦
developers grumble: â€œnot another stupid bugreport!â€œ
a developer takes the bug and analyses it. â€œsome idiot has added â€˜xâ€™ to this dropdown. letâ€™s remove that.â€œ
tester validates: go into the screen; open the dropdown; no â€œxâ€ is added. bug closed.
software gets shipped to customer
done. next bug!

easy. only took a few minutes to fix and (a few days later) a few minutes to test. and another release to build, ship and install.
a few days laterâ€¦

customer reopens their bug report: when they enter the screen, there is no â€œxâ€. to be clear: they expectÂ exactly one â€œxâ€ in this dropdown (and all similar dropdowns in other screens). not zero. not an infinite number.
developers grumble. â€œwhen will they make up their mind?â€œ
developer takes the bug and fixes it: an â€œxâ€ is added to the dropdown unless thereâ€™s already an â€œxâ€ in the dropdown
tester validates: go into the screen, one â€œxâ€; go into the screen again, still one â€œxâ€; go into the screen again, still one â€œxâ€; go into other screens; go into this screen, still one â€œxâ€. stop the application. restart the application. go into the screen, one â€œxâ€; go into the screen again, still one â€œxâ€.Â bug closed.
software gets shipped to customer
done

easy. only took an hour or two to fix and (a few days later) an hour to test.Â and another release to build, ship and install.
and they all lived happily ever after
except for the customer who grumbles â€œhow can i trust them with the important stuff if they canâ€™t get the simple stuff right?â€
except for the developers who grumble â€œhow can we get any work done if we have to keep fixing these stupid bugs?â€
except for the testers who grumble â€œwhy do we have to retest every bugfix a thousand times? canâ€™t â€˜theyâ€™ get things right the first time? we need fewer releases, more detailed specs, more elaborate test scripts, more time to test and, above all, a lot more testers to get any quality in this application.â€œ

april 3rd, 2013 |  category: agile, systems thinking 


apr02 
a simple property dialog â€“ adventures in root cause analysis			 

iâ€™ve been going on a bit lately about the value i get from of performing â€œroot cause analysisâ€ when i encounter a bug.

 great! another bug  from agilecoach.net
a simple algorithm to write perfect code
you donâ€™t have to watch the presentation. the message is this: each time thereâ€™s a bug:

someone finds a problem and reports it to you
thank the reporter
reproduce the problem
add (at least) one failing test
correct the problem
rerun all tests and correct your correction until the tests pass
improve your tests
improve the way you write tests
look for similar problems. goto 2
make this type of problem impossible
perform the actions that were identified during the root cause analysis

Â but thatâ€™s just common sense!
weâ€™re all agile lean continuously improving test driven extreme programmers, arenâ€™t we? doesnâ€™t everybody do this?
yesâ€¦ butâ€¦

it takes too much time
we canâ€™t do this for every bug because weâ€™ve got too many bugs
we donâ€™t want to waste time on bugs, we prefer to spend time adding valuable features
as dijkstra said: â€œÂ program testing can be a very effective way to show the presence of bugs, but it is hopelessly inadequate for showing their absence.â€
<your objection here>

just a little taste
the presentation tells the story of a team that performed a root cause analysis. hereâ€™s another teamâ€™s story:
coach: can we try a small root cause analysis experiment?
dev: yes, butâ€¦ we donâ€™t have a lot of time.
coach: do you have one hour? we can timebox the experiment.
dev: sure, butâ€¦ you canâ€™t do anything useful in one hour.
coach: maybe you canâ€™t; maybe we can.
dev: ???
coach: have you seen any interesting bugs lately?
dev: well, i just fixed a bug. but itâ€™s so trivial you wonâ€™t discover anything useful.
coach: thank you! letâ€™s see.
i love clear bug reports
first we look at the bug report:





given a swizzled foobar (*)

when i change its properties

then i expect to be offered a choice between 'a', 'b' or 'z' for the status

but i'm offered 'a', 'b' or 'c'

(*) all names of domain objects have been changed to protect the innocent and the guilty




123456789

given a swizzled foobar (*)Â when i change its propertiesÂ then i expect to be offered a choice between 'a', 'b' or 'z' for the statusÂ but i'm offered 'a', 'b' or 'c'Â (*) all names of domain objects have been changed to protect the innocent and the guilty






Â letâ€™s apply the algorithm
coach: have you thanked the reporter yet?
dev: noâ€¦
coach: letâ€™s do it now.
dev: okâ€¦ <calls product manager>
dev: hi, i just called to thank you for reporting the bug about the property dialog of the swizzled foobar.
product manager: <surprised> okâ€¦
dev: the report was so clear the bugfix almost wrote itself. thanks!
product manager: youâ€™re welcome!
coach: ok. weâ€™ve now done the hardest part; letâ€™s do the next steps.
next step: reproduce the problem. thatâ€™s quite easy: start up the application, select a foobar, swizzle it and open its property dialog. look at the dropdown control for the status: it presents options â€˜aâ€™, â€˜bâ€™ and â€˜câ€™.
coach: we have a manual test procedure. can we automate this test?
dev: no. this is user-interface code. weâ€™ve performed some experiments earlier and decided that it wasnâ€™t worth the time and effort to create and maintain automated tests for the user interface.
coach: ok.
fix and test
the fix was really simple
buggy :





 status.add('c') ;




1

 status.add('c') ;





fixed :





 if (foobar.swizzled()) {
   status.add('z') ;
 } else {
   status.add('c') ;
 }




12345

 if (foobar.swizzled()) {Â Â  status.add('z') ; } else {Â Â  status.add('c') ; }





rerun the application, perform the manual test: the correct options are now offered.
result! bug fixed! high five! job well done.

hmmmmâ€¦ the fix is indeed simple and the bug is now gone. but that if is worrying. iâ€™ve seen (and had to maintain) code that was riddled with ifs: each time a bug was detected, the developers added an if for the specific conditions and expected outcomes described in the bugreport. letâ€™s add a post-rca action: letâ€™s read and discuss the anti-if campaign.
dev: you see now: this is a trivial bugfix, thereâ€™s nothing to learn here.
coach:maybe. weâ€™ve only spent 10 minutes yet. letâ€™s see the rest of this code
show me the code
if i take off my glasses, squint a bit and look at the code, this is what i see:





void showproperties(foobar foobar) {




1

void showproperties(foobar foobar) {











}




1

}





red lines are user interface code, calls to the ui toolkit. green lines are simple java, c#, rubyâ€¦ ui-independent code.
guess where the bug isâ€¦ in the green code. but we canâ€™t test it, because we canâ€™t test user interface code. so, that doesnâ€™t help us.
a small step for a programmerâ€¦
we donâ€™t have any tests, so any refactoring is going to be risky. letâ€™s do some simple, safe refactoring: letâ€™s move all the green and red code together.
now the code looks something like this:





void showproperties(foobar foobar) {




1

void showproperties(foobar foobar) {











}




1

}





exactly the same code, it does exactly the same thing. what have we achieved? nothing. yet.
another small step
now that weâ€™ve got all the green code together, what does it do? essentially, it fills in a number of variables (like the list of values for the status), which are filled up with values to put into the ui controls.
letâ€™s do another small, safe refactor:

create a new class. i donâ€™t have a good name yet, letâ€™s call it â€œstuffâ€ for the moment
move every local variable in the green code into the stuff class
create an instance of stuff in the function
fill in all fields of the instance, just like you fill in the local variables

again, nothing really changes. weâ€™ve just collected all local variables in one object.
our code now looks something like this.





void showproperties(foobar foobar) {
 // green
 stuff stuff = new stuff() ;
 stuff.status.add('a') ;
 stuff.status.add('b') ;
 if (foobar.swizzled()) {
   stuff.status.add('z') ;
 } else {
   stuff.status.add('c') ;
 }
....
 // red
 statusdropdown.setoptions(stuff.status) ;
}




1234567891011121314

void showproperties(foobar foobar) { // green stuff stuff = new stuff() ; stuff.status.add('a') ; stuff.status.add('b') ; if (foobar.swizzled()) {Â Â  stuff.status.add('z') ; } else {Â Â  stuff.status.add('c') ; }.... // red statusdropdown.setoptions(stuff.status) ;}






finally, a real refactoring
now the code has been reorganised, we can finally do a real refactoring, still taking small steps because weâ€™re working without a safety net.
letâ€™s extract the green and red code in separate methods. our code now looks like this:





void showproperties(foobar foobar) {
 // green
 stuff stuff = prepare(foobar) ; // todo: find a better name
 // red
 display(stuff) ;
}




123456

void showproperties(foobar foobar) { // green stuff stuff = prepare(foobar) ; // todo: find a better name // red display(stuff) ;}





note to self: â€œstuffâ€ and â€œprepareâ€ arenâ€™t very descriptive. thatâ€™s probably because we donâ€™t understand the code well enough yet. letâ€™s revisit naming when we understand better.
weâ€™re now 30 minutes into the root cause analysis
a giant leap for testing
aha! we now have a method â€œprepareâ€ which contains the bug and doesnâ€™t depend on the ui. can we test the code now? yes we can!





void swizzledfoobarshavezstatus() {
 // given a swizzled foobar
 foobar foobar = makeafoobarsomehow() ;
 foobar.swizzle() ;

 // when i change its properties
 stuff stuff = foobarpropertydialog.prepare(foobar) ;

 // then i expect to be offered a choice between 'a', 'b' or 'z' for the status
 assertequals(3,stuff.status.getsize()) ;
 assertequals('a',stuff.status.get(0)) ;
 assertequals('b',stuff.status.get(1)) ;
 assertequals('z',stuff.status.get(2)) ;
}




1234567891011121314

void swizzledfoobarshavezstatus() { // given a swizzled foobar foobar foobar = makeafoobarsomehow() ; foobar.swizzle() ;Â  // when i change its properties stuff stuff = foobarpropertydialog.prepare(foobar) ;Â  // then i expect to be offered a choice between 'a', 'b' or 'z' for the status assertequals(3,stuff.status.getsize()) ; assertequals('a',stuff.status.get(0)) ; assertequals('b',stuff.status.get(1)) ; assertequals('z',stuff.status.get(2)) ;}





this test fails before the fix. it succeeds after the fix. we now have an automated regression test for this bug.
improve your tests
this test raises a lot of questions:

what if the foobar isnâ€™t swizzled? are the options â€˜aâ€™, â€˜bâ€™ and â€˜câ€™ correct in this case? should we add a test?
are there any other special cases for status depending on the properties of a foobar?
are there any other fields that depend on the properties of the foobar?
are there any other properties of the domain that are important?
â€¦.

this could be the start of a really great conversation with the product manager and testers!
result: weâ€™re 45 min into the rca and weâ€™ve written an automated regression test of that bug in supposedly untestable code.
names
those stupid names â€œstuffâ€ and â€œprepareâ€ irritate me. now that weâ€™ve got automated tests for this code, we can refactor more audaciously.
what is â€œstuffâ€? it contains the data as its shown in the view of the foobar property dialog. itâ€™s a viewmodel. letâ€™s rename it to foobarproperties.
what does â€œprepareâ€ do? it creates a foobarproperties and stuffs values into it. what does foobarproperties do? nothing, it just sits there and contains these values. we might as well move the code from prepare into foobarproperties:





void showproperties(foobar foobar) {
  foobarproperties properties = new foobarproperties(foobar) ;
  display(properties) ;
}




1234

void showproperties(foobar foobar) {Â Â foobarproperties properties = new foobarproperties(foobar) ;Â Â display(properties) ;}





and now the unit test becomes a unit test of foobarproperties, a pure processing class, no longer of the mixed processing/ui class foobarpropertydialog:





void swizzledfoobarshavezstatus() {
 // given a swizzled foobar
 foobar foobar = makeafoobarsomehow() ;
 foobar.swizzle() ;

 // when i change its properties
 foobarproperties properties = new foobarproperties(foobar) ;

 // then i expect to be offered a choice between 'a', 'b' or 'z' for the status
 assertequals(3,properties.status.getsize()) ;
 assertequals('a',properties.status.get(0)) ;
 assertequals('b',properties.status.get(1)) ;
 assertequals('z',properties.status.get(2)) ;
}




1234567891011121314

void swizzledfoobarshavezstatus() { // given a swizzled foobar foobar foobar = makeafoobarsomehow() ; foobar.swizzle() ;Â  // when i change its properties foobarproperties properties = new foobarproperties(foobar) ;Â  // then i expect to be offered a choice between 'a', 'b' or 'z' for the status assertequals(3,properties.status.getsize()) ; assertequals('a',properties.status.get(0)) ; assertequals('b',properties.status.get(1)) ; assertequals('z',properties.status.get(2)) ;}






improve the way you test
now that we have a unit test for foobarproperties we can add more tests for different cases. looking at those tests weâ€™ll see that some properties of a foobar are independent of its state. e.g. we always have a status â€˜aâ€™ and â€˜bâ€™. we can include those invariants in our tests. weâ€™ll talk with the product manager first and together extend the testcase.
we can now see that we consider too much code as â€œui codeâ€ and therefore not testable. if we separate viewmodel code from view code, we can cover more code with fast unit tests.
look for similar bugs
coach: are there other places in the ui where we display or change the status of a foobar?
dev: yesâ€¦ maybe 3-4 screens and dialogs
coach: did we make the same mistake there?
dev: letâ€™s quickly test the application. it would be quite embarassing if we got the same bug report for another screenâ€¦
dev: ooops! we made the same mistake in one other dialog. the other dialogs are ok.
coach: letâ€™s note the dialog to be fixed and move on, because weâ€™ve only got a few minutes left in our timebox and iâ€™d like to do the next step first before fixing this bug.
dev: what more can we do?
make this type of problem impossible
now, before we fix the buggy dialog, weâ€™ll apply the same safe refactorings to make the code testable, add tests to demonstrate the bug (and serve as regression tests once the bug is fixed) and then fix the bug. if we consistently test our viewmodel code and validate our viewmodel behaviour with the product manager, weâ€™re less likely to overlook certain cases.
the fact that this bug appears in some dialogs and not in others tells us that weâ€™re looking at different code. some code takes into account the â€œswizzlednessâ€ of a foobar, some code doesnâ€™t.
but we can do better: why is this code duplicated? ideally, weâ€™d want oneÂ instance of the code that decides which status options to show. otherwise, if the rules change (or more likely, if we discover weâ€™ve missed an existing rule) weâ€™ll have to remember to update all the pieces of code that determine status options of a foobar. so, once weâ€™ve extracted and fixed the viewmodels from both our buggy dialogs, weâ€™ll extract the common code that determines the status options. of course, this common code will have unit tests that verify this common behaviour.
afterwards, weâ€™ll do the same to the dialogs that were implemented correctly. we can do this refactoring gradually, once the two bugs have been corrected. ideally, weâ€™ll do this if we have to change the code of those dialogs anyway, to fix a bug or add a feature. even if we donâ€™t touch these classes, weâ€™ll make sure we refactor them within x time, so that we donâ€™t have to remember these â€œdanglingâ€ refactorings too long.
some developers have taken the â€œswizzlednessâ€ into account, some havenâ€™t. letâ€™s share our findings with the team and the product manager. we may have to organise a session to clarify some of the subtleties of our domain. once theyâ€™re clear we can encode and document them as automated regression tests, so that we get a failing test next time we forget to take into account one of those subtleties.
Â dev: hey, coach, your hour is up! shall we get a coffee?
coach: great! letâ€™s step away from the keyboard ğŸ™‚
looking back (over a cup of coffee)
what have we done in 60 minutes?

isolated code that contains a bug from â€œuntestableâ€ code
added an automated test that shows the bug and can be run as a regression test after the fix
simplified the code by separating the viewmodel from the view
identified a new class of testable code, the viewmodels
found another bug before anyone noticed
we know how to make code testable and better factored for this type of bug
we know how to avoid this type of problem and be ready for changes in the domain

Â looking forward
during the root cause analysis we noted a number of actions to be taken. we make each of these tasks visible (for example, by adding them to the kanban board):

extend the foobartest to cover more cases, in collaboration with the product manager and testers
test and fix the Â second foobar status bug
make the code that determines the allowed status values common between the two bugfixed dialogs. validate tests with the product manager
refactor the other dialogs that display foobar statuses so that they use the common code
list the subtleties of the domain and organise a knowledge sharing session with the product manager
ask the team to review and discussÂ theÂ anti-ifÂ campaign
letâ€™s read some more about viewmodel architectures. if it looks useful weâ€™ll make this a standard way of designing our user interfaces. if we do, weâ€™ll have to see how we transition gradually to such an architecture.

dev: wellâ€¦ i never expected so many issues and ideas to come out of a root cause analysis of such a simple bugreport. letâ€™s hope we donâ€™t do too many of those root cause analyses, because they generate a lot of work.
coach: ????
dev: that was a joke, coach. ğŸ™‚ Â iâ€™m sold. now, letâ€™s get back and fix that bug we found.
Â 

april 2nd, 2013 |  category: agile, presentations, systems thinking 


 Â Â  older entries Â»




event calendar

Â«Â jan





marÂ Â»


february 2025

m
t
w
t
f
s
s


Â 12
3456789
10111213141516
17181920212223
2425262728ec



archives 
december 2013Â (1)
june 2013Â (1)
may 2013Â (1)
april 2013Â (3)
march 2013Â (1)
february 2013Â (3)
november 2012Â (1)
june 2011Â (3)
april 2011Â (6)
march 2011Â (2)
february 2011Â (1)
december 2010Â (2)
november 2010Â (1)
october 2010Â (1)
september 2010Â (6)
august 2010Â (1)
july 2010Â (1)
june 2010Â (3)
may 2010Â (3)
february 2010Â (4)
january 2010Â (7)
december 2009Â (2)
november 2009Â (2)
october 2009Â (6)
september 2009Â (7)
august 2009Â (15)
july 2009Â (6)
june 2009Â (7)
may 2009Â (9)
april 2009Â (8)
march 2009Â (6)
february 2009Â (5)
january 2009Â (5)
december 2008Â (4)
november 2008Â (7)
october 2008Â (6)
september 2008Â (4)
august 2008Â (17)
july 2008Â (7)
june 2008Â (3)
may 2008Â (5)
april 2008Â (7)
march 2008Â (5)
february 2008Â (2)
january 2008Â (3)
december 2007Â (9)
november 2007Â (9)
october 2007Â (1)
july 2007Â (1)
june 2007Â (5)
may 2007Â (7)
april 2007Â (2)
december 2006Â (3)
november 2006Â (6)
october 2006Â (13)
september 2006Â (4)
august 2006Â (2)
july 2006Â (7)
june 2006Â (2)
may 2006Â (4)
april 2006Â (11)
march 2006Â (14)
february 2006Â (7)
january 2006Â (5)
december 2005Â (12)
november 2005Â (1)
october 2005Â (3)
august 2005Â (4)
july 2005Â (22)










    copyright Â© 2025 thinking for a change - all rights reservedpowered by wordpress & the atahualpa wp theme by bytesforall. now with tutorials & support 








