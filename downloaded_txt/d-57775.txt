


deepak extross' blog — livejournal






















































































































































?

?









































































































































                                    





























































































































































livejournal






find more


communities


rss reader




shop









                                help
                            







                                    search
                                









                                    log in



























log in




                                        join free
                                    

                                        join
                                    







                                          english
                                        
                                        (en)
                                

english (en)

русский (ru)

українська (uk)

français (fr)

português (pt)

español (es)

deutsch (de)

italiano (it)

беларуская (be)













































dextross
—














< no suspend reason >









readability




















                    more
                














dextross







                                archive
                            



                                photos
                            



                                video
                            





readability






























log in


no account?
create an account








































remember me



                                forgot password
                            




                        log in
                        














log in









qr code

























































no account?

                    create an account
                
by logging in to livejournal using a third-party service you accept livejournal's user agreement








deepak extross' blog
[entries|archive|friends|userinfo]






dextross


[
userinfo
|
livejournal userinfo
]


[
archive
|
journal archive
]















2005: the year that was
[jan. 1st, 2006|08:58 pm]
dextross




2005 saw 84 documented defacements  of australian websites. 63 of these were government websites - and the vast majority of these were state / territory and local government websites. in may, a queensland-based website that called for a boycott of bali was compromised within an hour of going online.this far, we have been fortunate that most attacks have been little more than vandalism - redirections to turkish websites, images of middle eastern flags, and just plain "i was here and you couldn't stop me" messages. most electronic attacks were sourced externally and dos attacks continued to be the gretest cause of financial loss.in most cases, the affected website was unusable for several hours, with normalcy restored only after 24 to 48 hours, leaving us to wonder whether intrusion detection systems and incident response plans exist.  we've been amused with statements like "it was an attack on our infrastructure; not on our organisation" and "the website only contained public information anyway". the 2005 survey by auscert showed that despite of the wide-spread use of it security standards (65% as compared to 35% in 2003), only 7% of respondents believed that computer security issues were being managed reasonably well! the "pain areas" for 2005 were inadequate staff training in computer security management and poor security culture within the organisation. interestingly, the auscert survey indicated that only 18% of incident allegations reported to law enforcement agencies resulted in a charge. with such poor odds of redressal, the best strategy is incident avoidance through vigilence and protection. the world wide web is a war zone. being out there without your armour is suicide.

linkpost comment





using http get calls in your website.
[jun. 25th, 2005|10:39 pm]
dextross




recently i logged into a school website to check my niece's results. i had to login using her user-id and password first. presumably, this would allow me to see her results for each unit she has completed so far. there were a couple of very basic security flaws with the website. first, the login form was submitted in plain-text html without ssl - meaning that anybody with a rudimentary packet sniffer could intercept and read the login credentials. second, on login, the system seemed to validate the password and then append the student id to all subsequent get requests. that was how the system knew which student's details to display. by tweaking the student id parameter in my browser navigation bar, i could see the results of other students. obviously, this was a breach of the basic security and privacy requirements of a public website. another shining example of poor get request validation was on a local agency's website search page. the user was presented with a search form that allowed the user to enter the search terms, and then limit the search extents by selecting one of the values from a drop-down list that included values like "this website only", "all australian websites", or "the web". entering "child support" and selecting "all australian websites" returned a results page with the heading "your search for child support within all australian websites found the following results...".i found that the get request constructed was something like this "http....&searchstring=child support&within=all australian websites"i was able to get some amusing responses by tweaking the url request. for example, "http....&searchstring=intelligence&within=the parliament" got a response saying "your search for intelligence within the parliament found no results" it took a good deal of self-restraint not to send this url to my friends and acquaintences.the moral of the episode is to use http get requests with extreme care. use post requests wherever possible. if you really need to use get, ensure that each parameter is carefully validated. if possible, parameterise parameters. for example, in the example above, if the site used "...&within=1" to mean "this website only", "...&within=2" to mean "all australian websites" and so on, it would not have been possible for a user pass in arbitrary string data. does your website handle missing http get parameters elegantly? inadequate get request validation could lead to compromise of confidential data, unauthorised access to private information, or inappropriate content being displayed on your website.

linkpost comment
















running a secure website
[jun. 13th, 2005|01:07 pm]
dextross




ok, so you have a web service that has been penetration tested. you run it in a "secure" environment, which no unauthorised user has access to. you even use a network monitoring system to make sure you don't have uninvited visitors in your network. and you get this warm fuzzy feeling that your system is safe. why wouldn't it be?for starters, new exploits and security vulnerabilities are being constantly found and fixed. software that may have been deemed secure at the time your penetration testing was done, may no more be so. you need to quickly apply the security patches or upgrade to newer versions, leaving your systems vulnerable for the shortest possible time. while your network is being monitored for illegal activity, you should know that some exploits do not need network access, but can be launched from the internet. i have explained cross-site scripting and sql injection attacks, which fall under this category, in an earlier posting. so unless you shutdown http traffic, and thereby possibly your website, you may be vulnerable to these kind of attacks.   over time, new services may be deployed on your web service hosting server or others servers on the same network, that may open up new vulnerabilities which did not exist at the time penetration testing was done. most penetration testing includes a scan of all possible ports on the server to ensure that unnecessary ports are not accessible. however, this is useless in case one or more ports are opened up later for any reason whatsoever. a random security breach that may have been detected and terminated quickly, may have been enough to install a backdoor entry point or run processes that could compromise the security of the system.for these reasons, a one-time penetration test is sadly inadequate to guarantee the security of your website. there is only one known way to secure your website completely, and that is to take it offline. that not being an option, consider the following measures: - a repeat penetration test at least every 3 months. - weekly monitoring of log files.  - constant network monitoring and intrusion detection. - all software upgrades or patches installed after the initial penetration testing should be vetted by a security officer. - daily monitoring of processes running on the server to detect any unauthorised processes. - daily scan of server to detect unauthorised open ports.the key to running a secure website is vigilance. regular monitoring and inspection of your system and its logs is crucial to minimising the impact of a security breach. 

linkpost comment





keeping an eye on your website
[jun. 12th, 2005|05:49 pm]
dextross




every website is a potential target for malicious activity. with the internet rapidly becoming more accessible to people in even the remotest areas of the world, the risk of a security breach is high. with each passing day, the number of wannabe “hackers” is increasing, eager to match their wits against the defences your website has to offer. unfortunately, till very recently, the people that wrote software believed that application security was the concern of the application hosting guys, or the network guys, or some benevolent force that would keep the bad people away. instead of secure software, we’ve ended up with layers of security built around vulnerable software. we’ve come to rely on mechanisms like firewalls, application logging, network monitoring, etc to keep our software and data protected.some of most dangerous security exploits in web applications like cross-site scripting and sql injection are best handled in the application itself. re-developing the application to address security issues could be an expensive and time-consuming task, if it is at all possible. the good news is that it is fairly simple to identify attempts to exploit these vulnerabilities by the trails left in the logs. this article focuses on the web server logs, which would normally log every http get and post request.cross site scripting (xss)  xss is possible when your server echoes some data entered by the user. for example, lets say your website has a search functionality. if user does a search for “abc”, your sever probably responds with a html page that says “your search for abc found these results…”. now the user enters the search string as "<script>alert(‘hello’)</script>", and gets back a html page that contains “your search for <script>alert(‘hello’)</script> found these results…” the user’s browser may interpret this to mean that it should pop up an alert box that says “hello”. using this technique, a malicious user can embed an offensive image from another site, a form that sends sensitive data to an unauthorised site, or even steal a cookie from the users session that contains private information. once the user has crafted a url that exploits the xss vulnerability on your website, the url may be posted on a newsgroup or website, or sent by email to an unsuspecting victim. this is an extremely dangerous vulnerability because it “uses” your website as a tool to launch the attack on the victim. moreover, by embedding an onload() or onmouseover() event, sensitive data can be silently sent to an unauthorised location without the user clicking on the page – by the time the user realises the page looks suspicious, it is too late.  fortunately, this kind of exploit is relatively easily to notice in the web server log files. since this attack needs your server to echo the inappropriate content, the inappropriate content embedded in a url must be sent to your sever first. any http requests containing the following strings could be indications of an xss attack:  <script> or </script> (indicates javascript embedded into a url)  <form> or </form>  (indicates an embedded form, or attempt to terminate a legitimate form served up by your server)onclick, onmouseover, onload (indicates an attempt to modify the bahaviour of your form elements)href=  	(indicates an unauthorised link embedded in the url)src=   (indicates an attempt to load an unauthorised image or other file)finally, any occurrences of external urls in your log files like  http://external-site or www.external-site  or just an unrecognised ip address must be investigated, as they could indicate an xss attack.sql injectionsql injection is an exploit that is possible when user-supplied data is used to construct queries without prior validation.   for example, consider a typical login page, where the user is expected to enter a username and password. the application then creates the following query to validate the login:sql = 'select (*) from users where userid = ' + user + ' and password = ' + password + ';'if the user enters username as "* ; -- " and password as xyz, the query becomessql=select (*)  from users where userid = * ; -- and password = xyz ;if your database recognises the ";" character as the end of a query and the "--" as a comment, the user may be able to compromise your login check.another popular hack is to enter the username and password as " ' or '' = ' ".in this case, the query formed issql=select (*) from users where userid='' or ''='' and password ='' or '' = ''since ''='' is always true, the query will return all rows in the users table, and your login validation will be compromised.using this technique, an unauthorised user can access confidential data, and perhaps even obtain the entire contents of a table in the database. generally, forms that deal with this kind of transactions user the http post method or use ssh. therefore, the best place to look for this kind of activity is the application logs, instead of the web server logs. the tell-tale signs of this sql injection attempts are: quotes – either ' or "    (indicates manipulation of the query parameters) ;		(indicates an attempt to prematurely terminate a query, potentially stripping off a ‘where’ clause in the query) --  or # 		(indicates an attempt to comment out the remaining portion of a query, potentially stripping off a ‘where’ clause in the query) *, %, _  	(these are sql wildcard characters and should never be allowed as user input to a query)attempts to get directory listingthe default behaviour of a web server is to allow a listing of the contents of directories in the web root unless this is specifically turned off. in itself, allowing a user to see the contents of your directories seems harmless, but this could give a malicious user enough information to plan an attack on your system. i have touched upon this point in an earlier posting. if your web server logs contain multiple get requests terminating with "/", it indicates that a user tried to obtain the directory listing. this is typically done by browsing your site, stripping off the filename from the end of the url and resubmitting the http request. multiple such requests coming from the same client is reason enough to investigate.remember, a malicious user does not need to login to your servers to launch a xss or sql injection attack. therefore, network monitoring and login control is no deterrent against attacks like this. your best defence is regular monitoring of your log files for signs of suspicious activity.

linkpost comment
















is your website too verbose?
[jun. 8th, 2005|08:53 pm]
dextross




“i often regret that i have spoken; never that i have been silent.” - publilius syrusthe first step in a planned attack on your website is the gathering of information about the site, in order to formulate a plan of attack. obviously, one of the most effective ways to protect your website is to reveal as little information about your website as possible. here are some tips to minimise the amount of information your website reveals about itself.1.	deny directory listings. unless you are running a ftp site, your users do not need to view the contents of a directory on your website. exposing the directory structure and list of files to internet users is unnecessary and can give a hacker valuable information about your website. what you need to do: with the appropriate entry in your web server configuration file, you should be able to disable directory listings. consult your web server documentation for the syntax.2.	avoid revealing file names and directory structure in html comments. too often, html comments hanging around from the development days reveal enough information for a malicious user to piece together the directory structure and list of files in the directories. what you need to do: remove all comments that reveal unnecessary information like file names, directory structure or other sensitive information. you may be able to suppress comments by an application-level configuration setting, if the application you are running supports it.3.	minimise the information sent out in http headers. a simple tool to view what information your server http headers reveal is the good old text-based browser lynx. run “lynx –head http://your-server” to see what your http headers contains. do they reveal the server build number or the ssl version? once a hacker has this information, it is fairly trivial to identify the vulnerabilities in the product. for example, apache 2.0.52 is vulnerable to dos attacks via a http get request with a mime header containing multiple lines with a large number of space characters. if your http header announces that you are running “apache/2.0.52”, you are practically telling a malicious user how to launch a dos attack on your website.what you need to do: minimise the verbosity of http headers by an appropriate setting in your server configuration file. consult your web server documentation for the syntax.in apache, set the servertokens parameter in httpd.conf to “prod”.4.	customise your error responses.in case of an error, redirect the user to a pre-defined error page. not only is this an elegant way to handle errors, it may avoid the display of an error message that reveals sensitive information. for example, if a jsp page being invoked encounters an exception, it outputs a stack trace that may reveal file names and locations. setting up an error response to serve up an error page instead, will prevent this information from being visible to the user.what you need to do: customise error responses by making an appropriate entry in your web server configuration file. consult your web server documentation for the syntax.5.	minimise published site information. if your website has a “site information” or “about this site” page, keep the technical details on this page to the minimum. does the user really need to know the version of web server you are using, or the platform your website runs on? are you revealing information that may help a hacker plan an attack on your website?what you need to do: avoid mentioning product names and versions, operating system information, etc on your website. instead, provide an email address where users can contact the administrator for details, which may be revealed on a need-to-know basis.remember, knowledge is power, and more so in the hands of a malicious user. every superfluous piece of information your website reveals about itself makes it more vulnerable to attack. disclaimer: this article does not suggest that minimising the verbosity of a website is an alternative to the usual security measures employed (firewalls, intrusion detection, logfile analysis, etc.)

linkpost comment















navigation





[
viewing
|
most recent entries
]




































