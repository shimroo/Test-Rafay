   cafe con leche xml news and resources            cafe con leche xml news and resources quote of the day   i remember the early days of the web -- and the last days of cd rom -- when there was this mainstream consensus that the web and pcs were too durned geeky and difficult and unpredictable for "my mom" (it's amazing how many tech people have an incredibly low opinion of their mothers). if i had a share of aol for every time someone told me that the web would die because aol was so easy and the web was full of garbage, i'd have a lot of aol shares. and they wouldn't be worth much.  --cory doctorow read the rest in why i won't buy an ipad (and think you shouldn't, either)   today's news   i've released xom 1.2.5,  my free-as-in-speech (lgpl)dual streaming/tree-based api for processing xml with java.1.2.5 is a very minor release. the only visible change is that builder.build((reader) null) now throws a nullpointerexception instead of a confusing malformedurlexception. i've also added support for maven 2, and hope to get the packages uploaded to the central repository in a week or two.     in other news, i have had very little time to work on this site lately.in order to have any time to work on other projects including xom and jaxen, i've had to let this site slide. i expect to have more news about that soon.     also, speaking of jaxen, i noticed that the website has been a little out of date for a while now because i neglected to update the releases page when 1.1.2 was released in 2008. consequently, a lot of folks have been missing out on the latest bug fixes and optimizations. if you're still using jaxen 1.1.1 or earlier, please upgrade when you get a minute. also, note that the official site is http://jaxen.codehaus.org/. jaxen.org is a domain name spammer. i'm not sure who let that one slide, but we'll have to see about grabbing it back one of these days.      permalink to today's news |recent news |today's java news on cafe au lait |the cafes |older news |e-mail elliotte rusty harold   recommended reading  selected content that might have some relevance or interest for this site's visitors:  
    

    
    
    
    
    

    
        
    

    
    
    
    
    
    
    

    
	

    
        $('document').ready(function() { del.init(); });
    

    
        delicious.com - discover yourself!
    
    var _sf_startpt=(new date()).gettime();


  
	
		    
	delicious
    
    
        
        
        
    



		
    
	
		
		    
                the server is currently experiencing a high load.
                please try again.
                
                
                    return to the home page
                    find answers in the help section
                
            
    	
    
 
    
        about
        blog
        help
        tools
        developers
    
    
        terms
        privacy
        copyright
    









  var _gaq = _gaq || [];
  _gaq.push(['_setaccount', 'ua-26285904-1']);
  _gaq.push(['_setdomainname', 'delicious.com']);
  _gaq.push(['_trackpageview']);

  (function() {
    var ga = document.createelement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getelementsbytagname('script')[0]; s.parentnode.insertbefore(ga, s);
  })();






 you can also see previousrecommended reading or subscribe to the recommended reading rss feed  if you like.  recent news   friday, february 12, 2010 (permalink)  yesterday i figured out how to process form input.today i figured out how to parse strings into nodes in exist. this is very exist specific, but briefly: let $doc := "<html xmlns='http://www.w3.org/1999/xhtml'><div>foo</div></html>"let $list := util:catch('*',            (util:parse($doc)),            ($util:exception-message)) return$list i'll need this for posts and comments. there's also a parse-htmlfunction but it's based on the flaky nekohtml instead of the m,orereliable tagsoup.    wednesday, february 10, 2010 (permalink)  i'm slowly continuing to work on the new backend. i've finally gotten indexing to work. it turns out that exist's namespace hanlding for index configuration files is broken in 1.4.0, but that shouldn be fixed in the release. i've also manged to get the source built and most of the tests to run so i can contribute patches back. next up i'm looking into the supoprt for the atom publishing protocol.    wednesday, february 3, 2010 (permalink)  i spent a morning debugging a problem that i have now boiled down to this test case. the following queryprints 3097: <html>{let $num := count(collection("/db/quotes")/quote)return $num}</html>and this query prints 0:<html xmlns="http://www.w3.org/1999/xhtml">{let $num := count(collection("/db/quotes")/quote)return $num}</html> the only difference is the default namespace declaration. in thedocuments being queried the quote elements are indeed in no namespace.much to my surprise xquery has broken the semantics of xpath 1.0 by  applying default namespaces tounqualified names in path expressions. who thought it wouldbe a good idea to break practice with xslt, every single xpathimplementation on the planet, and years of experience anddocumentation?there's an argument to be made for default namespaces applying in pathexpressions, but the time for that argument to be made was 1998. oncethe choice was made, the cost of switching was far higher than anyincremental improvement you might make. stare decisis isn't just forthe supreme court.   saturday, january 30, 2010 (permalink)  xquery executing for about an hour now. o(n^2) algorithm perhaps?maybe i should learn about indexes? or is exist just hung? declare namespace xmldb="http://exist-db.org/xquery/xmldb";declare namespace html="http://www.w3.org/1999/xhtml";declare namespace xs="http://www.w3.org/2001/xmlschema";declare namespace atom="http://www.w3.org/2005/atom";for $date in distinct-values(    for $updated in collection("/db/news")/atom:entry/atom:updated    order by $updated descending    return xs:date(xs:datetime($updated)))    let $entries := collection("/db/news")/atom:entry[xs:date(xs:datetime(atom:updated)) = $date]return <div>  for $entry in $entries  return $entry/atom:title  <hr /></div>   friday, january 29, 2010 (permalink)  i've got a lot of the old data loaded into exist (news and quotes; readings and other pages i still have to think about). i'm now focusing on how toget it back out again and put it in web pages. once that's done, the remaining piece is setting up some system for putting new data in. it will probably be a fairly simple html form, but some sort of markdown support might be nice. perhaps i can hack something together that will insert paragraphs if there are no existing paragraphs, and otherwise leave the markup alone. i'm also divided on the subject of whether to store the raw text, the xhtml converted text, or both. this will be even more critical when i add comment support.    tuesday, january 26, 2010 (permalink)  i've more or less completed the script that converts the oldnews into atom entry documents: xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";declare namespace html="http://www.w3.org/1999/xhtml";declare namespace xs="http://www.w3.org/2001/xmlschema";declare namespace atom="http://www.w3.org/2005/atom";declare namespace text="http://exist-db.org/xquery/text";declare function local:leading-zero($n as xs:decimal) as xs:string {    let $result := if ($n >= 10)     then string($n)    else concat("0", string($n))   return $result};declare function local:parse-date($date as xs:string) as xs:string {    let $day := normalize-space(substring-before($date, ","))    let $string-date := normalize-space(substring-after($date, ","))    let $y1 := normalize-space(substring-after($string-date, ","))    (: strip permalink :)    let $year := if (contains($y1, "("))                 then normalize-space(substring-before($y1, "("))                 else $y1        let $month-day := normalize-space(substring-before($string-date, ","))    let $months := ("january", "february", "march", "april", "may", "june", "july", "august", "september", "october", "november", "december")        let $month := substring-before($month-day, " ")    let $day-of-month := local:leading-zero(xs:decimal(substring-after($month-day, " ")))    let $monthnum := local:leading-zero(index-of($months,$month))    (: i don't necessarily know the time so i'll pick something vaguely plausible. :)    return concat($year, "-", $monthnum, "-", $day-of-month, "t07:00:31-05:00")};declare function local:first-sentence($text as xs:string) as xs:string {    let $r0 := normalize-space($text)    let $r1 := substring-before($text, '. ')    let $penultimate := substring($r1, string-length($r1)-1, 1)    let $sentence := if ($penultimate != " " or not(contains($r1, ' ')))                   then concat($r1, ".")                   else concat($r1, ". ", local:first-sentence($r1))    return $sentence};declare function local:make-id($date as xs:string, $position as xs:integer) as xs:string {    let $day := normalize-space(substring-before($date, ","))    let $string-date := normalize-space(substring-after($date, ","))    let $y1 := normalize-space(substring-after($string-date, ","))    (: strip permalink :)    let $year := if (contains($y1, "("))                 then normalize-space(substring-before($y1, "("))                 else $y1    let $month-day := normalize-space(substring-before($string-date, ","))    let $months := ("january", "february", "march", "april", "may", "june", "july", "august", "september", "october", "november", "december")        let $month := substring-before($month-day, " ")    let $day-of-month := local:leading-zero(xs:decimal(substring-after($month-day, " ")))    let $monthnum := local:leading-zero(index-of($months,$month))    return concat($month, "_", $day-of-month, "_", $year, "_", $position)};declare function local:permalink-date($date as xs:string) as xs:string {    let $day := normalize-space(substring-before($date, ","))    let $string-date := normalize-space(substring-after($date, ","))    let $y1 := normalize-space(substring-after($string-date, ","))    (: strip permalink :)    let $year := if (contains($y1, "("))                 then normalize-space(substring-before($y1, "("))                 else $y1    let $month-day := normalize-space(substring-before($string-date, ","))    let $month := substring-before($month-day, " ")    let $day-of-month := xs:decimal(substring-after($month-day, " "))    return concat($year, $month, $day-of-month)};for $newsyear in (1998 to 2009)return for $dt in doc(concat("file:///users/elharo/cafe%20con%20leche/news", $newsyear ,".html"))/html:html/html:body/html:dl/html:dtlet $dd := $dt/following-sibling::html:dd[1]let $date := string($dt)let $itemstoday := count($dd/html:div)return    for $item at $count in $dd/html:div    let $sequence := $itemstoday - $count + 1    let $id := if ($item/@id)               then string($item/@id)               else local:make-id($date, $sequence)                         let $published := if ($item/@class)                 then string($item/@class)                 else local:parse-date($date)    let $link := concat("http://www.cafeconleche.org/#", $id)    let $permalink := if ($item/@id)                      then concat("http://www.cafeconleche.org/oldnews/news", local:permalink-date($date), ".html#", $item/@id)                      else concat("http://www.cafeconleche.org/oldnews/news", local:permalink-date($date), ".html")    return    <atom:entry xml:id="{$id}">        <atom:author>         <atom:name>elliotte rusty harold</atom:name>         <atom:uri>http://www.elharo.com/</atom:uri>       </atom:author>       <atom:id>{$link}</atom:id>       <atom:title>{local:first-sentence(string($item))}</atom:title>       <atom:updated>{$published}</atom:updated>       <atom:content type="xhtml" xml:lang="en"            xml:base="http://www.cafeconleche.org/"           xmlns="http://www.w3.org/1999/xhtml">{$item/node()}</atom:content>       <link rel="alternate" href="{$link}"/>       <link rel="permalink" href="{$permalink}"/>    </atom:entry> i should probably figure out how to remove some of the duplicate date parsing code, but it's basically a one-off migration script so i may not bother. i think i have enough in place now that i can start setting up the templates for the main index.html page and the quote and news archives. then i can start exploring the authoring half of the equation.    monday, january 25, 2010 (permalink)  i'm beginning to seriously hate the runtime error handling (or lack thereof) in xquery.it's just too damn hard to debug what's going wrong where compared to java.you can't see where the bad data is coming from, and there's no try-catch facility to help you out. now that i think about it, i had very similar problems with haskell last year. i wonder if this is a common issue with functional languages?     thursday, january 21, 2010 (permalink)  i've just about finished importing all the old quotes into exist.(there was quite a bit of cleanup work going back 12 years. the format changed solowly over time.) next up is the news. i am wondering if maybe this is backwards. perhaps first i should build the forms and backend for posting new content, and then import the old data? after all, it's the new content people are interested in. there's not that much call for breaking xml news from 1998. :-)   wednesday, january 20, 2010 (permalink)  parsing a date in the form "wednesday, january 20, 2010" in xquery: xquery version "1.0";declare function local:leading-zero($n as xs:decimal) as xs:string {    let $result := if ($n >= 10)     then string($n)    else concat("0", string($n))   return $result};declare function local:parse-date($date as xs:string) as element() {    let $day := normalize-space(substring-before($date, ","))    let $string-date := normalize-space(substring-after($date, ","))    let $year := normalize-space(substring-after($string-date, ","))    let $month-day := normalize-space(substring-before($string-date, ","))    let $months := ("january", "february", "march", "april", "may", "june", "july", "august", "september", "october", "november", "december")        let $month := substring-before($month-day, " ")    let $day-of-month := number(substring-after($month-day, " "))        return       <postdate>        <day>{$day}</day>        <date>{$year}-{local:leading-zero(index-of($months,$month))}-{local:leading-zero($day-of-month)}</date>      </postdate>};local:parse-date("monday, april 27, 2009")   tuesday, january 19, 2010 (permalink)  today i went from merely splitting the quotes files apart into indiviodual quotes to actually storing them back into the database: xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";declare namespace html="http://www.w3.org/1999/xhtml";for $dt in doc("/db/quoteshtml/quotes2009.html")/html:html/html:body/html:dl/html:dtlet $id := string($dt/@id)let $date := string($dt)let $dd := $dt/following-sibling::html:dd[1]let $quote := $dd/html:blockquotelet $cite := string($quote/@cite)let $source := $quote/following-sibling::*let $sourcetext := normalize-space(substring-after($source, "--"))let $author := if (contains($sourcetext, "read the"))               then substring-before($sourcetext, "read")               else substring-before($sourcetext, "on the")let $location := if ($source/html:a)               then $source/html:a               else substring-after($sourcetext, "on the")let $quotedate := if (contains($sourcetext, "list,"))               then  normalize-space(substring-after($sourcetext, "list,"))               else ""let $justlocation := if (contains($location, "list,"))               then  normalize-space(substring-after(substring-before($sourcetext, ","), "on the"))               else $locationlet $singlequote := <quote>   <id>{$id}</id>   <postdate>{$date}</postdate>   <content>{$quote}</content>   <cite>{$cite}</cite>   <author>{$author}</author>   <location>{$justlocation}</location>   {     if ($quotedate)      then <quotedate>{$quotedate}</quotedate>     else ""   }</quote>let $name := concat("quote_", $id)let $store-return := xmldb:store("quotes", $name, $singlequote)return<store-result>   <store>{$store-return}</store>   <documentname>{$name}</documentname></store-result> i suspect the next thing i should do is work on iomproving the dates somewhat since i'll likely want to sort and query by them. right now they're human reabale but not so easy to process. e.g. <postdate>monday, april 27, 2009</postdate> i should try to turn this into  <postdate>  <day>monday</day>   <date>2009-04-27</date></postdate> time to read up on the xquery date and time functions. hmm, looks like it's going to be regular expressions after all.    friday, january 15, 2010 (permalink)  i've converted all the old quotes archives to well-formed (though not necessarily valid) xhtml and uploaded them into exist. now i have to come up with an xquery that breaks them up into individual quotes. this is proving trickier than expected (and i expected it to be pretty tricky, especially since a lot of the old quotes aren't in perfectly consistent formats.  maybe it's time to try out oxygen's xquery debugger since they sent me a freebie? if only the interface weren't such a horrow show. they say they have a debugger but i can't find it, and the buttons they're using in the screencast don't seem to be present in the latest version. in the meantime, can anyone see the syntax error in this code? xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";declare namespace html="http://www.w3.org/1999/xhtml";     for $dt in doc("/db/quoteshtml/quotes2010.html")/html:html/html:body/html:dl/html:dt        let $id := string($dt/@id)        let $date := string($dt)        let $dd := $dt/following-sibling::html:dd        let $quote := $dd/html:blockquote        let $cite := string($quote/@cite)        let $source := $quote/following-sibling::html:p        let $author := normalize-space(substring-after($source/*[1], "--"))     return        <quote>           <id>{$id}</id>           <date>{$date}</date>           <quote>{$quote}</quote>           <cite>{$cite}</cite>           <source>{$quote}</source>           <author>{$author}</author>        </quote> the error message from exist is "the actual cardinality for parameter 1 does not match the cardinality declared in the function's signature: string($arg as item()?) xs:string. expected cardinality: zero or one, got 4." found the bug: the debugger wasn't very helpful (once i found it--apparently author and oxygen are not the same thing), but saxon had much better error messages than exist.i needed to change let $dd := $dt/following-sibling::html:dd tolet $dd := $dt/following-sibling::html:dd[1].exist didn't tell me which line had the problem so i was looking in the wrong place. saxon pointed me straight to it. score 1 for saxon. here's the finished script. it works for at least the lasy couple of years.i still have to test it out on some of the older files: xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";declare namespace html="http://www.w3.org/1999/xhtml"; for $dt in doc("/db/quoteshtml/quotes2009.html")/html:html/html:body/html:dl/html:dt    let $id := string($dt/@id)    let $date := string($dt)    let $dd := $dt/following-sibling::html:dd[1]    let $quote := $dd/html:blockquote    let $cite := string($quote/@cite)    let $source := $quote/following-sibling::*    let $sourcetext := normalize-space(substring-after($source, "--"))    let $author := if (contains($sourcetext, "read the"))                   then substring-before($sourcetext, "read")                   else substring-before($sourcetext, "on the")    let $location := if ($source/html:a)                   then $source/html:a                   else substring-after($sourcetext, "on the")    let $quotedate := if (contains($sourcetext, "list,"))                   then  normalize-space(substring-after($sourcetext, "list,"))                   else ""    let $justlocation := if (contains($location, "list,"))                   then  normalize-space(substring-after(substring-before($sourcetext, ","), "on the"))                   else $location return    <quote>       <id>{$id}</id>       <postdate>{$date}</postdate>       <quote>{$quote}</quote>       <cite>{$cite}</cite>       <author>{$author}</author>       <location>{$justlocation}</location>       {         if ($quotedate)          then <quotedate>{$quotedate}</quotedate>         else ""       }    </quote>   thursday, january 14, 2010 (permalink)  the xquery work continues to roll along. i think i've roughly figured out to configure the server. i found and reported a few more bugs in exists, none too critical.i now have exist serving this entire web site on my local box, though i haven't changed the server here on ibiblio yet. that's still apache and php. the next step is to convert all the static files from the last 12 years--quotes, news, books, conferences, etc.--into smaller documents in the database. for instance, each quote will be its own document. then i have to rewrite the pages the as xquery "templates" that query the database. from that point i can add suppor for new posts, submissions, and comments via a web browser and forms.   friday, january 8, 2010 (permalink)  i didn't really like the format of yesterday's twitter dump so today i opened another can of xquery ass-kicking to improve it. first, let's group by date: xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom";let $tweets := for $entry in reverse(document("/db/twitter/elharo")/atom:feed/atom:entry)return   <div><date>{substring-before($entry/atom:updated/text(), "t")}</date> <p> <span>{substring-before(substring-after($entry/atom:updated/text(), "t"), "+")} utc</span> {substring-after($entry/atom:title/text(), "elharo:")}</p></div>return   for $date in distinct-values($tweets/date)  return <div><h3>{$date}</h3>   {   for $tweet in $tweets   where $tweet/date = $date   return $tweet/p  }</div> now let's hyperlink the urls: xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom";let $tweets := for $entry in reverse(document("/db/twitter/elharo")/atom:feed/atom:entry)return   <div><date>{substring-before($entry/atom:updated/text(), "t")}</date> <p> <span>{substring-before(substring-after($entry/atom:updated/text(), "t"), "+")} </span>{replace(substring-after($entry/atom:title/text(), "elharo:"), "(http://[^\s]+)", "<a href='http://$1'>http://$1</a>")}</p></div>return   for $date in distinct-values($tweets/date)  return <div><h3>{$date}</h3>   {   for $tweet in $tweets   where $tweet/date = $date   return $tweet/p  }</div> let's do the same for @names: xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom"; let $tweets := for $entry in reverse(document("/db/twitter/elharo")/atom:feed/atom:entry)return   <div><date>{substring-before($entry/atom:updated/text(), "t")}</date> <p> <span>{substring-before(substring-after($entry/atom:updated/text(), "t"), "+")} </span>{replace (    replace(substring-after($entry/atom:title/text(), "elharo:"),         "(http://[^\s]+)",         "<a href='$1'>$1</a>"),    " @([a-za-z]+)",    " <a href='http://twitter.com/$1'>@$1</a>")}</p></div>return   for $date in distinct-values($tweets/date)  return <div><h3>{$date}</h3>   {   for $tweet in $tweets   where $tweet/date = $date   return $tweet/p  }</div> and one more time for hash tags: xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom"; let $tweets := for $entry in reverse(document("/db/twitter/elharo")/atom:feed/atom:entry)return   <div><date>{substring-before($entry/atom:updated/text(), "t")}</date> <p> <span>{substring-before(substring-after($entry/atom:updated/text(), "t"), "+")} </span>{replace (    replace (        replace(substring-after($entry/atom:title/text(), "elharo:"),             "(http://[^\s]+)",             "<a href='$1'>$1</a>"),        " @([a-za-z]+)",        " <a href='http://twitter.com/$1'>@$1</a>"    ),    " #([a-za-z]+)",    " <a href='http://twitter.com/search?q=#$1'>#$1</a>")}</p></div>return   for $date in distinct-values($tweets/date)  return <div><h3>{$date}</h3>   {   for $tweet in $tweets   where $tweet/date = $date   return $tweet/p  }</div> and here's the finished result.    thursday, january 7, 2010 (permalink)  this morning a simple practice exercise to get my toes wet. first load my tweets from their atom feed into exist: xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";let $collection := xmldb:create-collection("/db", "twitter")let $filename := ""let $uri := xs:anyuri("file:///users/elharo/backups/elharo_statuses.xml")let $retcode := xmldb:store($collection, "elharo", $uri)return $retcode then generate html of each tweet: xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom";for $entry in document("/db/twitter/elharo")/atom:feed/atom:entry   return   <p>{$entry/atom:updated/text()} {substring-after($entry/atom:title/text(), "elharo:")}</p> can i reverse them so they go forward in time? yes, easily: for $entry in reverse(document("/db/twitter/elharo")/atom:feed/atom:entry) now how do i dump that to a file? maybe something like this? xquery version "1.0";declare namespace atom="http://www.w3.org/2005/atom";let $tweets := <html> {for $entry in document("/db/twitter/elharo")/atom:feed/atom:entry   return   <p>{$entry/atom:updated/text()} {substring-after($entry/atom:title/text(), "elharo:")}</p>} </html>return  xmldb:store("/db/twitter", "/users/elharo/tmp/tweets.html", $tweets) oh damn. almost, but that puts it back into the database instead of the filesystem. still i can now run a query that grabs just that and copy and paste the result since there's only 1. the first query gave almost 1000 results and the query sandbox only shows one at a time.  tomorrow: how do i serve that query as a web page?   wednesday, january 6, 2010 (permalink)  what i've learned about exist so far:  i can use virtual hosting to run it, either at rackspace cloud, amazon ec2, or right here on ibiblio; and use jetty as my web server. however i probably should proxy it behind apache anyway. i can upload files into the repository. i can execute simple xqueries using the xquery sandbox.  what i still don't know:  how to address the documents i upload from inside the xquery sandbox; and in general how to manage and manipulate collections.  partial answer: xquery version "1.0";declare namespace xmldb="http://exist-db.org/xquery/xmldb";for $foo in collection("/db/collectionname")return $foo   tuesday, january 5, 2010 (permalink)  first bug  filed against exist during this project: excessive confirmation, a common ui anti-pattern, especially on windows though in this case it's cross-platform.   second bug filed. this one comes with potential for data loss. third bug and i haven't even left the installer yet. time to check out the source code. (i hope i don't have to fix izpack too.)    monday, january 4, 2010 (permalink)  at the tune of the new year and a new decade, i've decided to explore some changes here. several points are behind this:  since starting to work more as a software developer and less as an author, i don't have as much free time to work on these sites as i once did, nor is it as obviously relevant to my day job. when i was a full-time author, these sites gave me new ideas and  new things to write about. they still do, but i no longer have the time to write about those things.  cafe con leche  and cafe au lait and are some of the oldest blogs on the web. in fact, i only know a couple that predate cafe au lait. when cafe au lait started mysql wasn't open source, and php, xml, and xslt didn't exist yet. in other words, the technology that powers them is old.  wordpress  helped me rethink a lot of how i suspect a blog site should work from the user interface side. these sites are a lot more automated and well-formed than they used to be; but it's still basically static html driven by some client side applescript and xslt run out of cron jobs. i'd like to do better. i considered just porting them to wordpress; but, as nice as the wordpress frontend is, it has some flaws; the most fundamental of which is that it's trying to stuff triangular pegs into rectangular holes.   i don't have a lot of spare time these days; and what i do have is mostly occupied with photography and chasing birds, but i've decided that there's not a lot of point to continuing with this site as it is.  don't worry though. it's not going away. i'm just going to focus on building a new infrastructure rather than on posting more news. i'm going to dogfood my work right here on cafe con leche. (i will keep cafe au lait on the old system until i'm happy with the new one.) i've decided to begin by experimenting with bringing the site up on top of existdb. it may go down in flames. it may not work at all. i may have to revert to the old version. it will probably sometimes be unavailable. there will have to be several iterations. but certainly along the way i'll learn a few things about xquery databases, and just maybe i'll produce something that's more widely useful than a few bits of applescript and xslt. see you on the other side!      older news     xml books | xml examples| xml trade shows | xml mailing lists | xml quotes | cafe con leche rss feed  | the cafes | cafe au lait           copyright 1998-2009 elliotte rusty harold elharo@metalab.unc.edu last modified at tuesday, april 6, 2010 7:45:24 am    processing xml with java| xml in a nutshell| effective xml| the xml 1.1 bible| the xml bible, gold edition| xml: extensible markup language| special reports | xml book list| xml examples| xml seminar slides| xml conferences| xml mailing lists| xml quotes| rss feed | atom feed | the cafes | | mokka mit schlag | cafe au lait | amazon plog                    xml overview  the xml faq list the annotated xml spec  tutorials  xslt xsl-fo xlinks xpointers schemas  upcoming conferences  balisage  more conferences projects  xom jaxen sax conformance tests xquisitor xinclude  seminar notes  effective xml  stax  xom  dom level 3  intro to xml  processing xml with java  xml fundamentals xquery namespaces xslt 2.0 and beyond schemas dtds  dom sax xlinks and xpointers  xsl transformations  xml: hype vs. hope  xinclude  advanced xml   random notes  about this web site cafe au lait  specifications  xml 1.0 errata in xml 1.0 annotated xml 1.0 specification xml namespaces css level 1 css level 2 html 4.0 xhtml 1.0 xsl formatting objects xsl transformations 1.0 xpath 1.0 xml schema part 0: primer  xml schema part 1: structures xml schema part 2: datatypes xlinks xpointers soap dom sax urls uris dublin core unicode  books  effective xml processing xml with java xml in a nutshell xml bible xml: extensible markup language   xml resources  xml conferences and trade shows xml book list xml mailing lists quotes  development tools validating parsers  libxml (c) exml (eiffel) xerces-j (java) xerces-c (c++) xerces-p (perl) msxml (java) xml parser for java (java) xmlproc (python) larval (java) sxp (java) rxp (c) crimson (java) fxp (ml)  xml for c++ ltxml (c) xml parser (delphi)  non-validating parsers  kxml (java) xml tools (applescript) xmltex (tex) xml engine (realbasic) xml::parser (perl) lark (java) xp (java) gnu jaxp (java) expat (c) exml (eiffel) nenie xml (eiffel) xparse (javascript) pyxml (python) xml.sax (python)  online validators and syntax checkers  stg xml validation form (validating) xml-check (validating)  formatting engines  libxslt (c) saxon (xslt)  xalan (xslt)  sablotron (xslt) xt (xslt)  fop (xsl-fo) xmlroff (xsl-fo) xep (xsl-fo) passivetex (xsl-fo) antenna house xsl formatter (xsl-fo) jade (dsssl) xslj (xslt) docproc koala xsl engine (xslt)   browsers  opera jumbo mozilla firefox safari internet explorer windows x-smiles techexplorer (mathml)  class libraries  jdom  xom  jaxen   editors  xml copy editor serna editix exchanger <oxygen/> editml xmetal xml spy xml pro  xmlwriter (windows) jumbo  cooktop xmlmind xml editor  xml applications  ecological metadata language (eml) itsy bitsy teeny weeny simple hypertext dtd(ibtwsh) molecular dynamics language (modl) chemical markup language mathematical markup language musicxml ice resource description framework flixml extensible mail transport protocol (xmtp) personalized informationdescription language (pidl) xhtml  channel definition format (cdf) open software description format (osd) scalable vectorgraphics  (svg)  external sites  the w3c xml.com microsoft's xml page robin cover's xml web page the xml files oasis <xml>fr (in french) cafe au lait          