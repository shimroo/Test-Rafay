






machine learning (theory) – machine learning and learning theory research



























skip to content







machine learning (theory)
machine learning and learning theory research

  scroll down to content








posts





posted on 4/5/2023an ai miracle malcontent 

the stark success of openai’s gpt4 model surprised me shifting my view from “really good autocomplete” (roughly inline with intuitions here) to a dialog agent exhibiting a significant scope of reasoning and intelligence.  some of the msr folks did a fairly thorough study of capabilities which seems like a good reference.  i think of gpt4 as an artificial savant: super-john capable in some language-centric tasks like style and summarization with impressive yet more limited abilities in other domains like spatial and reasoning intelligence.
and yet, i’m unhappy with mere acceptance because there is a feeling that a miracle happened.  how is this not a miracle, at least with hindsight?  and given this, it’s not surprising to see folks thinking about more miracles.  the difficulty with miracle thinking is that it has no structure upon which to reason for anticipation of the future, prepare for it, and act rationally.  given that, i wanted to lay out my view in some detail and attempt to understand enough to de-miracle what’s happening and what may come next.
deconstructing the autocomplete to dialog miracle
one of the ironies of the current situation is that an organization called “openai” created ai and isn’t really open about how they did it.  that’s an interesting statement about economic incentives and focus.  nevertheless, back when they were publishing, the instruct gpt paper suggested something interesting: that reinforcement learning on a generative model substrate was remarkably effective—good for 2 to 3 orders of magnitude improvement in the quality of response with a tiny (in comparison to language sources for next word prediction) amount of reinforcement learning. my best guess is that this was the first combination of 3 vital ingredients.

learning to predict the next word based on vast amounts of language data from the internet.  i have no idea how much, but wouldn’t be surprised if it’s a million lifetimes of reading generated by a billion people.  that’s a vast amount of information there with deeply intermixed details about the world and language.

why not other objectives?  well, they wanted something simple so they could maximize scaling.  there may indeed be room for improvement in choice of objective.
why language? language is fairy unique amongst information in that it’s the best expression of conscious thought.  there is thought without language (yes, i believe animals think in various ways), but you can’t really do language without thought.


the use of a large deep transformer model (pseudocode here) to absorb all of this information.  large here presumably implies training on many gpus with both data and model parallelism.  i’m sure there are many fine engineering tricks here.  i’m unclear on the scale, but expect the answer is more than thousands and less than millions.

why transformer models?  at a functional level, they embed ‘soft attention’ (=ability to look up a value with a key in a gradient friendly way).  at an optimization level, they are gpu-friendly.
why deep? the drive to minimize word prediction error in the context of differentiable depth creates a pressure to develop useful internal abstractions.


reinforcement learning on a small amount of data which ‘awakens’ a dialog agent.  with the right prompt (=prefix language) engineering a vanilla large language model can address many tasks as the information is there, but it’s awkward and clearly not a general purpose dialog agent.  at the same time, the learned substrate is an excellent representation upon which to apply rl creating a more active agent while curbing an inherited tendency to mimic internet flamebait.

why reinforcement learning?  one of the oddities of language is that there is more than one way of saying things.  hence, the supervised learning view that there is a right answer and everything else is wrong sets up inherent conflicts in the optimization. hence, “reinforcement learning from human feedback” pairs inverse reinforcement learning to discover a reward function and basic reinforcement learning to achieve better performance.  what’s remarkable about this is that the two-step approach is counter to the information processing inequality.



the overall impression that i’m left with is something like the “ghost of the internet”.  if you ask the internet for the answer to a question on the best forum available and get an answer, it might be in the ballpark of as useful and as correct as that which gpt4 provides (notably, in seconds).  peter lee’s book on the application to medicine is pretty convincing.  there are pluses and minuses here—gpt4’s abstraction of language tasks like summarization and style appear super-human, or at least better than i can manage.  for commonly discussed content (e.g. medicine) it’s fairly solid, but for less commonly discussed content (say, battletech fan designs) it becomes sketchy as the internet gives out. there are obviously times when it errs (often egregiously in a fully confident way), but that’s also true in internet forums.  i specifically don’t trust gpt4 with math and often find it’s reasoning and abstraction abilities shaky, although it’s deeply impressive that they exist at all.  and driving a car is out because it’s a task that you can’t really describe.
what about the future?
there’s been a great deal about the danger of ai discussed recently, and quite a mess of misexpectations about where we are.

is gpt4 and future variants the answer to [insert intelligence-requiring problem here]?  gpt4 seems most interesting as a language intelligence.  it’s clearly useful as an advisor or a brainstormer.  the meaning of “gpt5” isn’t clear, but i would expect substantial shifts in core algorithms/representations are necessary for mastering other forms of intelligence like memory, skill formation, information gathering, and optimized decision making.
are generative models the end of consensual reality?  human societies seem to have a systematic weakness in that people often prefer a consistent viewpoint even at the expense of fairly extreme rationalization.  that behavior in large language models is just looking at our collective behavior through a mirror.  generative model development (both language and video) do have a real potential to worsen this. i  believe we should be making real efforts as a society to harden and defend objective reality in a multiple ways.  this is not specifically about ai, but it would address a class of ai-related concerns and improve society generally.
is ai about to kill everyone? yudkowski’s editorial gives the impression that a terminator style apocalypse is just around the corner.  i’m skeptical about the short term (the next several years), but the longer term requires thought.

in the short term there are so many limitations of even gpt4 (even though it’s a giant advance) that i both lack the imagination to see a path to “everyone dies” and i expect it would be suicidal for an ai as well.  gpt4, as an ai, is using the borrowed intelligence of the internet.  without that source it’s just an amalgamation of parameters of no interesting capabilities.
for the medium term, i think there’s a credible possibility that drone warfare becomes ultralethal inline with this imagined future.  you can already see drone warfare in the ukraine-russia war significantly increasing the lethality of a battlefield.  this requires some significant advances, but nothing seems outlandish.  counterdrone technology development and limits on usage inline with other war machines seems prudent.
for the longer term, vinge’s classical singularity essay is telling here as he lays out the inevitability of developing intelligence for competitive reasons.  economists are often fond of pointing out how job creation has accompanied previous mechanization induced job losses and yet my daughter points out how we keep increasing the amount of schooling children must absorb to be capable members of society.  it’s not hard to imagine a desolation of jobs in a decade or two where ais can simply handle almost all present-day jobs and most humans can’t skill-up to be economically meaningful.  our society is not prepared for this situation—it seems like a quite serious and possibly inevitable possibility.  positive models for a nearly-fully-automated society are provided by star trek and iain banks although science fiction is very far from a working proposal for a working society.
i’m skeptical about a lawnmower man like scenario where a superintelligence suddenly takes over the world.  in essence, cryptographic barriers are plausibly real, even to a superintelligence.  as long as that’s so, the thing to watch out for is excessive concentrations of power without oversight.  we already have a functioning notion of super-human intelligence in organizational intelligence and are familiar with techniques for restraining organizational intelligence into useful-for-society channels.  starting with this and improving seems reasonable.







posted on 7/19/20217/19/2021icml 2021 invited speakers — ml for science 

by: stefanie jegelka and ameet talwalkar (icml21 communication chairs)
with icml 2021 underway, we wanted to briefly highlight the upcoming invited talks. a general theme of the invited talks this year is “machine learning for science.” the program chairs (marina meila and tong zhang) have invited world-renowned scientists from various disciplines to discuss their problems and the corresponding machine learning challenges.  by exposing the machine learning community to these fascinating problems, we hope that we can help to further expand the applicability of machine learning to a wide range of scientific domains. 
daphne koller (tuesday, july 20th at 8am pdt): dr. koller is a pioneer in the field of machine learning, and is currently the founder and ceo of insitro, which leverages machine learning for drug discovery. she was the rajeev motwani professor of computer science at stanford university, where she served on the faculty for 18 years. she was the co-founder, co-ceo and president of coursera, and the chief computing officer of calico, an alphabet company in the healthcare space. she received the macarthur foundation fellowship in 2004, was awarded the acm prize in computing in 2008, and was recognized as one of time magazine’s 100 most influential people in 2012.xiao cunde and dahe qin (tuesday, july 20th at 8pm pdt): dr. cunde is a glaciologist and deputy director of the institute of the climate system, chinese academy of meteorological sciences. he has worked in the fields of polar glaciology and meteorology since 1997. his major research focus has been ice core studies relating to paleo-climate and paleo-environment, and present day cold region meteorological and glaciological processes that impact environmental and climatic changes. dr. qin is the former director of the china meteorological administration. he is a glaciologist and the first chinese ever to cross the south pole. he was a member of the 1989 international cross south pole expedition and has published numerous ground-breaking articles, using evidence gathered from his antarctic expeditions.esther duflo (wednesday, july 21st at 8am pdt): dr. duflo is the abdul latif jameel professor of poverty alleviation and development economics in the department of economics at mit and a co-founder and co-director of the abdul latif jameel poverty action lab (j-pal). in her research, she seeks to understand the economic lives of the poor, with the aim to help design and evaluate social policies. she has worked on health, education, financial inclusion, environment and governance. in 2019, she received a nobel prize in economic sciences “for their experimental approach to alleviating global poverty”. in particular, she and co-authors have introduced a new approach to obtaining reliable answers about the best ways to fight global poverty.edward chang (wednesday, july 21st at 8pm pdt): dr. chang is a professor in the department of neurological surgery at the ucsf weill institute for neurosciences. he is a neurosurgeon and uses machine learning to understand brain functions. his research focuses on the brain mechanisms for speech, movement and human emotion. he co-directs the center for neural engineering and prostheses, a collaborative enterprise of ucsf and uc berkeley. the center brings together experts in engineering, neurology and neurosurgery to develop state-of-the-art biomedical technology to restore function for patients with neurological disabilities such as paralysis and speech disorders.cecilia clementi (thursday, july 22nd at 8am pdt):  dr. clementi is a professor of chemistry, and chemical and biomolecular engineering, and senior scientist in the center for theoretical biological physics at rice university, and an einstein fellow at fu berlin. she researches strategies to study complex biophysical processes on long timescales, and she is an expert in the simulation of biomolecules using large-scale ml. her group designs multiscale models, adaptive sampling approaches, and data analysis tools, and uses both data-driven methods and theoretical formulations.
to register for the conference and check out these talks, please visit: https://icml.cc/.




posted on 4/23/2021alt highlights – an interview with joelle pineau 

welcome to alt highlights, a series of blog posts spotlighting various happenings at the recent conference alt 2021, including plenary talks, tutorials, trends in learning theory, and more! to reach a broad audience, the series will be disseminated as guest posts on different blogs in machine learning and theoretical computer science. john has been kind enough to host the first post in the series. this initiative is organized by the learning theory alliance, and overseen by gautam kamath. all posts in alt highlights are indexed on the official learning theory alliance blog.
the first post is an interview with joelle pineau, by michal moshkovitz and keziah naggita.

we would like you to meet dr. joelle pineau, an astounding leader in ai, based in montreal, canada.


name: joelle pineau
institutions: joelle pineau is a faculty member at mila and an associate professor and william dawson scholar at the school of computer science at mcgill university, where she co-directs the reasoning and learning lab. she is a senior fellow of the canadian institute for advanced research (cifar), a co-managing director of facebook ai research, and the montreal, canada lab director. learn more information about  joelle here and her talk here.






reinforcement learning (rl)
how and why did you choose to work in reinforcement learning?   what are the things that inspired you to choose health as a domain of application for your rl work?
i started working in reinforcement learning at the beginning of my phd  in robotics at cmu.  quite honestly, i was delighted by the elegance of  the mathematical formulation.  it also had some link to topics i studied previously (in supervised learning & in operations search).   it was also useful for decision-making, which was complementary to state tracking & prediction, which was the topic studied by many other members of my lab at the time.
i started working on applications to health-care early in my career as a faculty at mcgill.  i was curious to explore practical applications, and found some colleagues in health-care who had some interesting decision-making problems with the right characteristics.
how would you recommend a newcomer enter the rl field?  for rl researchers interested in safety, is there some literature you can recommend as a starting point?
get familiar with the basic mathematical formalism & algorithm, try your hand at easy simulation cases.  for rl and safety, the literature is very small and quite recent, so it’s easy enough to get started.  work on constrained mdps (altman, 1999) is a good starting point.  see also the work on seldonian rl, by phil tomas and colleagues.
in your talk you mentioned applications of rl to different domains. what do you think is the main achievement of rl? 
the alphago result was very impressive!  recently, the work on using rl to control the flight of the loon balloons is also quite impressive.
what are the big open problems in rl? 
efficient exploration continues to be a major challenge.  stability of learning, even when the data is non-stationary (e.g. due to policy change), is also very important to address.  in my talk i also highlighted the importance of development methods for rl with responsible properties (safety, security, transparency, etc.) as a major open problem.
collaborations
based on your work in neurostimulation, it appears that people from different fields of expertise were involved. 
yes, this was a close collaboration between researchers in cs (my own lab) and researchers in neuroscience, with expertise in electrophysiology.
what advice would you give researchers in finding interdisciplinary collaborators?
this collaboration was literally started by me picking up the phone and calling a colleague in neuroscience to propose the project.  i then wrote a grant proposal and obtained funding to start the project.  more generally, these days it’s actually very easy for researchers in machine learning to find interdisciplinary collaborators.  giving talks, offering office hours, speaking to colleagues you meet in random events – i’ve had literally dozens of projects proposed to me in the last few years, from all sorts of disciplines.
what are some of the best ways to foster successful collaborations tackling work cutting across multiple disciplines?
spend time understanding the problems from the point of view of your collaborator, and commit to solving *that* problem.  don’t walk in with  your own hammer (or pre-selected set of techniques), and expect to find a problem to show-off your techniques. genuine curiosity about the other field is very valuable!  don’t hesitate to read the literature – don’t expect your collaborator to share all the needed knowledge.  co-supervising a student together is also often an effective way of working closely together.
academia, industry and everything in between 
during the talk, you mentioned variance in freedom of research for theoreticians in industry versus academia. could you elaborate more about this? are there certain personality traits or characteristics more likely to make someone more successful in academia versus industry?
for certain more theoretical work, it can be a long time until the impact and value of the work is realized.  this is perhaps harder to support in industry, which is better suited to appreciated shorter-term impact.  another big difference is that in academia, professors work closely with students and junior researchers, and should expect to dedicate a good amount of time and energy to training & developing them (even if it means the work might move along a bit slower).  in industry, a researcher will most often work with more senior researchers, and the project is likely to move along faster (also because no one is taking or teaching courses).
how do you balance leadership, for example, at fair, with students advising like at mcgill, research [cifair, fair, mcgill, mila], and personal life? 
it’s useful to have clarity about your priorities.  don’t let other people dictate what these are – you should decide for yourself.  and then spend your time according to this.  i enjoy my work at fair a lot, i also really enjoy spending time with my grad students at mcgill/mila, and of course i really enjoy time with my family & friends.  so i try to keep a good balance between all of this. i also try to be clear & transparent with other people about my availability & priorities, so they can plan accordingly.
what do you think distinguishes the mindset of an extraordinary researcher?
to be a strong researcher, it helps to be very curious, genuinely want to understand and find out new knowledge. the ability to find new connections between ideas, concepts, is also useful.  for scientific research, you also need discipline and good methodology, and a commitment to deep understanding (rather than “proving” whatever hypothesis you hold).   frankly, i also don’t think we need to further cultivate the myth of the “extraordinary researcher”.  research is primarily a collective institution, where many people contribute, in ways small and big, and it is through this collective work that we achieve big discoveries and breakthroughs!




posted on 12/14/202012/14/2020what is the right response to employer misbehavior in research? 

i enjoyed my conversations with timnit when she was in the msr-nyc lab, so her situation has been on my mind throughout neurips.
piecing together what happened second-hand is always tricky, but jeff dean’s account and timnit’s agree on a basic outline.  timnit and others wrote a paper for facct which was approved for submission by the normal internal review process, then later unapproved.  timnit threatened to leave unless various details about this unapproval were clarified.  google then declared her resigned.
the definition of resign makes it clear an employee does it, not an employer. since that apparently never happened, this is a mischaracterized firing.  it also seems quite credible that the unapproval process was highly unusual based on various reactions i’ve seen and my personal expectations of what researchers would typically tolerate.
this frankly looks bad to me and quite a number of other people.  aside from the plain facts, this is also consistent with racism and/or sexism given the roles of those involved.  google itself now faces a substantial rebellion amongst employees.
however, i worry about consequences to some of these reactions.

some people suggest not reviewing papers from google-based researchers.  as a personal decision, this is making a program chair’s difficult job harder. as a communal decision, this would devastate the community since a substantial fraction are employed at google.  these people did not make this decision and many actively support timnit there (at some risk to their job) so a mass-punishment approach seems deeply counterproductive.
others have suggested that google should not be a sponsor at major machine learning conferences.  since all of these are run as nonprofits, the lost grants will either be made up by increasing costs for everyone or reducing grants to students and diversity sponsorship.  reduced grants in particular seem deeply counterproductive.
some have suggested that all industry research in general is bad.  industrial research varies substantially from place to place, perhaps much more so than in academia.  as an example, microsoft research has no similar internal review process for publications.  overall, the stereotyping inherent in this view makes me uncomfortable and there are some real advantages to working in industry in terms of ability to concentrate on research or effecting real change.

it’s critical to understand that the strength of the research community is incredibly valuable to the community.  it’s not hard to imagine a different arrangement where all industrial research is proprietary, with only a few major companies operating competitive  internal research teams.  this sort of structure exists in some other fields, often to the detriment of anyone other than a major company.  researchers at those companies can’t as easily switch jobs and researchers outside of those companies may lack the context to even contribute to the state of the art.  the field itself progresses slower and in a more secretive way due to lack of sharing.  anticommunal acts based on  mass ostracization or abandonment could shift our structure from the current relatively happy equilibrium where people from all over can participate, learn, and contribute towards a much worse situation.
this is not to say that there are no consequences.  the substantial natural consequences of a significant moral-impacting event will play out regardless of anything else.  the marketplace for top researchers is quite competitive so for many of them uncertainty about the feasibility of publication, the disposition and competence of senior leadership, or constraints on topics tips the balance towards other offers.  that may be severe this year, since this all blew up as the recruiting season was launching and i expect it to last over many years unless some significant action is taken.  in this sense, i expect all the competitors may be looking forward to recruiting more than they were previously and the cost of not resolving the conflict here in a better way may be much, much higher than just about any other course of action.  this is not particularly hypothetical—i saw it play out over the years after the silicon valley lab was cut as the brain drain of other great researchers in competitive areas was severe for several years afterwards.
i don’t think a general answer to the starting question is possible, since it will always depend on circumstances.  even this instance is complex with actions that could cause unintuitive adverse impacts on unanticipated parts of our community or damage the community as a whole.  i personally hope that the considerable natural consequences here form a substantial deterrent to misbehavior in the long term. please think this through when considering your actions here.
edits: tweaked conclusion wording a bit with advice from reshamas.




posted on 12/1/2020experiments with the icml 2020 peer-review process 

this post is cross-listed on the cmu ml blog.
the international conference on machine learning (icml) is a flagship machine learning conference that in 2020 received 4,990 submissions and managed a pool of 3,931 reviewers and area chairs. given that the stakes in the review process are high — the careers of researchers are often significantly affected by the publications in top venues — we decided to scrutinize several components of the peer-review process in a series of experiments. specifically, in conjunction with the icml 2020 conference, we performed three experiments that target: resubmission policies, management of reviewer discussions, and reviewer recruiting. in this post, we summarize the results of these studies.
resubmission bias
motivation. several leading ml and ai conferences have recently started requiring authors to declare previous submission history of their papers. in part, such measures are taken to reduce the load on reviewers by discouraging resubmissions without substantial changes. however, this requirement poses a risk of bias in reviewers’ evaluations.
research question. do reviewers get biased when they know that the paper they are reviewing was previously rejected from a similar venue?
procedure. we organized an auxiliary conference review process with 134 junior reviewers from 5 top us schools and 19 papers from various areas of ml. we assigned participants 1 paper each and asked them to review the paper as if it was submitted to icml. unbeknown to participants, we allocated them to a test or control condition uniformly at random:
control. participants review the papers as usual.
test. before reading the paper, participants are told that the paper they review is a resubmission.
hypothesis. we expect that if the bias is present, reviewers in the test condition should be harsher than in the control. 
key findings. reviewers give almost one point lower score (95% confidence interval: [0.24, 1.30]) on a 10-point likert item for the overall evaluation of a paper when they are told that a paper is a resubmission. in terms of narrower review criteria, reviewers tend to underrate “paper quality” the most.
implications. conference organizers need to evaluate a trade-off between envisaged benefits such as the hypothetical reduction in the number of submissions and the potential unfairness introduced to the process by the resubmission bias. one option to reduce the bias is to postpone the moment in which the resubmission signal is revealed until after the initial reviews are submitted. this finding must also be accounted for when deciding whether the reviews of rejected papers should be publicly available on systems like openreview.net and others. 
details. http://arxiv.org/abs/2011.14646
herding effects in discussions
motivation. past research on human decision making shows that group discussion is susceptible to various biases related to social influence. for instance, it is documented that the decision of a group may be biased towards the opinion of the group member who proposes the solution first. we call this effect herding and note that, in peer review, herding (if present) may result in undesirable artifacts in decisions as different area chairs use different strategies to select the discussion initiator.
research question. conditioned on a set of reviewers who actively participate in a discussion of a paper, does the final decision of the paper depend on the order in which reviewers join the discussion?
procedure. we performed a randomized controlled trial on herding in icml 2020 discussions that involved about 1,500 papers and 2,000 reviewers. in peer review, the discussion takes place after the reviewers submit their initial reviews, so we know prior opinions of reviewers about the papers. with this information, we split a subset of icml papers into two groups uniformly at random and applied different discussion-management strategies to them: 
positive group. first ask the most positive reviewer to start the discussion, then later ask the most negative reviewer to contribute to the discussion.
negative group. first ask the most negative reviewer to start the discussion, then later ask the most positive reviewer to contribute to the discussion.
hypothesis. the only difference between the strategies is the order in which reviewers are supposed to join the discussion. hence, if the herding is absent, the strategies will not impact submissions from the two groups disproportionately. however, if the herding is present, we expect that the difference in the order will introduce a difference in the acceptance rates across the two groups of papers.
key findings. the analysis of outcomes of approximately 1,500 papers does not reveal a statistically significant difference in acceptance rates between the two groups of papers. hence, we find no evidence of herding in the discussion phase of peer review.
implications. regarding the concern of herding which is found to occur in other applications involving people, discussion in peer review does not seem to be susceptible to this effect and hence no specific measures to counteract herding in peer-review discussions are needed.
details. https://arxiv.org/abs/2011.15083
novice reviewer recruiting
motivation.  a surge in the number of submissions received by leading ml and  ai conferences has challenged the sustainability of the review process by increasing the burden on the pool of qualified reviewers. leading conferences have been addressing the issue by relaxing the seniority bar for reviewers and inviting very junior researchers with limited or no publication history, but there is mixed evidence regarding the impact of such interventions on the quality of reviews. 
research question. can very junior reviewers be recruited and guided such that they enlarge the reviewer pool of leading ml and ai conferences without compromising the quality of the process?
procedure. we implemented a twofold approach towards managing novice reviewers:
selection. we evaluated reviews written in the aforementioned auxiliary conference review process involving 134 junior reviewers, and invited 52 of these reviewers who produced the strongest reviews to join the reviewer pool of icml 2020. most of these 52 “experimental” reviewers come from the population not considered by the conventional way of reviewer recruiting used in icml 2020.mentoring. in the actual conference, we provided these experimental reviewers with a senior researcher as a point of contact who offered additional mentoring.
hypothesis. if our approach allows to bring strong reviewers to the pool, we expect experimental reviewers to perform at least as good as reviewers from the main pool on various metrics, including the quality of reviews as rated by area chairs.
key findings. a combination of the selection and mentoring mechanisms results in reviews of at least comparable and on some metrics even higher-rated quality as compared to the conventional pool of reviews: 30% of reviews written by the experimental reviewers exceeded the expectations of area chairs (compared to only 14% for the main pool).
implications. the experiment received positive feedback from participants who appreciated the opportunity to become a reviewer in icml 2020 and from authors of papers used in the auxiliary review process who received a set of useful reviews without submitting to a real conference. hence, we believe that a promising direction is to replicate the experiment at a larger scale and evaluate the benefits of each component of our approach.
details. http://arxiv.org/abs/2011.15050
conclusion
all in all, the experiments we conducted in icml 2020 reveal some useful and actionable insights about the peer-review process. we hope that some of these ideas will help to design a better peer-review pipeline in future conferences.
we thank icml area chairs, reviewers, and authors for their tremendous efforts. we would also like to thank the microsoft conference management toolkit (cmt) team for their continuous support and implementation of features necessary to run these experiments, the authors of papers contributed to the auxiliary review process for their responsiveness, and participants of the resubmission bias experiment for their enthusiasm. finally, we thank ed kennedy and devendra chaplot for their help with designing and executing the experiments.
the post is based on the works by  ivan stelmakh, nihar b. shah, aarti singh, hal daumé iii, and charvi rastogi.



posts navigation
page 1
page 2
…
page 110
next page  




details

a modest proposal
how to contribute a post
who? what? why?
why did my comment not appear?




search for:


  search

 subscribe 
john on twitterrecent commentsjohn langford on an ai miracle malcontentnikos karampatziakis on an ai miracle malcontentcomputer science junction on icml 2021 invited speakers — ml for sciencecomputer science junction on alt highlights – an interview with joelle pineaujohn langford on what is the right response to employer misbehavior in research?categories

active (12)

ai (18)

announcements (116)

applications (4)

bayesian (18)

boosting (1)

code (17)

competitions (24)

computation (6)

conferences (95)

coronavirus (2)

cs (4)

deep (5)

definitions (11)

economics (3)

empirical (6)

exploration (6)

funding (23)

general (73)

generative (1)

graduates (2)

information theory (4)

interactive (5)

language (12)

machine learning (327)

mathematics (2)

mdl (1)

medical (1)

meta (4)

neuroscience (2)

online (38)

organization (16)

papers (45)

parallel (1)

politics (1)

prediction theory (20)

privacy (1)

problem design (2)

problems (31)

questions (3)

reductions (32)

reinforcement (30)

research (54)

reviewing
 (24)

robots (2)

semisupervised (4)

solutions (5)

statistics (6)

structured (7)

supervised (15)

teaching (18)

theory (8)

trees (2)

universal learning (5)

unsupervised (7)

vision (6)

workshop (36)


ml related

gelman—smciss
icml paper discussion
inductio ex machina
kd nuggets
kernel machines
machine learning thoughts
mloss
reinforcement learning
sm, dm, & ml
wikipedia: machine learning


research

computational complexity
computer research policy
geomblog
mathematics
mathematics and computation
michael nielsen
oddhead
quantum algorithms
quantum pontiff


meta

register log in
entries feed
comments feed
wordpress.org













































































































































































