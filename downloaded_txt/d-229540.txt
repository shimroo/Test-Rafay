









autonomous learning laboratory













toggle navigation













people


publications


join


ai safety


















autonomous learning laboratory








college of information and computer sciences

university of massachusetts













				the
				autonomous learning laboratory (all) conducts foundational
				artificial intelligence (ai) research, with emphases on
				ai safety and
				reinforcement learning (rl), and particularly the intersection of these two areas.
			

				the long-term goals of the laboratory are to develop more capable artificial agents, ensure that systems that use artificial
				intelligence methods are safe and well-behaved, improve our understanding of biological learning and its neural basis,
				and to forge stronger links between studies of learning by computer scientists, engineers, neuroscientists, and psychologists.
			

				for an overview of the ml papers from umass in 2021, see our 2021 retrospective here.
			









people




directors







bruno castro da silva

co-director
bsilva@cs.umass.edu






philip s. thomas

co-director
pthomas@cs.umass.edu



staff







matt lustig

grants and contracts coordinator
mllustig@cs.umass.edu




doctoral students







blossom metevier

phd student
bmetevier@cs.umass.edu






james kostas

phd student
jekostas@cs.umass.edu






aline weber

phd student
alineweber@cs.umass.edu






dhawal gupta

phd student
dgupta@cs.umass.edu






shreyas chaudhari

phd student
schaudhari@cs.umass.edu






will schwarzer

phd student
wschwarzer@cs.umass.edu






john raisbeck

phd student
jraisbeck@cs.umass.edu






alexandra burushkina

phd student
aburushkina@cs.umass.edu






norman renhao zhang

phd student
renhaozhang@cs.umass.edu











alumni




directors






sridhar mahadevan

director
					
not accepting new students

mahadeva@cs.umass.edu






andrew barto

founder
					
not accepting new students
						

barto@cs.umass.edu


doctoral students



name
adviser
year
current website




chris nota
philip s. thomas
2023

link



scott jordan
philip s. thomas
2022

link



yash chandak
philip s. thomas
2022

link



stephen giguere
philip s. thomas
2021

link



francisco garcia
philip s. thomas
2019

link



clemens rosenbaum
sridhar mahadevan
2019

link



ian gemp
sridhar mahadevan
2019

link



thomas boucher
sridhar mahadevan
2018

link



cj carey
sridhar mahadevan
2017

link



bo liu
sridhar mahadevan
2015

link



chris vigorito
andrew barto
2015

link



philip thomas
andrew barto
2015

link



bruno castro da silva
andrew barto
2015

link



william dabney
andrew barto
2014

link



scott niekum
andrew barto
2013

link



yariv z. levy
andrew barto
2012

link



scott kuindersma
andrew barto
2012

link



george konidaris
andrew barto
2011

link



jeffrey johns
sridhar mahadevan
2010

link



chang wang
sridhar mahadevan
2010

link



alicia "pippin" peregrin wolfe
andrew barto
2010

link



sarah osentoski
sridhar mahadevan
2009

link



ashvin shah
andrew barto
2008

link



özgür şimşek
andrew barto
2008

link



khashayar rohanimanesh
sridhar mahadevan
2006



mohammad ghavamzadeh
sridhar mahadevan
2005

link



anders jonsson
andrew barto
2005

link



thomas kalt
andrew barto
2005



balaraman ravindran
andrew barto
2004

link



michael rosenstein
andrew barto
2003



michael duff
andrew barto
2002



amy mcgovern
andrew barto
2002

link



theodore perkins
andrew barto
2002

link



doina precup
andrew barto
2000

link



bob crites
andrew barto
1996



s. j. bradtke
andrew barto
1994



satinder singh
andrew barto
1993

link



j. r. backrach
andrew barto
1992

link



vijaykumar gullapalli
andrew barto
1992



robert a. jacobs
andrew barto
1990

link



j. s. judd
andrew barto
1988



charles w. anderson
andrew barto
1986

link



richard s. sutton
andrew barto
1984

link




postdocs



name
adviser
year
current website




jay buckingham
andrew barto




michael kositsky
andrew barto
1998 - 2001



matthew schlesinger
andrew barto
1998 - 2000

link



andrew h. fagg
andrew barto
1998 - 2004

link



sascha e. engelbrecht
andrew barto
1996 - 2002



vijaykumar gullapalli
andrew barto
1992 - 1994



michael jordan
andrew barto


link




masters and bachelors students



name
adviser
year
degree




sarah brockman
p. s. thomas
2019
bs


michael amirault
p. s. thomas
2018
bs


stefan dernbach
sridhar mahadevan
2015
ms


jonathan leahey
sridhar mahadevan
2013
ms


jie chen
sridhar mahadevan
2013
ms


andrew stout
andrew barto
2011
ms


armita kaboli
andrew barto
2011
ms


peter krafft
andrew barto
2010
bs


colin barringer
andrew barto
2007
ms


suchi saria
sridhar mahadevan
2002 - 2004
bs


eric sondhi
sridhar mahadevan

bs


ilya scheidwasser
sridhar mahadevan

bs












publications






2024


						j. p. hanna, y. chandak, p. s. thomas, m. white, p. stone, and s. niekum
						
data-efficient policy evaluation through behavior policy search

						in journal of machine learning research, vol. 25, issue 313, pages 1–58, 2024.
						
						[pdf]
					

						s. chaudhari, a. deshpande, b. castro da silva, and p. s. thomas
						
abstract reward processes: leveraging state abstraction for consistent off-policy evaluation

						in advances in neural information processing systems (neurips), 2024.
						
						[arxiv]
					

						a. ayoub, d. szepesvari, f. zanini, d. gupta, b. chan, b. c. da silva, d. schuurmans
						
mitigating the curse of horizon in monte-carlo returns

						in the 1st reinforcement learning conference (rlc), 2024.
						
						[pdf]
					

						k. choudhary, d. gupta, and p. s. thomas
						
icu-sepsis: a benchmark mdp built from real medical data

						reinforcement learning journal, vol. 4, pages 1546–1566, september 2024.
						
						[pdf, arxiv]
					

						s. m. jordan, s. neumann, j. e. kostas, a. white, and p. s. thomas
						
the cliff of overcommitment with policy gradient step sizes

						reinforcement learning journal, vol. 2, pages 864–883, september 2024.
						
						[pdf]
					

						s. m. jordan, b. castro da silva, a. white, m. white, and p. s. thomas
						
position: benchmarking in reinforcement learning is limited and alternatives are needed

						in proceedings of the international conference on machine learning (icml), 2024.
						
						[pdf, arxiv]
					

						vinay samuel, henry peng zou, yue zhou, shreyas chaudhari, ashwin kalyan, tanmay rajpurohit, ameet deshpande, karthik narasimhan, vishvak murahari
						
personagym: evaluating persona agents and llms

						[arxiv]
					

						s. yeh, b. metevier, a. hoag, and p. s. thomas
						
analyzing the relationship between difference and ratio-based fairness metrics

						in proceedings of the acm conference on fairness, accountability, and transparency (acm facct), 2024.
						
						[pdf]
					

						d. m. bossens and p. s. thomas
						
low variance off-policy evaluation with state-based importance sampling

						in proceedings of the ieee conference on artificial intelligence (ieee cai), 2024.
						
						[pdf, arxiv]
					

						shreyas chaudhari, pranjal aggarwal, vishvak murahari, tanmay rajpurohit, ashwin kalyan, karthik narasimhan, ameet deshpande, bruno castro da silva
						
rlhf deciphered: a critical analysis of reinforcement learning from human feedback for llms

						[arxiv]
					

						abhiman neelakanteswara, shreyas chaudhari, hamed zamani
						
rags to style: personalizing llms with style embeddings

						in workshop on personalization of generative ai systems @ eacl '24.
						
						[pdf]
					

						d. gupta, s. m. jordan, s. chaudhari, b. liu, p. s. thomas, and b. castro da silva
						
from past to future: rethinking eligibility traces

						in the 38th annual aaai conference on artificial intelligence (aaai), 2024.
						
						[pdf, arxiv]
					

						shreyas chaudhari, david arbour, georgios theocharous, nikos vlassis
						
distributional off-policy evaluation for slate recommendations

						in aaai '24.
						
						[pdf]
					


2023



						d. gupta, y. chow, m. ghavamzadeh, c. boutilier
						
offline reinforcement learning for mixture-of-expert dialogue management

						in thirty-seventh conference on neural information processing systems (neurips), 2023.
						
						[arxiv]
					

						d. gupta, y. chandak, s. m. jordan, p. s. thomas, and b. castro da silva
						
behavior alignment via reward function optimization

						in advances in neural information processing systems (neurips), 2023.
						
						[pdf, arxiv]
					

						s. chaudhari, p. s. thomas, and b. castro da silva
						
learning models and evaluating policies with offline off-policy data under partial observability

						in neurips 2023 workshop on adaptive experimental design and active learning in the real world (realml-2023), 2023.
						
						[pdf]
					

						y. luo, a. hoag, and p. s. thomas
						
learning fair representations with high-confidence guarantees

						arxiv:2310.15358, 2023.
						
						[pdf, arxiv]
					

						j. e. kostas, s. m. jordan, y. chandak, g. theocharous, d. gupta, m. white, b. castro da silva, and p. s. thomas
						
coagent networks: generalized and scaled

						arxiv:2305.09838, 2023.
						
						[pdf, arxiv]
					


						c. nota
						
on the convergence of discounted policy gradient methods

arxiv:2212.14066, 2023.

						[arxiv]
					

						a. hoag, j. kostas, b. da silva, p. s. thomas, and y. brun
						
seldonian toolkit: building software with safe and fair machine learning

						at 
						icse 2023.


						v. liu, y. chandak, p. s. thomas, and m. white
						
asymptotically unbiased off-policy policy evaluation when reusing old data in nonstationary environments.

						at
						ai stats 2023.



						y. chow, a. tulepbergenov, o. machum, d. gupta, m. ryu, m. ghavamzadeh, c. boutilier
						
a mixture-of-expert approach to rl-based dialogue management

						in international conference on learning representations (iclr), 2023.
						
						[pdf]
					

2022


						y. chandak, s. niekum, b. c. d. silva, e. learned-miller, e. brunskill, p. s. thomas
						
universal off-policy evaluation


rldm 2022 best paper winner!


						[arxiv]
					

						a. weber*, b. metevier*,  y. brun, p. s. thomas, b.c. da silva 
						
enforcing delayed-impact fairness guarantees
at
						rldm 2022.
					

						j. e. kostas, s. m. jordan, y. chandak, g. theocharous, d. gupta, p. s. thomas
						
a generalized learning rule for asynchronous coagent networks
at
						rldm 2022.
					

						c nota, c wong, and p. s. thomas
						
auto-encoding recurrent representations
at
						rldm 2022.
						
						[pdf]
					

						w. tan, d. koleczek, s. pradhan, n. perello, v. chettiar, n. ma, a. rajaram, v. rohra, s. srinivasan, h. m. s. hossain, y. chandak
						
on optimizing interventions in shared autonomy
in
						aaai 2022.
						
						[arxiv]
					

						c. yuan, y. chandak, s. giguere, p. s. thomas, s. niekum
						
sope: spectrum of off-policy estimators
at
						aaai 2022.
						
						[arxiv]
					

						s. giguere, b. metevier, y. brun, b. castro da silva, p. s. thomas, and s. niekum
						
fairness guarantees under demographic shift
in 
						iclr 2022.
						
						[pdf]
					

						j. yeager, e. moss, m. norrish, and p. s. thomas
						
mechanizing soundness of off-policy evaluation
in 
						itp 2022.
					

						a. bhatia, p. s. thomas, s. zilberstein
						
adaptive rollout length for model-based rl using model-free deep rl

arxiv:2206.02380, 2022.


						y. chandak, s. shankar, n. bastian, b. castro da silva, e. brunskill, and p. s. thomas.
						
off-policy evaluation for action-dependent non-stationary environments

						in
						neurips 2022.



2021


						j. kostas, y. chandak, s. jordan, g. theocharous, and p. s. thomas 
						
high confidence generalization for reinforcement learning
in 
						icml 2021.
						
						[pdf]
						[link]
					

						c. nota, b. castro da silva, and p. s. thomas
						
posterior value functions: hindsight baselines for policy gradient methods
in 
						icml 2021.
						
						[pdf]
						[link]
					

						y. chandak, s. shankar, p.s. thomas
						
high confidence off-policy (or counterfactual) variance estimation

						in
						aaai 2021.
						
						[pdf]
						[link]
						[arxiv]
					

						m. phan, p. s. thomas, and e. learned-miller
						
towards practical mean bounds for small samples
in 
						icml 2021.
						
						[link]
						[arxiv]
					

						l. alegre, a. l. bazzan, and b. castro da silva
						
minimum-delay adaptation in non-stationary reinforcement learning via online high-confidence change-point detection
in
						aamas 2021.
						
						[pdf]
						[link]
					

						w. tan, d. koleczek, s. pradhan, n. perello, v. chettiar, n. ma, a. rajaram, v. rohra, s. srinivasan, h. m. s. hossain, y. chandak
						
intervention aware shared autonomy

humanai@icml, 2021.
						
						[pdf]
					

						y. chandak, s. niekum, b. c. d. silva, e. learned-miller, e. brunskill, p. s. thomas
						
universal off-policy evaluation
in
						neurips 2021.
						
						[arxiv]
					

						c. yuan, y. chandak, s. giguere, p. s. thomas, s. niekum
						
sope: spectrum of off-policy estimators
in
						neurips 2021.
						
						[arxiv]
					

						e. lobo, y. chandak, d. subramanian, j. hanna, m. petrik
						
behavior policy search for risk estimators in reinforcement learning
at
						saferl@neurips 2021.
						
						[arxiv]
					


2020


						y. chandak, s. jordan, g. theocharous, m. white, p. s. thomas
						
towards safe policy improvement for non-stationary mdps

in neurips 2020.
						
						[pdf]
						[link]
						[arxiv]
					

						j. kostas, c. nota, and p. s. thomas
						
asynchronous coagent networks
in
						icml 2020.
						
						[pdf]
						[supplementary materials]
					

						s. m. jordan, y. chandak, d. cohen, m. zhang, and p. s. thomas
						
evaluating the performance of reinforcement learning algorithms
in
						icml 2020.
						
						[pdf]
						[arxiv]
						[code]
					

						y. chandak, g. theocharous, s. shankar, m. white, s. mahadevan, and p. s. thomas
						
optimizing for the future in non-stationary mdps
in
						icml 2020.
						
						[pdf]
						[arxiv]
					

						c. nota and p. s. thomas
						
is the policy gradient a gradient?
in
						aamas 2020.
						
						[pdf]
						[arxiv]
					

						y. chandak, g. theocharous, c. nota, and p. s. thomas
						
lifelong learning with a changing action set
in
						aaai 2020.
						
						[pdf]
						[arxiv]
					

						y. chandak, g. theocharous, b. metevier, p. s. thomas
						
reinforcement learning when all actions are not always available
in
						aaai 2020.
						
						[pdf]
						[arxiv]
					

						g. theocharous, y. chandak, p. s. thomas, and f. de nijs.
						
reinforcement learning for strategic recommendations.

arxiv:2009.07346, 2020.

						[pdf]
						[arxiv]
					


2019


						p. s. thomas, b. castro da silva, a. g. barto, s. giguere, y. brun, and e. brunskill.
						
preventing undesirable behavior of intelligent machines
in
						science vol. 366, issue 6468, pages 999–1004, 2019.
						
						[link]
						[supplementary materials]
					

						b. metevier, s. giguere, s. brockman, a. kobren, y. brun, e. brunskill, and p. s. thomas
						
offline contextual bandits with high probability fairness guarantees
in
						neurips, 2019.
						
						[pdf]
						[link]
					

						f. garcia and p. s. thomas
						
a meta-mdp approach to exploration for lifelong reinforcement learning
in
						neurips 2019

						[pdf]
						[link]
					

						y. chandak, g. theocharous, j. kostas, s. m. jordan, and p. s. thomas
						
learning action representations for reinforcement learning
in
						icml, 2019.
						
						[pdf]
						[arxiv]
					

						p. s. thomas and e. learned-miller
						
concentration inequalities for conditional value at risk
in
						icml, 2019.

						[pdf]
					

						s. tiwari and p. s. thomas
						
natural option critic
in
						aaai, 2019.
						
						[pdf]
						[arxiv]
					

						s. m. jordan, y. chandak, m. zhang, d. cohen, p. s. thomas
						
evaluating reinforcement learning algorithms using cumulative distributions of performance
at
						rldm, 2019.


						y. chandak, g. theocharous, j. kostas, s. m. jordan, and p. s. thomas
						
improving generalization over large action sets
at
						rldm, 2019.


						p. s. thomas, s. m. jordan, y. chandak, c. nota, and j. kostas
						
classical policy gradient: preserving bellman's principle of optimality

						[arxiv]
					

						e. learned-miller and p.s. thomas
						
a new confidence interval for the mean of a bounded random variable

						[pdf]
						[arxiv]
					

						j. kostas, c. nota, and p. s. thomas
						
asynchronous coagent networks: stochastic networks for reinforcement learning without backpropagation or a clock

						[pdf]
						[arxiv]
					



2018


						p. s. thomas, c. dann, and e. brunskill.
						
decoupling gradient-like learning rules from representations
in
						icml, 2018.
						[
						pdf ]
					

						c. rosenbaum, t. klinger, and m. riemer
						
routing networks: adaptive selection of non-linear functions for multi-task learning
in
						iclr, 2018.
						[
						pdf ]
					

						m. machado, c. rosenbaum, x. guo, m. liu, g. tesauro, and m. campbell
						
eigenoption discovery through the deep successor representation
in
						iclr, 2018.
						[
						pdf ]
					
 
						y. chandak, g. theocharous, j. kostas, and p. s. thomas 
						
reinforcement learning with a dynamic action set
in
						continual learning workshop , nips 2018.
					

						s. m. jordan, d. cohen, and p. s. thomas
						
using cumulative distribution based performance analysis to benchmark models
in
						critiquing and correcting trends in ml workshop, nips 2018.
						[
						pdf ]
					

						s. giguere and p. s. thomas.
						
classification with probabilistic fairness guarantees
presented at
						fairware, 2018.
					

						a. jagannatha, p. s. thomas, and h. yu.
						
towards high confidence off-policy reinforcement learning for clinical applications
presented at
						causalml, 2018.
						[
						pdf ]
					



2017


						i. durugkar, i. gemp, and s. mahadevan
						
generative multi-adversarial networks
in
						iclr, 2017.
						[
						pdf ]
					

						x. guo, t. klinger, c. rosenbaum, j. p. bigus, m. campbell, b. kawas, k. talamadupula, g. tesauro, and s. singh
						
learning to query, reason, and answer questions on ambiguous texts
in
						iclr, 2017.
						[
						pdf ]
					

						c. rosenbaum, t. gao, and t. klinger
						
e-qraq: a multi-turn reasoning dataset and simulator with explanations
in
						whi@icml, 2017.
						[
						pdf ]
					


1978 – 2016
click here for a listing of older publications.
			










joining








prospective doctoral students:

					prof. da silva will be accepting one student. prof. thomas will not be recruiting doctoral students for fall 2024. during years that we are recruiting, submit your application here.
					if you mention the lab directors and your interest in the lab in your application, we will be notified and will look through your application materials.
				

prospective interns:

					the autonomous learning laboratory is not accepting applications for interns at any level at this time.
				

prospective masters students:

					the autonomous learning laboratory is not accepting applications for masters level positions at this time.
				

prospective postdoctoral researchers:

					the autonomous learning laboratory is not accepting applications for postdoctoral researchers at this time.
				

















